{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The autoreload extension is already loaded. To reload it, use:\n",
      "  %reload_ext autoreload\n"
     ]
    }
   ],
   "source": [
    "from keras.models import Sequential\n",
    "from keras.layers import Conv2D, ZeroPadding2D, Activation, Input, concatenate\n",
    "from keras.models import Model\n",
    "from keras.layers.normalization import BatchNormalization\n",
    "from keras.layers.pooling import MaxPooling2D, AveragePooling2D\n",
    "from keras.layers.merge import Concatenate\n",
    "from keras.layers.core import Lambda, Flatten, Dense\n",
    "from keras.initializers import glorot_uniform\n",
    "from keras.engine.topology import Layer\n",
    "from keras import backend as K\n",
    "\n",
    "#------------用于绘制模型细节，可选--------------#\n",
    "from IPython.display import SVG\n",
    "from keras.utils.vis_utils import model_to_dot\n",
    "from keras.utils import plot_model\n",
    "#------------------------------------------------#\n",
    "\n",
    "K.set_image_data_format('channels_first')\n",
    "\n",
    "import time\n",
    "import cv2\n",
    "import os\n",
    "import numpy as np\n",
    "from numpy import genfromtxt\n",
    "import pandas as pd\n",
    "import tensorflow as tf\n",
    "import fr_utils\n",
    "from inception_blocks_v2 import *\n",
    "\n",
    "%matplotlib inline\n",
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "#np.set_printoptions(threshold=np.nan)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 简单的人脸验证\n",
    "在人脸验证中，你需要给出两张照片并想知道是否是同一个人，最简单的方法是逐像素地比较这两幅图像，如果图片之间的误差小于选择的阈值，那么则可能是同一个人。 \n",
    "<img src=\"1.png\" style=\"width:380px;height:150px;\">\n",
    "当然，如果你真的这么做的话效果一定会很差，因为像素值的变化在很大程度上是由于光照、人脸的朝向、甚至头部的位置的微小变化等等。接下来与使用原始图像不同的是我们可以让系统学习构建一个编码$f(img)$，对该编码的元素进行比较，可以更准确地判断两幅图像是否属于同一个人。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 将人脸图像编码为128位的向量\n",
    "## 使用卷积网络来进行编码\n",
    "FaceNet模型需要大量的数据和长时间的训练，因为，遵循在应用深度学习设置中常见的实践，我们要加载其他人已经训练过的权值。在网络的架构上我们遵循Szegedy et al.等人的初始模型。这里我们提供了初始模型的实现方法，你可以打开inception_blocks.py文件来查看是如何实现的。\n",
    "\n",
    "关键信息如下：\n",
    "\n",
    "- 该网络使用了96×9696×96的RGB图像作为输入数据，图像数量为mm，输入的数据维度为$(m, n_C, n_H, n_W) = (m, 3, 96, 96)$ \n",
    "\n",
    "- 输出为$(m,128)$的已经编码的mm个128128位的向量。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From D:\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\framework\\op_def_library.py:263: colocate_with (from tensorflow.python.framework.ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Colocations handled automatically by placer.\n",
      "参数数量：3743280\n"
     ]
    }
   ],
   "source": [
    "#获取模型\n",
    "FRmodel = faceRecoModel(input_shape=(3,96,96))\n",
    "\n",
    "#打印模型的总参数数量\n",
    "print(\"参数数量：\" + str(FRmodel.count_params()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "通过使用128神经元全连接层作为最后一层，该模型确保输出是大小为128的编码向量，然后使用比较两个人脸图像的编码如下：\n",
    "<img src=\"2.png\" style=\"width:500px;height:200px;\">\n",
    "因此，如果满足下面两个条件的话，编码是一个比较好的方法：\n",
    "\n",
    "- 同一个人的两个图像的编码非常相似。\n",
    "\n",
    "- 两个不同人物的图像的编码非常不同。\n",
    "\n",
    " 三元组损失函数将上面的形式实现，它会试图将同一个人的两个图像（对于给定的图和正例）的编码“拉近”，同时将两个不同的人的图像（对于给定的图和负例）进一步“分离”。\n",
    " <img src=\"3.png\" style=\"width:500px;height:200px;\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 三元组损失函数\n",
    "对于给定的图像$x$，其编码为$f(x)$，其中$f$为神经网络的计算函数\n",
    "我们将使用三元组图像$（A，P，N）$进行训练：\n",
    "\n",
    "- A是“Anchor”，是一个人的图像。\n",
    "\n",
    "- P是“Positive”，是相对于“Anchor”的同一个人的另外一张图像。\n",
    "\n",
    "- N是“Negative”，是相对于“Anchor”的不同的人的另外一张图像。\n",
    "这些三元组来自训练集，我们使用$(A(i),P(i),N(i))$来表示第$i$个训练样本。我们要保证图像$A(i)$与图像$P(i)$的差值至少比与图像$N(i)$的差值相差$\\alpha$：\n",
    "$$\\mid \\mid f(A^{(i)}) - f(P^{(i)}) \\mid \\mid_2^2 + \\alpha < \\mid \\mid f(A^{(i)}) - f(N^{(i)}) \\mid \\mid_2^2$$\n",
    "我们希望让三元组损失变为最小：\n",
    "$$\\mathcal{J} = \\sum^{m}_{i=1} \\large[ \\small \\underbrace{\\mid \\mid f(A^{(i)}) - f(P^{(i)}) \\mid \\mid_2^2}_\\text{(1)} - \\underbrace{\\mid \\mid f(A^{(i)}) - f(N^{(i)}) \\mid \\mid_2^2}_\\text{(2)} + \\alpha \\large ] \\small_+ \\tag{3}$$\n",
    "- 在这里，我们使用“[⋅⋅⋅]+”来表示函数max(z,0)\n",
    "- $\\alpha$是间距，这个需要我们来手动选择，这里我们使用$\\alpha$=0.2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def triplet_loss(y_true, y_pred, alpha = 0.2):\n",
    "    \"\"\"\n",
    "    根据公式（4）实现三元组损失函数\n",
    "\n",
    "    参数：\n",
    "        y_true -- true标签，当你在Keras里定义了一个损失函数的时候需要它，但是这里不需要。\n",
    "        y_pred -- 列表类型，包含了如下参数：\n",
    "            anchor -- 给定的“anchor”图像的编码，维度为(None,128)\n",
    "            positive -- “positive”图像的编码，维度为(None,128)\n",
    "            negative -- “negative”图像的编码，维度为(None,128)\n",
    "        alpha -- 超参数，阈值\n",
    "\n",
    "    返回：\n",
    "        loss -- 实数，损失的值\n",
    "    \"\"\"\n",
    "    #获取anchor, positive, negative的图像编码\n",
    "    anchor, positive, negative = y_pred[0], y_pred[1], y_pred[2]\n",
    "\n",
    "    #第一步：计算\"anchor\" 与 \"positive\"之间编码的距离，这里需要使用axis=-1\n",
    "    pos_dist = tf.reduce_sum(tf.square(tf.subtract(anchor,positive)),axis=-1)\n",
    "\n",
    "    #第二步：计算\"anchor\" 与 \"negative\"之间编码的距离，这里需要使用axis=-1\n",
    "    neg_dist = tf.reduce_sum(tf.square(tf.subtract(anchor,negative)),axis=-1)\n",
    "\n",
    "    #第三步：减去之前的两个距离，然后加上alpha\n",
    "    basic_loss = tf.add(tf.subtract(pos_dist,neg_dist),alpha)\n",
    "\n",
    "    #通过取带零的最大值和对训练样本的求和来计算整个公式\n",
    "    loss = tf.reduce_sum(tf.maximum(basic_loss,0))\n",
    "\n",
    "    return loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loss = 528.1432\n"
     ]
    }
   ],
   "source": [
    "with tf.Session() as test:\n",
    "    tf.set_random_seed(1)\n",
    "    y_true = (None, None, None)\n",
    "    y_pred = (tf.random_normal([3, 128], mean=6, stddev=0.1, seed = 1),\n",
    "              tf.random_normal([3, 128], mean=1, stddev=1, seed = 1),\n",
    "              tf.random_normal([3, 128], mean=3, stddev=4, seed = 1))\n",
    "    loss = triplet_loss(y_true, y_pred)\n",
    "\n",
    "    print(\"loss = \" + str(loss.eval()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 加载训练好了的模型"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "D:\\Anaconda3\\lib\\site-packages\\ipykernel_launcher.py:2: DeprecationWarning: time.clock has been deprecated in Python 3.3 and will be removed from Python 3.8: use time.perf_counter or time.process_time instead\n",
      "  \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "执行了：2分23秒\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "D:\\Anaconda3\\lib\\site-packages\\ipykernel_launcher.py:11: DeprecationWarning: time.clock has been deprecated in Python 3.3 and will be removed from Python 3.8: use time.perf_counter or time.process_time instead\n",
      "  # This is added back by InteractiveShellApp.init_path()\n"
     ]
    }
   ],
   "source": [
    "#开始时间\n",
    "start_time = time.clock()\n",
    "\n",
    "#编译模型\n",
    "FRmodel.compile(optimizer = 'adam', loss = triplet_loss, metrics = ['accuracy'])\n",
    "\n",
    "#加载权值\n",
    "fr_utils.load_weights_from_FaceNet(FRmodel)\n",
    "\n",
    "#结束时间\n",
    "end_time = time.clock()\n",
    "\n",
    "#计算时差\n",
    "minium = end_time - start_time\n",
    "\n",
    "print(\"执行了：\" + str(int(minium / 60)) + \"分\" + str(int(minium%60)) + \"秒\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 模型的应用\n",
    "## 人脸验证\n",
    "我们构建一个数据库，里面包含了允许进入的人员的编码向量，我们使用fr_uitls.img_to_encoding(image_path, model)函数来生成编码，它会根据图像来进行模型的前向传播。 \n",
    " 我们这里的数据库使用的是一个字典来表示，这个字典将每个人的名字映射到他们面部的128维编码上。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "database = {}\n",
    "database[\"danielle\"] = fr_utils.img_to_encoding(\"images/danielle.png\", FRmodel)\n",
    "database[\"younes\"] = fr_utils.img_to_encoding(\"images/younes.jpg\", FRmodel)\n",
    "database[\"tian\"] = fr_utils.img_to_encoding(\"images/tian.jpg\", FRmodel)\n",
    "database[\"andrew\"] = fr_utils.img_to_encoding(\"images/andrew.jpg\", FRmodel)\n",
    "database[\"kian\"] = fr_utils.img_to_encoding(\"images/kian.jpg\", FRmodel)\n",
    "database[\"dan\"] = fr_utils.img_to_encoding(\"images/dan.jpg\", FRmodel)\n",
    "database[\"sebastiano\"] = fr_utils.img_to_encoding(\"images/sebastiano.jpg\", FRmodel)\n",
    "database[\"bertrand\"] = fr_utils.img_to_encoding(\"images/bertrand.jpg\", FRmodel)\n",
    "database[\"kevin\"] = fr_utils.img_to_encoding(\"images/kevin.jpg\", FRmodel)\n",
    "database[\"felix\"] = fr_utils.img_to_encoding(\"images/felix.jpg\", FRmodel)\n",
    "database[\"benoit\"] = fr_utils.img_to_encoding(\"images/benoit.jpg\", FRmodel)\n",
    "database[\"arnaud\"] = fr_utils.img_to_encoding(\"images/arnaud.jpg\", FRmodel)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "现在，当有人出现在你的门前刷他们的身份证的时候，你可以在数据库中查找他们的编码，用它来检查站在门前的人是否与身份证上的名字匹配。\n",
    "\n",
    " 现在我们要实现 verify() 函数来验证摄像头的照片(image_path)是否与身份证上的名称匹配，这个部分可由以下步骤构成：\n",
    "\n",
    "- 根据image_path来计算编码。\n",
    "\n",
    "- 计算与存储在数据库中的身份图像的编码的差距。\n",
    "\n",
    "- 如果差距小于0.7，那么就打开门，否则就不开门。\n",
    "\n",
    " 如上所述，我们使用L2(np.linalg.norm)来计算差距。(注意:在本实现中，将L2的误差(而不是L2误差的平方)与阈值0.7进行比较。)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def verify(image_path, identity, database, model):\n",
    "    \"\"\"\n",
    "    对“identity”与“image_path”的编码进行验证。\n",
    "\n",
    "    参数：\n",
    "        image_path -- 摄像头的图片。\n",
    "        identity -- 字符类型，想要验证的人的名字。\n",
    "        database -- 字典类型，包含了成员的名字信息与对应的编码。\n",
    "        model -- 在Keras的模型的实例。\n",
    "\n",
    "    返回：\n",
    "        dist -- 摄像头的图片与数据库中的图片的编码的差距。\n",
    "        is_open_door -- boolean,是否该开门。\n",
    "    \"\"\"\n",
    "    #第一步：计算图像的编码，使用fr_utils.img_to_encoding()来计算。\n",
    "    encoding = fr_utils.img_to_encoding(image_path, model)\n",
    "\n",
    "    #第二步：计算与数据库中保存的编码的差距\n",
    "    dist = np.linalg.norm(encoding - database[identity])\n",
    "\n",
    "    #第三步：判断是否打开门\n",
    "    if dist < 0.7:\n",
    "        print(\"欢迎 \" + str(identity) + \"回家！\")\n",
    "        is_door_open = True\n",
    "    else:\n",
    "        print(\"经验证，您与\" + str(identity) + \"不符！\")\n",
    "        is_door_open = False\n",
    "\n",
    "    return dist, is_door_open"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "欢迎 younes回家！\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(0.6710072, True)"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "verify(\"images/camera_0.jpg\",\"younes\",database,FRmodel)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "经验证，您与kian不符！\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(0.8580013, False)"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "verify(\"images/camera_2.jpg\", \"kian\", database, FRmodel)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 人脸识别\n",
    "面部验证系统基本运行良好，但是自从Kian的身份证被偷后，那天晚上他回到房子那里就不能进去了!为了减少这种恶作剧，你想把你的面部验证系统升级成面部识别系统。这样就不用再带身份证了，一个被授权的人只要走到房子前面，前门就会自动为他们打开!\n",
    "\n",
    " 我们将实现一个人脸识别系统，该系统将图像作为输入，并确定它是否是授权人员之一(如果是，是谁),与之前的人脸验证系统不同，我们不再将一个人的名字作为输入的一部分。\n",
    "\n",
    " 现在我们要实现who_is_it()函数，实现它需要有以下步骤：\n",
    "\n",
    "- 根据image_path计算图像的编码。\n",
    "\n",
    "- 从数据库中找出与目标编码具有最小差距的编码。\n",
    "\n",
    "   - 初始化min_dist变量为足够大的数字（100），它将找到与输入的编码最接近的编码。\n",
    "   - 遍历数据库中的名字与编码，可以使用for (name, db_enc) in database.items()语句。 \n",
    "       - 计算目标编码与当前数据库编码之间的L2差距。\n",
    "       - 如果差距小于min_dist，那么就更新名字与编码到identity与min_dist中。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "def who_is_it(image_path, database,model):\n",
    "    \"\"\"\n",
    "    根据指定的图片来进行人脸识别\n",
    "\n",
    "    参数：\n",
    "        images_path -- 图像地址\n",
    "        database -- 包含了名字与编码的字典\n",
    "        model -- 在Keras中的模型的实例。\n",
    "\n",
    "    返回：\n",
    "        min_dist -- 在数据库中与指定图像最相近的编码。\n",
    "        identity -- 字符串类型，与min_dist编码相对应的名字。\n",
    "    \"\"\"\n",
    "    #步骤1：计算指定图像的编码，使用fr_utils.img_to_encoding()来计算。\n",
    "    encoding = fr_utils.img_to_encoding(image_path, model)\n",
    "\n",
    "    #步骤2 ：找到最相近的编码\n",
    "    ## 初始化min_dist变量为足够大的数字，这里设置为100\n",
    "    min_dist = 100\n",
    "\n",
    "    ## 遍历数据库找到最相近的编码\n",
    "    for (name,db_enc) in database.items():\n",
    "        ### 计算目标编码与当前数据库编码之间的L2差距。\n",
    "        dist = np.linalg.norm(encoding - db_enc)\n",
    "\n",
    "        ### 如果差距小于min_dist，那么就更新名字与编码到identity与min_dist中。\n",
    "        if dist < min_dist:\n",
    "            min_dist = dist\n",
    "            identity = name\n",
    "\n",
    "    # 判断是否在数据库中\n",
    "    if min_dist > 0.7:\n",
    "        print(\"抱歉，您的信息不在数据库中。\")\n",
    "\n",
    "    else:\n",
    "        print(\"姓名\" + str(identity) + \"  差距：\" + str(min_dist))\n",
    "\n",
    "    return min_dist, identity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "姓名younes  差距：0.6710072\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(0.6710072, 'younes')"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "who_is_it(\"images/camera_0.jpg\", database, FRmodel)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": true
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
