{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "\n",
    "from keras import layers\n",
    "from keras.layers import Input, Add, Dense, Activation, ZeroPadding2D, BatchNormalization, Flatten, Conv2D, AveragePooling2D, MaxPooling2D, GlobalMaxPooling2D\n",
    "from keras.models import Model, load_model\n",
    "from keras.preprocessing import image\n",
    "from keras.utils import layer_utils\n",
    "from keras.utils.data_utils import get_file\n",
    "from keras.applications.imagenet_utils import preprocess_input\n",
    "from keras.utils.vis_utils import model_to_dot\n",
    "from keras.utils import plot_model\n",
    "from keras.initializers import glorot_uniform\n",
    "\n",
    "import pydot\n",
    "from IPython.display import SVG\n",
    "import scipy.misc\n",
    "from matplotlib.pyplot import imshow\n",
    "import keras.backend as K\n",
    "K.set_image_data_format('channels_last')\n",
    "K.set_learning_phase(1)\n",
    "\n",
    "import resnets_utils "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 构建一个残差网络\n",
    "在残差网络中，一个“捷径（shortcut）”或者说“跳跃连接（skip connection）”允许梯度直接反向传播到更浅的层，如下图：\n",
    "<img src=\"20180509112530701.png\" style=\"width:650px;height:200px;\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 恒等块（Identity block）\n",
    "恒等块是残差网络使用的的标准块，对应于输入的激活值（比如$a^{[l]}$）与输出激活值（比如$a^{[l+1]}$）具有相同的维度。\n",
    "<img src=\"18.png\" style=\"width:650px;height:200px;\">\n",
    "上图中，上面的曲线路径是“捷径”，下面的直线路径是主路径。在上图中，我们依旧把CONV2D 与 ReLU包含到了每个步骤中，为了提升训练的速度，我们在每一步也把数据进行了归一化（BatchNorm）\n",
    "\n",
    "在实践中，我们要做一个更强大的版本：跳跃连接会跳过3个隐藏层而不是两个，就像下图： \n",
    "<img src=\"19.png\" style=\"width:650px;height:200px;\">\n",
    "每个步骤如下：\n",
    "\n",
    "主路径的第一部分：\n",
    "\n",
    "- 第一个CONV2D有$F_1$个过滤器，其大小为（1，1），步长为（1，1），使用填充方式为“valid”，命名规则为`conv_name_base + '2a'`，使用0作为随机种子为其初始化。\n",
    "\n",
    "- 第一个BatchNorm是通道的轴归一化，其命名规则为`bn_name_base + '2a'`。\n",
    "\n",
    "- 接着使用ReLU激活函数，它没有命名也没有超参数。\n",
    "\n",
    "主路径的第二部分：\n",
    "\n",
    "- 第二个CONV2D有$F_2$个过滤器，其大小为$(f,f)$ ，步长为（1，1），使用填充方式为“same”，命名规则为 `conv_name_base + '2b'`，使用0作为随机种子为其初始化。\n",
    "\n",
    "- 第二个BatchNorm是通道的轴归一化，其命名规则为`bn_name_base + '2b'`。\n",
    "\n",
    "- 接着使用ReLU激活函数，它没有命名也没有超参数。\n",
    "\n",
    "主路径的第三部分：\n",
    "\n",
    "- 第三个CONV2D有$F_3$个过滤器，其大小为（1，1），步长为（1，1），使用填充方式为“valid”，命名规则为`conv_name_base + '2c'`，使用0作为随机种子为其初始化。\n",
    "\n",
    "- 第三个BatchNorm是通道的轴归一化，其命名规则为`bn_name_base + '2c'`。\n",
    "\n",
    "- 注意这里没有ReLU函数\n",
    "\n",
    "最后一步：\n",
    "\n",
    "- 将捷径与输入加在一起\n",
    "\n",
    "- 使用ReLU激活函数，它没有命名也没有超参数。\n",
    "\n",
    "接下来我们就要实现残差网络的恒等块了，请务必查看下面的中文手册：\n",
    "- 实现Conv2D：[参见这里](https://keras.io/layers/convolutional/#conv2d)\n",
    "- 实现BatchNorm: [参见这里](https://faroit.github.io/keras-docs/1.2.2/layers/normalization/) (axis: Integer, the axis that should be normalized (typically the channels axis))\n",
    "- 实现激活:  `Activation('relu')(X)`\n",
    "- 添加快捷方式传递的值: [参见这里](https://keras.io/layers/merge/#add)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "code_folding": []
   },
   "outputs": [],
   "source": [
    "def identity_block(X, f, filters, stage, block):\n",
    "    \"\"\"\n",
    "    实现图3的恒等块\n",
    "\n",
    "    参数：\n",
    "        X - 输入的tensor类型的数据，维度为( m, n_H_prev, n_W_prev, n_H_prev )\n",
    "        f - 整数，指定主路径中间的CONV窗口的维度\n",
    "        filters - 整数列表，定义了主路径每层的卷积层的过滤器数量\n",
    "        stage - 整数，根据每层的位置来命名每一层，与block参数一起使用。\n",
    "        block - 字符串，据每层的位置来命名每一层，与stage参数一起使用。\n",
    "\n",
    "    返回：\n",
    "        X - 恒等块的输出，tensor类型，维度为(n_H, n_W, n_C)\n",
    "\n",
    "    \"\"\"\n",
    "\n",
    "    #定义命名规则\n",
    "    conv_name_base = \"res\" + str(stage) + block + \"_branch\"\n",
    "    bn_name_base   = \"bn\"  + str(stage) + block + \"_branch\"\n",
    "\n",
    "    #获取过滤器\n",
    "    F1, F2, F3 = filters\n",
    "\n",
    "    #保存输入数据，将会用于为主路径添加捷径\n",
    "    X_shortcut = X\n",
    "\n",
    "    #主路径的第一部分\n",
    "    ##卷积层\n",
    "    X = Conv2D(filters=F1, kernel_size=(1,1), strides=(1,1) ,padding=\"valid\",\n",
    "               name=conv_name_base+\"2a\", kernel_initializer=glorot_uniform(seed=0))(X)\n",
    "    ##归一化\n",
    "    X = BatchNormalization(axis=3,name=bn_name_base+\"2a\")(X)\n",
    "    ##使用ReLU激活函数\n",
    "    X = Activation(\"relu\")(X)\n",
    "\n",
    "    #主路径的第二部分\n",
    "    ##卷积层\n",
    "    X = Conv2D(filters=F2, kernel_size=(f,f),strides=(1,1), padding=\"same\",\n",
    "               name=conv_name_base+\"2b\", kernel_initializer=glorot_uniform(seed=0))(X)\n",
    "    ##归一化\n",
    "    X = BatchNormalization(axis=3,name=bn_name_base+\"2b\")(X)\n",
    "    ##使用ReLU激活函数\n",
    "    X = Activation(\"relu\")(X)\n",
    "\n",
    "\n",
    "    #主路径的第三部分\n",
    "    ##卷积层\n",
    "    X = Conv2D(filters=F3, kernel_size=(1,1), strides=(1,1), padding=\"valid\",\n",
    "               name=conv_name_base+\"2c\", kernel_initializer=glorot_uniform(seed=0))(X)\n",
    "    ##归一化\n",
    "    X = BatchNormalization(axis=3,name=bn_name_base+\"2c\")(X)\n",
    "    ##没有ReLU激活函数\n",
    "\n",
    "    #最后一步：\n",
    "    ##将捷径与输入加在一起\n",
    "    X = Add()([X,X_shortcut])\n",
    "    ##使用ReLU激活函数\n",
    "    X = Activation(\"relu\")(X)\n",
    "\n",
    "    return X"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 卷积块\n",
    "我们已经实现了残差网络的恒等块，现在，残差网络的卷积块是另一种类型的残差块，它适用于输入输出的维度不一致的情况，它不同于上面的恒等块，与之区别在于，捷径中有一个CONV2D层，如下图：\n",
    "<img src=\"20.png\" style=\"width:650px;height:200px;\">\n",
    "\n",
    "捷径中的卷积层将把输入xx卷积为不同的维度，因此在主路径最后那里需要适配捷径中的维度。比如：把激活值中的宽高减少2倍，我们可以使用1x1的卷积，步伐为2。捷径上的卷积层不使用任何非线性激活函数，它的主要作用是仅仅应用（学习后的）线性函数来减少输入的维度，以便在后面的加法步骤中的维度相匹配。\n",
    "每个步骤如下：\n",
    "\n",
    "主路径的第一部分：\n",
    "\n",
    "- 第一个CONV2D有$F_1$个过滤器，其大小为（1，1），步长为（1，1），使用填充方式为“valid”，命名规则为`conv_name_base + '2a'`，使用0作为随机种子为其初始化。\n",
    "\n",
    "- 第一个BatchNorm是通道的轴归一化，其命名规则为`bn_name_base + '2a'`。\n",
    "\n",
    "- 接着使用ReLU激活函数，它没有命名也没有超参数。\n",
    "\n",
    "主路径的第二部分：\n",
    "\n",
    "- 第二个CONV2D有$F_2$个过滤器，其大小为$(f,f)$ ，步长为（1，1），使用填充方式为“same”，命名规则为 `conv_name_base + '2b'`，使用0作为随机种子为其初始化。\n",
    "\n",
    "- 第二个BatchNorm是通道的轴归一化，其命名规则为`bn_name_base + '2b'`。\n",
    "\n",
    "- 接着使用ReLU激活函数，它没有命名也没有超参数。\n",
    "\n",
    "主路径的第三部分：\n",
    "\n",
    "- 第三个CONV2D有$F_3$个过滤器，其大小为（1，1），步长为（1，1），使用填充方式为“valid”，命名规则为`conv_name_base + '2c'`，使用0作为随机种子为其初始化。\n",
    "\n",
    "- 第三个BatchNorm是通道的轴归一化，其命名规则为`bn_name_base + '2c'`。\n",
    "\n",
    "- 注意这里没有ReLU函数\n",
    "\n",
    "最后一步：\n",
    "\n",
    "- 将捷径与输入加在一起\n",
    "\n",
    "- 使用ReLU激活函数，它没有命名也没有超参数。\n",
    "\n",
    "接下来我们就要实现残差网络的恒等块了，请务必查看下面的中文手册：\n",
    "- 实现Conv2D：[参见这里](https://keras.io/layers/convolutional/#conv2d)\n",
    "- 实现BatchNorm: [参见这里](https://faroit.github.io/keras-docs/1.2.2/layers/normalization/) (axis: Integer, the axis that should be normalized (typically the channels axis))\n",
    "- 实现激活:  `Activation('relu')(X)`\n",
    "- 添加快捷方式传递的值: [参见这里](https://keras.io/layers/merge/#add)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": true
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
