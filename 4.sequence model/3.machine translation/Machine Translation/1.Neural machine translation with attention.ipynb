{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "欢迎来到本周第一个作业。你将建立一个将人类可读日期（“2009年6月25日”）转换为机器可读日期（“2009-06-25”）的神经机器翻译（NMT）模型。 你将使用注意力机制来执行此操作，这是模型序列中最尖端的一个序列。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "from keras.layers import Bidirectional, Concatenate, Permute, Dot, Input, LSTM, Multiply\n",
    "from keras.layers import RepeatVector, Dense, Activation, Lambda\n",
    "from keras.optimizers import Adam\n",
    "from keras.utils import to_categorical\n",
    "from keras.models import load_model, Model\n",
    "import keras.backend as K\n",
    "import numpy as np\n",
    "\n",
    "from faker import Faker\n",
    "import random\n",
    "from tqdm import tqdm\n",
    "from babel.dates import format_date\n",
    "from nmt_utils import *\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 将人类可读日期翻译为机器可读日期\n",
    "你将创建的模型可用于从一种语言翻译为另一种语言，如从英语翻译为印地安语。 但是，语言翻译需要大量的数据集，并且通常需要几天的GPU训练。 在不使用海量数据的情况下，为了让你有机会尝试使用这些模型，我们使用更简单的“日期转换”任务。\n",
    "\n",
    "网络以各种可能格式（例如“1958年8月29日”，“03/30/1968”，“1987年6月24日”）写成的日期作为输入，并将它们转换成标准化的机器可读的日期（例如“1958 -08-29“，”1968-03-30“，”1987-06-24“），让网络学习以通用机器可读格式YYYY-MM-DD输出日期。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 数据集\n",
    "我们将在一个包含10000个可读日期的数据集及其等效的标准化机器可读日期上训练模型。 执行以下命令加载数据并查看一些示例。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████████████████████████████████████████| 10000/10000 [00:00<00:00, 14542.82it/s]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[('16 june 2009', '2009-06-16'),\n",
       " ('monday june 29 2015', '2015-06-29'),\n",
       " ('saturday february 13 1999', '1999-02-13'),\n",
       " ('1/27/91', '1991-01-27'),\n",
       " ('thursday april 20 1972', '1972-04-20'),\n",
       " ('friday december 3 1971', '1971-12-03'),\n",
       " ('26 mar 2015', '2015-03-26'),\n",
       " ('9 06 95', '1995-06-09'),\n",
       " ('monday march 13 1989', '1989-03-13'),\n",
       " ('saturday november 14 1970', '1970-11-14')]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "m = 10000\n",
    "dataset, human_vocab, machine_vocab, inv_machine_vocab = load_dataset(m)\n",
    "dataset[:10]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "其中\n",
    "- dataset：一个二元组列表（人类可读日期，机器可读日期）\n",
    "- human_vocab: 人类可读日期中使用到的所有字符到字典索引值的映射\n",
    "- machine_vocab: 机器可读日期中使用到的所有字符到字典索引值的映射(与human_vocab的索引没必要完全一致)\n",
    "- inv_machine_vocab: machine_vocab 的翻转映射，从索引映射到字符\n",
    "\n",
    "预处理数据并将原始文本数据映射到索引。 我们还将使用Tx=30（假设它是人类可读日期的最大长度;如果得到更长的输入，将不得不截断它）并且Ty=10（因为“YYYY-MM-DD”是10个字符）"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X.shape: (10000, 30)\n",
      "Y.shape: (10000, 10)\n",
      "Xoh.shape: (10000, 30, 37)\n",
      "Yoh.shape: (10000, 10, 11)\n"
     ]
    }
   ],
   "source": [
    "Tx = 30\n",
    "Ty = 10\n",
    "X, Y, Xoh, Yoh = preprocess_data(dataset, human_vocab, machine_vocab, Tx, Ty)\n",
    "\n",
    "print(\"X.shape:\", X.shape)\n",
    "print(\"Y.shape:\", Y.shape)\n",
    "print(\"Xoh.shape:\", Xoh.shape)\n",
    "print(\"Yoh.shape:\", Yoh.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "目前已有：\n",
    "\n",
    "- X: 经过处理的训练集中人类可读日期，其中每个字符都替换为其在human_vocab中映射到的索引。 每个日期用特殊字符进一步填充为Tx长度。 X.shape =（m，Tx）\n",
    "- Y: 经过处理的训练集中机器可读日期，其中每个字符都替换为其在machine_vocab中映射到的索引。 你应该有Y.shape =（m，Ty）。\n",
    "- Xoh：X的one-hot向量，Xoh.shape = (m，Tx，len(human_vocab))\n",
    "- Yoh：Y的one-hot向量，Yoh.shape = (m，Tx，len(machine_vocab))。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Source date: 16 june 2009\n",
      "Target date: 2009-06-16\n",
      "\n",
      "Source after preprocessing (indices): [ 4  9  0 22 31 25 17  0  5  3  3 12 36 36 36 36 36 36 36 36 36 36 36 36\n",
      " 36 36 36 36 36 36]\n",
      "Target after preprocessing (indices): [ 3  1  1 10  0  1  7  0  2  7]\n",
      "\n",
      "Source after preprocessing (one-hot): [[0. 0. 0. ... 0. 0. 0.]\n",
      " [0. 0. 0. ... 0. 0. 0.]\n",
      " [1. 0. 0. ... 0. 0. 0.]\n",
      " ...\n",
      " [0. 0. 0. ... 0. 0. 1.]\n",
      " [0. 0. 0. ... 0. 0. 1.]\n",
      " [0. 0. 0. ... 0. 0. 1.]]\n",
      "Target after preprocessing (one-hot): [[0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1.]\n",
      " [1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0.]\n",
      " [1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0.]]\n"
     ]
    }
   ],
   "source": [
    "index = 0\n",
    "print(\"Source date:\", dataset[index][0])\n",
    "print(\"Target date:\", dataset[index][1])\n",
    "print()\n",
    "print(\"Source after preprocessing (indices):\", X[index])\n",
    "print(\"Target after preprocessing (indices):\", Y[index])\n",
    "print()\n",
    "print(\"Source after preprocessing (one-hot):\", Xoh[index])\n",
    "print(\"Target after preprocessing (one-hot):\", Yoh[index])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 采用注意力机制的机器翻译\n",
    "如果你需要将一段法文翻译成英文，你不会阅读整段然后关闭书本再翻译。 即使在翻译过程中，您也会阅读/重读，并专注于你正在翻译的部分。\n",
    "\n",
    "注意力机制告诉神经翻译模型什么时候应该关注什么\n",
    "## 注意力机制\n",
    "在这一部分，你将会实现注意力机制，图中展示了注意力机制的工作原理。上图是注意力机制的模型，下图是每步注意力变量$\\alpha^{\\langle t, t' \\rangle}$的计算,该变量将用于计算每步输出的$context^{\\langle t \\rangle}$, 其中 ($t=1, \\ldots, T_y$). \n",
    "\n",
    "<table>\n",
    "    <tr>\n",
    "        <td> \n",
    "            <img src=\"images/attn_model.png\" style=\"width:500;height:500px;\"><br>\n",
    "        </td> \n",
    "        <td> \n",
    "            <img src=\"images/attn_mechanism.png\" style=\"width:500;height:500px;\"><br>\n",
    "        </td>\n",
    "    </tr>\n",
    "</table>\n",
    "<caption><center><b>Figure 1</b>: Neural machine translation with attention</center></caption>\n",
    "下面是关于模型的一些属性说明：\n",
    "\n",
    "- 模型中有两个单独的LSTM。 \n",
    "  - 图片底部的是双向LSTM，处在关注机制之前，将其称为pre-attention Bi-LSTM。\n",
    "  - 图片顶部的LSTM出现在关注机制之后，将其称为post-attention LSTM。\n",
    "  - pre-attention Bi-LSTM经过$T_x$个时间步。\n",
    "  - post-attention LSTM经历$T_y$个时间步。\n",
    "- post-attention LSTM将$s^{\\langle t \\rangle}, c^{\\langle t \\rangle}$从一个时间步传给下一个时间步。 \n",
    "  - 在视频课程中，我们仅使用了基本的RNN作为post-attention 序列模型，因此RNN输出激活状态为$s^{\\langle t\\rangle}$。\n",
    "  - 这里我们使用LSTM，因此LSTM具有输出激活状态$s^{\\langle t\\rangle}$和隐藏单元状态$c^{\\langle t\\rangle}$。\n",
    "  - 然而，与先前的文本生成示例（例如第1周的恐龙）不同，在该模型中，在时间t的post-activation LSTM不会将具体生成的$y^{\\langle t-1 \\rangle}$ 作为输入; 而只是$s^{\\langle t\\rangle}$和$c^{\\langle t\\rangle}$作为输入。\n",
    "  - 我们以这种方式设计了模型，因为（与邻近字符高度相关的语言生成不同），在YYYY-MM-DD日期中，前一个字符与下一个字符之间的依赖性不强。\n",
    "- 我们使用$a^{\\langle t \\rangle} = [\\overrightarrow{a}^{\\langle t \\rangle}; \\overleftarrow{a}^{\\langle t \\rangle}]$ 来表示pre-attention Bi-LSTM的前向激活函数和反向激活函数。\n",
    "- 使用RepeatVector节点复制$T_x$次$s^{\\langle t-1 \\rangle}$的值，然后连接$s^{\\langle t-1 \\rangle}$和$a^{\\langle t \\rangle}$ 来计算$e^{\\langle t, t' \\rangle}$>, 然后$e^{\\langle t, t' \\rangle}$通过softmax来计算 $\\alpha^{\\langle t, t' \\rangle}$。\n",
    "我们将在下面解释如何在Keras中使用RepeatVector和Concatenation。\n",
    "\n",
    " 下面实现模型，首先需要实现one_step_attention() 和 model():\n",
    "- 1.one_step_attention(): 在第t个时间步，利用Bi-LSTM的所有隐藏状态 ($[a^{\\langle 1 \\rangle},a^{\\langle 2 \\rangle}, ..., a^{\\langle T_x \\rangle}]$)  和第二个LSTM的之前的隐藏状态($s^{\\langle t-1 \\rangle}$)，函数 one_step_attention() 可以计算出注意力权重($[\\alpha^{\\langle t,1 \\rangle},\\alpha^{\\langle t,2 \\rangle}, ..., \\alpha^{\\langle t,T_x \\rangle}]$)然后输出上下文向量(见图二) \n",
    "$$context^{\\langle t \\rangle} = \\sum_{t' = 0}^{T_x} \\alpha^{\\langle t,t' \\rangle}a^{\\langle t' \\rangle}\\tag{1}$$ \n",
    "- 2.model(): 实现了完整的模型。首先执行Bi-LSTM得到 $[a^{\\langle 1 \\rangle},a^{\\langle 2 \\rangle}, ..., a^{\\langle T_x \\rangle}]$， \n",
    "然后在for循环中执行$T_y$次one_step_attention()。 \n",
    "每次迭代都是利用$c^{\\langle t\\rangle}$计算下一个LSTM 并通过softmax层产生预测值$\\hat{y}^{\\langle t \\rangle}$。\n",
    "\n",
    "**练习：实现 one_step_attention()**\n",
    "\n",
    "model() 将会在for循环中执行$T_y$次one_step_attention()，重要的是TyTy的每个副本都具有相同的权重，换言之，$T_y$的所有时间步共享权重。\n",
    "\n",
    "下面是如何在Keras中利用共享权重进行实现层 \n",
    "1. 定义层对象(例如全局变量) \n",
    "2. 前向传播时调用这些对象"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Defined shared layers as global variables\n",
    "repeator = RepeatVector(Tx)\n",
    "concatenator = Concatenate(axis=-1)\n",
    "densor1 = Dense(10, activation = \"tanh\")\n",
    "densor2 = Dense(1, activation = \"relu\")\n",
    "activator = Activation(softmax, name='attention_weights') # We are using a custom softmax(axis = 1) loaded in this notebook\n",
    "dotor = Dot(axes = 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "现在可以用这些层实现 one_step_attention() 了。为了在各层传播Keras 的对象X，调用layer(X) (或者layer([X, Y])如果需要多个输入的话)，densor(X) 将通过上面定义的Denser(1)层进行传播。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def one_step_attention(a, s_prev):\n",
    "    \"\"\"\n",
    "    Performs one step of attention: Outputs a context vector computed as a dot product of the attention weights\n",
    "    \"alphas\" and the hidden states \"a\" of the Bi-LSTM.\n",
    "    \n",
    "    Arguments:\n",
    "    a -- hidden state output of the Bi-LSTM, numpy-array of shape (m, Tx, 2*n_a)\n",
    "    s_prev -- previous hidden state of the (post-attention) LSTM, numpy-array of shape (m, n_s)\n",
    "    \n",
    "    Returns:\n",
    "    context -- context vector, input of the next (post-attetion) LSTM cell\n",
    "    \"\"\"\n",
    "    \n",
    "    ### START CODE HERE ###\n",
    "    # Use repeator to repeat s_prev to be of shape (m, Tx, n_s) so that you can concatenate it with all hidden states \"a\" (≈ 1 line)\n",
    "    s_prev = repeator(s_prev)\n",
    "    # Use concatenator to concatenate a and s_prev on the last axis (≈ 1 line)\n",
    "    concat = concatenator([a, s_prev])\n",
    "    # Use densor1 to propagate concat through a small fully-connected neural network to compute the \"intermediate energies\" variable e. (≈1 lines)\n",
    "    e = densor1(concat)\n",
    "    # Use densor2 to propagate e through a small fully-connected neural network to compute the \"energies\" variable energies. (≈1 lines)\n",
    "    energies = densor2(e)\n",
    "    # Use \"activator\" on \"energies\" to compute the attention weights \"alphas\" (≈ 1 line)\n",
    "    alphas = activator(energies)\n",
    "    # Use dotor together with \"alphas\" and \"a\" to compute the context vector to be given to the next (post-attention) LSTM-cell (≈ 1 line)\n",
    "    context = dotor([alphas, a])\n",
    "    ### END CODE HERE ###\n",
    "    \n",
    "    return context"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**练习：实现 model()**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_a = 32\n",
    "n_s = 64\n",
    "post_activation_LSTM_cell = LSTM(n_s, return_state = True)\n",
    "output_layer = Dense(len(machine_vocab), activation=softmax)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "现在你可以在for循环中使用Ty次这些层来生成输出了，它们的参数将不会重新初始化。\n",
    "\n",
    "1. 将输入传给一个 Bidirectional LSTM\n",
    "2. 循环 $t = 0, \\dots, T_y-1$: \n",
    "   1. 在$[\\alpha^{<t,1>},\\alpha^{<t,2>}, ..., \\alpha^{<t,T_x>}]$  和$s^{<t-1>}$上执行 one_step_attention() 得到上下文向量$context^{<t>}$\n",
    "   2. 将$context^{<t>}$输入post-attention LSTM。记得使用`initial_state= [previous hidden state, previous cell state]`传入前面的隐藏状态$s^{<t-1>}$和LSTM的单元状态$c^{<t-1>}$。得到新的隐藏状态$s^{<t>}$和新的单元状态$c^{<t>}$\n",
    "   3. 将$s^{<t>}$传给softmax层，得到输出\n",
    "   4. 将输出添加到输出列表中进行保存\n",
    "3. 创建你的Keras模型实例，模型有三个输入（“inputs”, $s^{<0>}$ and $c^{<0>}$），输出一个列表”output”"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# GRADED FUNCTION: model\n",
    "\n",
    "def model(Tx, Ty, n_a, n_s, human_vocab_size, machine_vocab_size):\n",
    "    \"\"\"\n",
    "    Arguments:\n",
    "    Tx -- length of the input sequence\n",
    "    Ty -- length of the output sequence\n",
    "    n_a -- hidden state size of the Bi-LSTM\n",
    "    n_s -- hidden state size of the post-attention LSTM\n",
    "    human_vocab_size -- size of the python dictionary \"human_vocab\"\n",
    "    machine_vocab_size -- size of the python dictionary \"machine_vocab\"\n",
    "\n",
    "    Returns:\n",
    "    model -- Keras model instance\n",
    "    \"\"\"\n",
    "    \n",
    "    # Define the inputs of your model with a shape (Tx,)\n",
    "    # Define s0 and c0, initial hidden state for the decoder LSTM of shape (n_s,)\n",
    "    X = Input(shape=(Tx, human_vocab_size))\n",
    "    s0 = Input(shape=(n_s,), name='s0')\n",
    "    c0 = Input(shape=(n_s,), name='c0')\n",
    "    s = s0\n",
    "    c = c0\n",
    "    \n",
    "    # Initialize empty list of outputs\n",
    "    outputs = []\n",
    "    \n",
    "    ### START CODE HERE ###\n",
    "    \n",
    "    # Step 1: Define your pre-attention Bi-LSTM. Remember to use return_sequences=True. (≈ 1 line)\n",
    "    a = Bidirectional(LSTM(n_a, return_sequences=True), input_shape=(m, Tx, n_a * 2))(X)\n",
    "    \n",
    "    # Step 2: Iterate for Ty steps\n",
    "    for t in range(Ty):\n",
    "    \n",
    "        # Step 2.A: Perform one step of the attention mechanism to get back the context vector at step t (≈ 1 line)\n",
    "        context = one_step_attention(a, s)\n",
    "        \n",
    "        # Step 2.B: Apply the post-attention LSTM cell to the \"context\" vector.\n",
    "        # Don't forget to pass: initial_state = [hidden state, cell state] (≈ 1 line)\n",
    "        s, _, c = post_activation_LSTM_cell(context, initial_state=[s, c])\n",
    "        \n",
    "        # Step 2.C: Apply Dense layer to the hidden state output of the post-attention LSTM (≈ 1 line)\n",
    "        out = output_layer(s)\n",
    "        \n",
    "        # Step 2.D: Append \"out\" to the \"outputs\" list (≈ 1 line)\n",
    "        outputs.append(out)\n",
    "    \n",
    "    # Step 3: Create model instance taking three inputs and returning the list of outputs. (≈ 1 line)\n",
    "    model = Model(inputs=[X, s0, c0], outputs=outputs)\n",
    "    \n",
    "    ### END CODE HERE ###\n",
    "    \n",
    "    return model\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From D:\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\framework\\op_def_library.py:263: colocate_with (from tensorflow.python.framework.ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Colocations handled automatically by placer.\n"
     ]
    }
   ],
   "source": [
    "#创建模型\n",
    "model = model(Tx, Ty, n_a, n_s, len(human_vocab), len(machine_vocab))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_1 (InputLayer)            (None, 30, 37)       0                                            \n",
      "__________________________________________________________________________________________________\n",
      "s0 (InputLayer)                 (None, 64)           0                                            \n",
      "__________________________________________________________________________________________________\n",
      "bidirectional_1 (Bidirectional) (None, 30, 64)       17920       input_1[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "repeat_vector_1 (RepeatVector)  (None, 30, 64)       0           s0[0][0]                         \n",
      "                                                                 lstm_1[0][0]                     \n",
      "                                                                 lstm_1[1][0]                     \n",
      "                                                                 lstm_1[2][0]                     \n",
      "                                                                 lstm_1[3][0]                     \n",
      "                                                                 lstm_1[4][0]                     \n",
      "                                                                 lstm_1[5][0]                     \n",
      "                                                                 lstm_1[6][0]                     \n",
      "                                                                 lstm_1[7][0]                     \n",
      "                                                                 lstm_1[8][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_1 (Concatenate)     (None, 30, 128)      0           bidirectional_1[0][0]            \n",
      "                                                                 repeat_vector_1[0][0]            \n",
      "                                                                 bidirectional_1[0][0]            \n",
      "                                                                 repeat_vector_1[1][0]            \n",
      "                                                                 bidirectional_1[0][0]            \n",
      "                                                                 repeat_vector_1[2][0]            \n",
      "                                                                 bidirectional_1[0][0]            \n",
      "                                                                 repeat_vector_1[3][0]            \n",
      "                                                                 bidirectional_1[0][0]            \n",
      "                                                                 repeat_vector_1[4][0]            \n",
      "                                                                 bidirectional_1[0][0]            \n",
      "                                                                 repeat_vector_1[5][0]            \n",
      "                                                                 bidirectional_1[0][0]            \n",
      "                                                                 repeat_vector_1[6][0]            \n",
      "                                                                 bidirectional_1[0][0]            \n",
      "                                                                 repeat_vector_1[7][0]            \n",
      "                                                                 bidirectional_1[0][0]            \n",
      "                                                                 repeat_vector_1[8][0]            \n",
      "                                                                 bidirectional_1[0][0]            \n",
      "                                                                 repeat_vector_1[9][0]            \n",
      "__________________________________________________________________________________________________\n",
      "dense_1 (Dense)                 (None, 30, 10)       1290        concatenate_1[0][0]              \n",
      "                                                                 concatenate_1[1][0]              \n",
      "                                                                 concatenate_1[2][0]              \n",
      "                                                                 concatenate_1[3][0]              \n",
      "                                                                 concatenate_1[4][0]              \n",
      "                                                                 concatenate_1[5][0]              \n",
      "                                                                 concatenate_1[6][0]              \n",
      "                                                                 concatenate_1[7][0]              \n",
      "                                                                 concatenate_1[8][0]              \n",
      "                                                                 concatenate_1[9][0]              \n",
      "__________________________________________________________________________________________________\n",
      "dense_2 (Dense)                 (None, 30, 1)        11          dense_1[0][0]                    \n",
      "                                                                 dense_1[1][0]                    \n",
      "                                                                 dense_1[2][0]                    \n",
      "                                                                 dense_1[3][0]                    \n",
      "                                                                 dense_1[4][0]                    \n",
      "                                                                 dense_1[5][0]                    \n",
      "                                                                 dense_1[6][0]                    \n",
      "                                                                 dense_1[7][0]                    \n",
      "                                                                 dense_1[8][0]                    \n",
      "                                                                 dense_1[9][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "attention_weights (Activation)  (None, 30, 1)        0           dense_2[0][0]                    \n",
      "                                                                 dense_2[1][0]                    \n",
      "                                                                 dense_2[2][0]                    \n",
      "                                                                 dense_2[3][0]                    \n",
      "                                                                 dense_2[4][0]                    \n",
      "                                                                 dense_2[5][0]                    \n",
      "                                                                 dense_2[6][0]                    \n",
      "                                                                 dense_2[7][0]                    \n",
      "                                                                 dense_2[8][0]                    \n",
      "                                                                 dense_2[9][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "dot_1 (Dot)                     (None, 1, 64)        0           attention_weights[0][0]          \n",
      "                                                                 bidirectional_1[0][0]            \n",
      "                                                                 attention_weights[1][0]          \n",
      "                                                                 bidirectional_1[0][0]            \n",
      "                                                                 attention_weights[2][0]          \n",
      "                                                                 bidirectional_1[0][0]            \n",
      "                                                                 attention_weights[3][0]          \n",
      "                                                                 bidirectional_1[0][0]            \n",
      "                                                                 attention_weights[4][0]          \n",
      "                                                                 bidirectional_1[0][0]            \n",
      "                                                                 attention_weights[5][0]          \n",
      "                                                                 bidirectional_1[0][0]            \n",
      "                                                                 attention_weights[6][0]          \n",
      "                                                                 bidirectional_1[0][0]            \n",
      "                                                                 attention_weights[7][0]          \n",
      "                                                                 bidirectional_1[0][0]            \n",
      "                                                                 attention_weights[8][0]          \n",
      "                                                                 bidirectional_1[0][0]            \n",
      "                                                                 attention_weights[9][0]          \n",
      "                                                                 bidirectional_1[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "c0 (InputLayer)                 (None, 64)           0                                            \n",
      "__________________________________________________________________________________________________\n",
      "lstm_1 (LSTM)                   [(None, 64), (None,  33024       dot_1[0][0]                      \n",
      "                                                                 s0[0][0]                         \n",
      "                                                                 c0[0][0]                         \n",
      "                                                                 dot_1[1][0]                      \n",
      "                                                                 lstm_1[0][0]                     \n",
      "                                                                 lstm_1[0][2]                     \n",
      "                                                                 dot_1[2][0]                      \n",
      "                                                                 lstm_1[1][0]                     \n",
      "                                                                 lstm_1[1][2]                     \n",
      "                                                                 dot_1[3][0]                      \n",
      "                                                                 lstm_1[2][0]                     \n",
      "                                                                 lstm_1[2][2]                     \n",
      "                                                                 dot_1[4][0]                      \n",
      "                                                                 lstm_1[3][0]                     \n",
      "                                                                 lstm_1[3][2]                     \n",
      "                                                                 dot_1[5][0]                      \n",
      "                                                                 lstm_1[4][0]                     \n",
      "                                                                 lstm_1[4][2]                     \n",
      "                                                                 dot_1[6][0]                      \n",
      "                                                                 lstm_1[5][0]                     \n",
      "                                                                 lstm_1[5][2]                     \n",
      "                                                                 dot_1[7][0]                      \n",
      "                                                                 lstm_1[6][0]                     \n",
      "                                                                 lstm_1[6][2]                     \n",
      "                                                                 dot_1[8][0]                      \n",
      "                                                                 lstm_1[7][0]                     \n",
      "                                                                 lstm_1[7][2]                     \n",
      "                                                                 dot_1[9][0]                      \n",
      "                                                                 lstm_1[8][0]                     \n",
      "                                                                 lstm_1[8][2]                     \n",
      "__________________________________________________________________________________________________\n",
      "dense_3 (Dense)                 (None, 11)           715         lstm_1[0][0]                     \n",
      "                                                                 lstm_1[1][0]                     \n",
      "                                                                 lstm_1[2][0]                     \n",
      "                                                                 lstm_1[3][0]                     \n",
      "                                                                 lstm_1[4][0]                     \n",
      "                                                                 lstm_1[5][0]                     \n",
      "                                                                 lstm_1[6][0]                     \n",
      "                                                                 lstm_1[7][0]                     \n",
      "                                                                 lstm_1[8][0]                     \n",
      "                                                                 lstm_1[9][0]                     \n",
      "==================================================================================================\n",
      "Total params: 52,960\n",
      "Trainable params: 52,960\n",
      "Non-trainable params: 0\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "和往常一样，在Keras中创建模型之后，需要编译模型。定义损失函数，优化器和指标。使用 categorical_crossentropy 损失，Adam优化器(learning rate = 0.005, β1=0.9, β2=0.999, decay = 0.01) 和 [‘accuracy’]指标来编译模型。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "### START CODE HERE ### (≈2 lines)\n",
    "opt = Adam(lr=0.0005, beta_1=0.9, beta_2=0.999, decay=0.01)\n",
    "model.compile(loss = 'categorical_crossentropy',optimizer=opt, metrics = ['accuracy'])\n",
    "### END CODE HERE ###"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "最后一步是定义模型的输入和输出：\n",
    "\n",
    "- 你已经有了包含所有训练样本的(m=10000,Tx=30)的X\n",
    "- 需要创建 s0,c0 来为 post_activation_LSTM_cell 进行0值初始化\n",
    "- model() 的输出为11个(m, Ty)元素的列表。所以：outputs[i][0], …, outputs[i][Ty] 表示第i的样本(X[i])的标签字符。更一般的说，outputs[i][j] 表示第i个训练样本的第j个字母的标签字符。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "s0 = np.zeros((m, n_s))\n",
    "c0 = np.zeros((m, n_s))\n",
    "outputs = list(Yoh.swapaxes(0,1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 2700/10000 [=======>......................] - ETA: 15s - loss: 21.0406 - dense_3_loss: 2.9821 - dense_3_acc: 0.0000e+00 - dense_3_acc_1: 0.6400 - dense_3_acc_2: 0.2200 - dense_3_acc_3: 0.0000e+00 - dense_3_acc_4: 1.0000 - dense_3_acc_5: 0.0000e+00 - dense_3_acc_6: 0.0000e+00 - dense_3_acc_7: 1.0000 - dense_3_acc_8: 0.0000e+00 - dense_3_acc_9: 0.0000e+ - ETA: 15s - loss: 20.8734 - dense_3_loss: 2.8940 - dense_3_acc: 0.0000e+00 - dense_3_acc_1: 0.6200 - dense_3_acc_2: 0.2200 - dense_3_acc_3: 0.0000e+00 - dense_3_acc_4: 1.0000 - dense_3_acc_5: 0.0000e+00 - dense_3_acc_6: 0.0000e+00 - dense_3_acc_7: 1.0000 - dense_3_acc_8: 0.0000e+00 - dense_3_acc_9: 0.0000e+ - ETA: 15s - loss: 20.8461 - dense_3_loss: 2.8527 - dense_3_acc: 0.0000e+00 - dense_3_acc_1: 0.6400 - dense_3_acc_2: 0.1933 - dense_3_acc_3: 0.0000e+00 - dense_3_acc_4: 1.0000 - dense_3_acc_5: 0.0000e+00 - dense_3_acc_6: 0.0000e+00 - dense_3_acc_7: 1.0000 - dense_3_acc_8: 0.0000e+00 - dense_3_acc_9: 0.0000e+ - ETA: 15s - loss: 20.8613 - dense_3_loss: 2.8649 - dense_3_acc: 0.0000e+00 - dense_3_acc_1: 0.6250 - dense_3_acc_2: 0.1925 - dense_3_acc_3: 0.0000e+00 - dense_3_acc_4: 1.0000 - dense_3_acc_5: 0.0000e+00 - dense_3_acc_6: 0.0000e+00 - dense_3_acc_7: 1.0000 - dense_3_acc_8: 0.0000e+00 - dense_3_acc_9: 0.0000e+ - ETA: 15s - loss: 20.8912 - dense_3_loss: 2.9020 - dense_3_acc: 0.0000e+00 - dense_3_acc_1: 0.6200 - dense_3_acc_2: 0.1940 - dense_3_acc_3: 0.0000e+00 - dense_3_acc_4: 1.0000 - dense_3_acc_5: 0.0000e+00 - dense_3_acc_6: 0.0000e+00 - dense_3_acc_7: 1.0000 - dense_3_acc_8: 0.0000e+00 - dense_3_acc_9: 0.0000e+ - ETA: 15s - loss: 20.9000 - dense_3_loss: 2.9102 - dense_3_acc: 0.0000e+00 - dense_3_acc_1: 0.6133 - dense_3_acc_2: 0.1983 - dense_3_acc_3: 0.0000e+00 - dense_3_acc_4: 1.0000 - dense_3_acc_5: 0.0000e+00 - dense_3_acc_6: 0.0000e+00 - dense_3_acc_7: 1.0000 - dense_3_acc_8: 0.0000e+00 - dense_3_acc_9: 0.0000e+ - ETA: 15s - loss: 20.8945 - dense_3_loss: 2.9245 - dense_3_acc: 0.0000e+00 - dense_3_acc_1: 0.6114 - dense_3_acc_2: 0.1914 - dense_3_acc_3: 0.0000e+00 - dense_3_acc_4: 1.0000 - dense_3_acc_5: 0.0000e+00 - dense_3_acc_6: 0.0000e+00 - dense_3_acc_7: 1.0000 - dense_3_acc_8: 0.0000e+00 - dense_3_acc_9: 0.0000e+ - ETA: 15s - loss: 20.8624 - dense_3_loss: 2.9159 - dense_3_acc: 0.0000e+00 - dense_3_acc_1: 0.5988 - dense_3_acc_2: 0.1950 - dense_3_acc_3: 0.0000e+00 - dense_3_acc_4: 1.0000 - dense_3_acc_5: 0.0000e+00 - dense_3_acc_6: 0.0000e+00 - dense_3_acc_7: 1.0000 - dense_3_acc_8: 0.0000e+00 - dense_3_acc_9: 0.0000e+ - ETA: 15s - loss: 20.8454 - dense_3_loss: 2.9080 - dense_3_acc: 0.0000e+00 - dense_3_acc_1: 0.5856 - dense_3_acc_2: 0.2044 - dense_3_acc_3: 0.0000e+00 - dense_3_acc_4: 1.0000 - dense_3_acc_5: 0.0000e+00 - dense_3_acc_6: 0.0000e+00 - dense_3_acc_7: 1.0000 - dense_3_acc_8: 0.0000e+00 - dense_3_acc_9: 0.0000e+ - ETA: 14s - loss: 20.8376 - dense_3_loss: 2.9063 - dense_3_acc: 0.0000e+00 - dense_3_acc_1: 0.5870 - dense_3_acc_2: 0.2010 - dense_3_acc_3: 0.0000e+00 - dense_3_acc_4: 1.0000 - dense_3_acc_5: 0.0000e+00 - dense_3_acc_6: 0.0000e+00 - dense_3_acc_7: 1.0000 - dense_3_acc_8: 0.0000e+00 - dense_3_acc_9: 0.0000e+ - ETA: 14s - loss: 20.8463 - dense_3_loss: 2.9125 - dense_3_acc: 0.0000e+00 - dense_3_acc_1: 0.5864 - dense_3_acc_2: 0.2009 - dense_3_acc_3: 0.0000e+00 - dense_3_acc_4: 1.0000 - dense_3_acc_5: 0.0000e+00 - dense_3_acc_6: 0.0000e+00 - dense_3_acc_7: 1.0000 - dense_3_acc_8: 0.0000e+00 - dense_3_acc_9: 0.0000e+ - ETA: 14s - loss: 20.8583 - dense_3_loss: 2.9213 - dense_3_acc: 0.0000e+00 - dense_3_acc_1: 0.5908 - dense_3_acc_2: 0.1967 - dense_3_acc_3: 0.0000e+00 - dense_3_acc_4: 1.0000 - dense_3_acc_5: 0.0000e+00 - dense_3_acc_6: 0.0000e+00 - dense_3_acc_7: 1.0000 - dense_3_acc_8: 0.0000e+00 - dense_3_acc_9: 0.0000e+ - ETA: 14s - loss: 20.8561 - dense_3_loss: 2.9180 - dense_3_acc: 0.0000e+00 - dense_3_acc_1: 0.5946 - dense_3_acc_2: 0.1946 - dense_3_acc_3: 0.0000e+00 - dense_3_acc_4: 1.0000 - dense_3_acc_5: 0.0000e+00 - dense_3_acc_6: 0.0000e+00 - dense_3_acc_7: 1.0000 - dense_3_acc_8: 0.0000e+00 - dense_3_acc_9: 0.0000e+ - ETA: 14s - loss: 20.8537 - dense_3_loss: 2.9329 - dense_3_acc: 0.0000e+00 - dense_3_acc_1: 0.6000 - dense_3_acc_2: 0.1921 - dense_3_acc_3: 0.0000e+00 - dense_3_acc_4: 1.0000 - dense_3_acc_5: 0.0000e+00 - dense_3_acc_6: 0.0000e+00 - dense_3_acc_7: 1.0000 - dense_3_acc_8: 0.0000e+00 - dense_3_acc_9: 0.0000e+ - ETA: 14s - loss: 20.8340 - dense_3_loss: 2.9371 - dense_3_acc: 0.0000e+00 - dense_3_acc_1: 0.6000 - dense_3_acc_2: 0.1933 - dense_3_acc_3: 0.0000e+00 - dense_3_acc_4: 1.0000 - dense_3_acc_5: 0.0000e+00 - dense_3_acc_6: 0.0000e+00 - dense_3_acc_7: 1.0000 - dense_3_acc_8: 0.0000e+00 - dense_3_acc_9: 0.0000e+ - ETA: 13s - loss: 20.8411 - dense_3_loss: 2.9298 - dense_3_acc: 0.0000e+00 - dense_3_acc_1: 0.5994 - dense_3_acc_2: 0.1963 - dense_3_acc_3: 0.0000e+00 - dense_3_acc_4: 1.0000 - dense_3_acc_5: 0.0000e+00 - dense_3_acc_6: 0.0000e+00 - dense_3_acc_7: 1.0000 - dense_3_acc_8: 0.0000e+00 - dense_3_acc_9: 0.0000e+ - ETA: 13s - loss: 20.8357 - dense_3_loss: 2.9348 - dense_3_acc: 0.0000e+00 - dense_3_acc_1: 0.5971 - dense_3_acc_2: 0.2041 - dense_3_acc_3: 0.0000e+00 - dense_3_acc_4: 1.0000 - dense_3_acc_5: 0.0000e+00 - dense_3_acc_6: 0.0000e+00 - dense_3_acc_7: 1.0000 - dense_3_acc_8: 0.0000e+00 - dense_3_acc_9: 0.0000e+ - ETA: 13s - loss: 20.8239 - dense_3_loss: 2.9302 - dense_3_acc: 0.0000e+00 - dense_3_acc_1: 0.5972 - dense_3_acc_2: 0.2067 - dense_3_acc_3: 0.0000e+00 - dense_3_acc_4: 1.0000 - dense_3_acc_5: 0.0000e+00 - dense_3_acc_6: 0.0000e+00 - dense_3_acc_7: 1.0000 - dense_3_acc_8: 0.0000e+00 - dense_3_acc_9: 0.0000e+ - ETA: 13s - loss: 20.8144 - dense_3_loss: 2.9297 - dense_3_acc: 0.0000e+00 - dense_3_acc_1: 0.5953 - dense_3_acc_2: 0.2100 - dense_3_acc_3: 0.0000e+00 - dense_3_acc_4: 1.0000 - dense_3_acc_5: 0.0000e+00 - dense_3_acc_6: 0.0000e+00 - dense_3_acc_7: 1.0000 - dense_3_acc_8: 0.0000e+00 - dense_3_acc_9: 0.0000e+ - ETA: 13s - loss: 20.7949 - dense_3_loss: 2.9284 - dense_3_acc: 0.0000e+00 - dense_3_acc_1: 0.5985 - dense_3_acc_2: 0.2125 - dense_3_acc_3: 0.0000e+00 - dense_3_acc_4: 1.0000 - dense_3_acc_5: 0.0000e+00 - dense_3_acc_6: 0.0000e+00 - dense_3_acc_7: 1.0000 - dense_3_acc_8: 0.0000e+00 - dense_3_acc_9: 0.0000e+ - ETA: 13s - loss: 20.7819 - dense_3_loss: 2.9249 - dense_3_acc: 0.0000e+00 - dense_3_acc_1: 0.5986 - dense_3_acc_2: 0.2129 - dense_3_acc_3: 0.0000e+00 - dense_3_acc_4: 1.0000 - dense_3_acc_5: 0.0000e+00 - dense_3_acc_6: 0.0000e+00 - dense_3_acc_7: 1.0000 - dense_3_acc_8: 0.0000e+00 - dense_3_acc_9: 0.0000e+ - ETA: 12s - loss: 20.7590 - dense_3_loss: 2.9190 - dense_3_acc: 0.0000e+00 - dense_3_acc_1: 0.5959 - dense_3_acc_2: 0.2155 - dense_3_acc_3: 0.0000e+00 - dense_3_acc_4: 1.0000 - dense_3_acc_5: 0.0000e+00 - dense_3_acc_6: 0.0000e+00 - dense_3_acc_7: 1.0000 - dense_3_acc_8: 0.0000e+00 - dense_3_acc_9: 0.0000e+ - ETA: 12s - loss: 20.7471 - dense_3_loss: 2.9113 - dense_3_acc: 0.0000e+00 - dense_3_acc_1: 0.5948 - dense_3_acc_2: 0.2157 - dense_3_acc_3: 0.0000e+00 - dense_3_acc_4: 1.0000 - dense_3_acc_5: 0.0000e+00 - dense_3_acc_6: 0.0000e+00 - dense_3_acc_7: 1.0000 - dense_3_acc_8: 0.0000e+00 - dense_3_acc_9: 0.0000e+ - ETA: 12s - loss: 20.7335 - dense_3_loss: 2.9138 - dense_3_acc: 0.0000e+00 - dense_3_acc_1: 0.5942 - dense_3_acc_2: 0.2171 - dense_3_acc_3: 0.0000e+00 - dense_3_acc_4: 1.0000 - dense_3_acc_5: 0.0000e+00 - dense_3_acc_6: 0.0000e+00 - dense_3_acc_7: 1.0000 - dense_3_acc_8: 0.0000e+00 - dense_3_acc_9: 0.0000e+ - ETA: 12s - loss: 20.7287 - dense_3_loss: 2.9194 - dense_3_acc: 0.0000e+00 - dense_3_acc_1: 0.5964 - dense_3_acc_2: 0.2168 - dense_3_acc_3: 0.0000e+00 - dense_3_acc_4: 1.0000 - dense_3_acc_5: 0.0000e+00 - dense_3_acc_6: 0.0000e+00 - dense_3_acc_7: 1.0000 - dense_3_acc_8: 0.0000e+00 - dense_3_acc_9: 0.0000e+ - ETA: 12s - loss: 20.7218 - dense_3_loss: 2.9217 - dense_3_acc: 0.0000e+00 - dense_3_acc_1: 0.5962 - dense_3_acc_2: 0.2185 - dense_3_acc_3: 0.0000e+00 - dense_3_acc_4: 1.0000 - dense_3_acc_5: 0.0000e+00 - dense_3_acc_6: 0.0000e+00 - dense_3_acc_7: 1.0000 - dense_3_acc_8: 0.0000e+00 - dense_3_acc_9: 0.0000e+ - ETA: 12s - loss: 20.7119 - dense_3_loss: 2.9225 - dense_3_acc: 0.0000e+00 - dense_3_acc_1: 0.5974 - dense_3_acc_2: 0.2174 - dense_3_acc_3: 0.0000e+00 - dense_3_acc_4: 1.0000 - dense_3_acc_5: 0.0000e+00 - dense_3_acc_6: 0.0000e+00 - dense_3_acc_7: 1.0000 - dense_3_acc_8: 0.0000e+00 - dense_3_acc_9: 0.0000e+ 5400/10000 [===============>..............] - ETA: 11s - loss: 20.7005 - dense_3_loss: 2.9211 - dense_3_acc: 0.0000e+00 - dense_3_acc_1: 0.5996 - dense_3_acc_2: 0.2218 - dense_3_acc_3: 0.0000e+00 - dense_3_acc_4: 1.0000 - dense_3_acc_5: 0.0000e+00 - dense_3_acc_6: 0.0000e+00 - dense_3_acc_7: 1.0000 - dense_3_acc_8: 0.0000e+00 - dense_3_acc_9: 0.0000e+ - ETA: 11s - loss: 20.6951 - dense_3_loss: 2.9200 - dense_3_acc: 0.0000e+00 - dense_3_acc_1: 0.6003 - dense_3_acc_2: 0.2241 - dense_3_acc_3: 0.0000e+00 - dense_3_acc_4: 1.0000 - dense_3_acc_5: 0.0000e+00 - dense_3_acc_6: 0.0000e+00 - dense_3_acc_7: 1.0000 - dense_3_acc_8: 0.0000e+00 - dense_3_acc_9: 0.0000e+ - ETA: 11s - loss: 20.6807 - dense_3_loss: 2.9187 - dense_3_acc: 0.0000e+00 - dense_3_acc_1: 0.6013 - dense_3_acc_2: 0.2257 - dense_3_acc_3: 0.0000e+00 - dense_3_acc_4: 1.0000 - dense_3_acc_5: 0.0000e+00 - dense_3_acc_6: 0.0000e+00 - dense_3_acc_7: 1.0000 - dense_3_acc_8: 0.0000e+00 - dense_3_acc_9: 0.0000e+ - ETA: 11s - loss: 20.6733 - dense_3_loss: 2.9165 - dense_3_acc: 0.0000e+00 - dense_3_acc_1: 0.6016 - dense_3_acc_2: 0.2284 - dense_3_acc_3: 0.0000e+00 - dense_3_acc_4: 1.0000 - dense_3_acc_5: 0.0000e+00 - dense_3_acc_6: 0.0000e+00 - dense_3_acc_7: 1.0000 - dense_3_acc_8: 0.0000e+00 - dense_3_acc_9: 0.0000e+ - ETA: 11s - loss: 20.6575 - dense_3_loss: 2.9116 - dense_3_acc: 0.0000e+00 - dense_3_acc_1: 0.6019 - dense_3_acc_2: 0.2278 - dense_3_acc_3: 0.0000e+00 - dense_3_acc_4: 1.0000 - dense_3_acc_5: 0.0000e+00 - dense_3_acc_6: 0.0000e+00 - dense_3_acc_7: 1.0000 - dense_3_acc_8: 0.0000e+00 - dense_3_acc_9: 0.0000e+ - ETA: 10s - loss: 20.6501 - dense_3_loss: 2.9157 - dense_3_acc: 0.0000e+00 - dense_3_acc_1: 0.6000 - dense_3_acc_2: 0.2276 - dense_3_acc_3: 0.0000e+00 - dense_3_acc_4: 1.0000 - dense_3_acc_5: 0.0000e+00 - dense_3_acc_6: 0.0000e+00 - dense_3_acc_7: 1.0000 - dense_3_acc_8: 0.0000e+00 - dense_3_acc_9: 0.0000e+ - ETA: 10s - loss: 20.6375 - dense_3_loss: 2.9117 - dense_3_acc: 0.0000e+00 - dense_3_acc_1: 0.6015 - dense_3_acc_2: 0.2265 - dense_3_acc_3: 0.0000e+00 - dense_3_acc_4: 1.0000 - dense_3_acc_5: 0.0000e+00 - dense_3_acc_6: 0.0000e+00 - dense_3_acc_7: 1.0000 - dense_3_acc_8: 0.0000e+00 - dense_3_acc_9: 0.0000e+ - ETA: 10s - loss: 20.6263 - dense_3_loss: 2.9096 - dense_3_acc: 0.0000e+00 - dense_3_acc_1: 0.6017 - dense_3_acc_2: 0.2271 - dense_3_acc_3: 0.0000e+00 - dense_3_acc_4: 1.0000 - dense_3_acc_5: 0.0000e+00 - dense_3_acc_6: 0.0000e+00 - dense_3_acc_7: 1.0000 - dense_3_acc_8: 0.0000e+00 - dense_3_acc_9: 0.0000e+ - ETA: 10s - loss: 20.6182 - dense_3_loss: 2.9077 - dense_3_acc: 0.0000e+00 - dense_3_acc_1: 0.6022 - dense_3_acc_2: 0.2272 - dense_3_acc_3: 0.0000e+00 - dense_3_acc_4: 1.0000 - dense_3_acc_5: 0.0000e+00 - dense_3_acc_6: 0.0000e+00 - dense_3_acc_7: 1.0000 - dense_3_acc_8: 0.0000e+00 - dense_3_acc_9: 0.0000e+ - ETA: 10s - loss: 20.6026 - dense_3_loss: 2.9047 - dense_3_acc: 0.0000e+00 - dense_3_acc_1: 0.6027 - dense_3_acc_2: 0.2278 - dense_3_acc_3: 0.0000e+00 - dense_3_acc_4: 1.0000 - dense_3_acc_5: 0.0000e+00 - dense_3_acc_6: 0.0000e+00 - dense_3_acc_7: 1.0000 - dense_3_acc_8: 0.0000e+00 - dense_3_acc_9: 0.0000e+ - ETA: 10s - loss: 20.5904 - dense_3_loss: 2.9023 - dense_3_acc: 0.0000e+00 - dense_3_acc_1: 0.6039 - dense_3_acc_2: 0.2274 - dense_3_acc_3: 0.0000e+00 - dense_3_acc_4: 1.0000 - dense_3_acc_5: 0.0000e+00 - dense_3_acc_6: 0.0000e+00 - dense_3_acc_7: 1.0000 - dense_3_acc_8: 0.0000e+00 - dense_3_acc_9: 0.0000e+ - ETA: 9s - loss: 20.5800 - dense_3_loss: 2.8981 - dense_3_acc: 0.0000e+00 - dense_3_acc_1: 0.6036 - dense_3_acc_2: 0.2262 - dense_3_acc_3: 2.5641e-04 - dense_3_acc_4: 1.0000 - dense_3_acc_5: 0.0000e+00 - dense_3_acc_6: 0.0000e+00 - dense_3_acc_7: 1.0000 - dense_3_acc_8: 0.0000e+00 - dense_3_acc_9: 0.0000e+00 - ETA: 9s - loss: 20.5657 - dense_3_loss: 2.8962 - dense_3_acc: 0.0000e+00 - dense_3_acc_1: 0.6035 - dense_3_acc_2: 0.2253 - dense_3_acc_3: 2.5000e-04 - dense_3_acc_4: 1.0000 - dense_3_acc_5: 0.0000e+00 - dense_3_acc_6: 0.0000e+00 - dense_3_acc_7: 1.0000 - dense_3_acc_8: 0.0000e+00 - dense_3_acc_9: 0.0000e+0 - ETA: 9s - loss: 20.5502 - dense_3_loss: 2.8931 - dense_3_acc: 0.0000e+00 - dense_3_acc_1: 0.6039 - dense_3_acc_2: 0.2259 - dense_3_acc_3: 7.3171e-04 - dense_3_acc_4: 1.0000 - dense_3_acc_5: 0.0000e+00 - dense_3_acc_6: 0.0000e+00 - dense_3_acc_7: 1.0000 - dense_3_acc_8: 0.0000e+00 - dense_3_acc_9: 0.0000e+0 - ETA: 9s - loss: 20.5377 - dense_3_loss: 2.8897 - dense_3_acc: 0.0000e+00 - dense_3_acc_1: 0.6045 - dense_3_acc_2: 0.2243 - dense_3_acc_3: 9.5238e-04 - dense_3_acc_4: 1.0000 - dense_3_acc_5: 0.0000e+00 - dense_3_acc_6: 0.0000e+00 - dense_3_acc_7: 1.0000 - dense_3_acc_8: 0.0000e+00 - dense_3_acc_9: 0.0000e+0 - ETA: 9s - loss: 20.5233 - dense_3_loss: 2.8868 - dense_3_acc: 0.0000e+00 - dense_3_acc_1: 0.6042 - dense_3_acc_2: 0.2242 - dense_3_acc_3: 0.0014 - dense_3_acc_4: 1.0000 - dense_3_acc_5: 0.0000e+00 - dense_3_acc_6: 0.0000e+00 - dense_3_acc_7: 1.0000 - dense_3_acc_8: 0.0000e+00 - dense_3_acc_9: 0.0000e+00    - ETA: 9s - loss: 20.5133 - dense_3_loss: 2.8865 - dense_3_acc: 0.0000e+00 - dense_3_acc_1: 0.6048 - dense_3_acc_2: 0.2252 - dense_3_acc_3: 0.0020 - dense_3_acc_4: 1.0000 - dense_3_acc_5: 0.0000e+00 - dense_3_acc_6: 0.0000e+00 - dense_3_acc_7: 1.0000 - dense_3_acc_8: 0.0000e+00 - dense_3_acc_9: 0.0000e+0 - ETA: 8s - loss: 20.5047 - dense_3_loss: 2.8847 - dense_3_acc: 0.0000e+00 - dense_3_acc_1: 0.6056 - dense_3_acc_2: 0.2240 - dense_3_acc_3: 0.0029 - dense_3_acc_4: 1.0000 - dense_3_acc_5: 0.0000e+00 - dense_3_acc_6: 0.0000e+00 - dense_3_acc_7: 1.0000 - dense_3_acc_8: 0.0000e+00 - dense_3_acc_9: 0.0000e+0 - ETA: 8s - loss: 20.4897 - dense_3_loss: 2.8848 - dense_3_acc: 0.0000e+00 - dense_3_acc_1: 0.6052 - dense_3_acc_2: 0.2239 - dense_3_acc_3: 0.0030 - dense_3_acc_4: 1.0000 - dense_3_acc_5: 0.0000e+00 - dense_3_acc_6: 0.0000e+00 - dense_3_acc_7: 1.0000 - dense_3_acc_8: 0.0000e+00 - dense_3_acc_9: 0.0000e+0 - ETA: 8s - loss: 20.4725 - dense_3_loss: 2.8818 - dense_3_acc: 0.0000e+00 - dense_3_acc_1: 0.6060 - dense_3_acc_2: 0.2245 - dense_3_acc_3: 0.0032 - dense_3_acc_4: 1.0000 - dense_3_acc_5: 0.0000e+00 - dense_3_acc_6: 0.0000e+00 - dense_3_acc_7: 1.0000 - dense_3_acc_8: 0.0000e+00 - dense_3_acc_9: 2.1277e-0 - ETA: 8s - loss: 20.4565 - dense_3_loss: 2.8785 - dense_3_acc: 0.0000e+00 - dense_3_acc_1: 0.6065 - dense_3_acc_2: 0.2240 - dense_3_acc_3: 0.0031 - dense_3_acc_4: 1.0000 - dense_3_acc_5: 0.0000e+00 - dense_3_acc_6: 0.0000e+00 - dense_3_acc_7: 1.0000 - dense_3_acc_8: 0.0000e+00 - dense_3_acc_9: 2.0833e-0 - ETA: 8s - loss: 20.4433 - dense_3_loss: 2.8781 - dense_3_acc: 0.0000e+00 - dense_3_acc_1: 0.6078 - dense_3_acc_2: 0.2243 - dense_3_acc_3: 0.0031 - dense_3_acc_4: 1.0000 - dense_3_acc_5: 0.0000e+00 - dense_3_acc_6: 0.0000e+00 - dense_3_acc_7: 1.0000 - dense_3_acc_8: 0.0000e+00 - dense_3_acc_9: 8.1633e-0 - ETA: 8s - loss: 20.4257 - dense_3_loss: 2.8781 - dense_3_acc: 0.0000e+00 - dense_3_acc_1: 0.6092 - dense_3_acc_2: 0.2246 - dense_3_acc_3: 0.0030 - dense_3_acc_4: 1.0000 - dense_3_acc_5: 0.0000e+00 - dense_3_acc_6: 0.0000e+00 - dense_3_acc_7: 1.0000 - dense_3_acc_8: 0.0000e+00 - dense_3_acc_9: 1.0000e-0 - ETA: 8s - loss: 20.4108 - dense_3_loss: 2.8753 - dense_3_acc: 0.0000e+00 - dense_3_acc_1: 0.6096 - dense_3_acc_2: 0.2253 - dense_3_acc_3: 0.0029 - dense_3_acc_4: 1.0000 - dense_3_acc_5: 0.0000e+00 - dense_3_acc_6: 0.0000e+00 - dense_3_acc_7: 1.0000 - dense_3_acc_8: 0.0000e+00 - dense_3_acc_9: 0.0018    - ETA: 7s - loss: 20.4024 - dense_3_loss: 2.8724 - dense_3_acc: 0.0000e+00 - dense_3_acc_1: 0.6100 - dense_3_acc_2: 0.2240 - dense_3_acc_3: 0.0031 - dense_3_acc_4: 1.0000 - dense_3_acc_5: 0.0000e+00 - dense_3_acc_6: 0.0000e+00 - dense_3_acc_7: 1.0000 - dense_3_acc_8: 0.0000e+00 - dense_3_acc_9: 0.003 - ETA: 7s - loss: 20.3917 - dense_3_loss: 2.8703 - dense_3_acc: 0.0000e+00 - dense_3_acc_1: 0.6085 - dense_3_acc_2: 0.2226 - dense_3_acc_3: 0.0034 - dense_3_acc_4: 1.0000 - dense_3_acc_5: 0.0000e+00 - dense_3_acc_6: 0.0000e+00 - dense_3_acc_7: 1.0000 - dense_3_acc_8: 0.0000e+00 - dense_3_acc_9: 0.005 - ETA: 7s - loss: 20.3775 - dense_3_loss: 2.8675 - dense_3_acc: 0.0000e+00 - dense_3_acc_1: 0.6081 - dense_3_acc_2: 0.2219 - dense_3_acc_3: 0.0048 - dense_3_acc_4: 1.0000 - dense_3_acc_5: 0.0000e+00 - dense_3_acc_6: 0.0000e+00 - dense_3_acc_7: 1.0000 - dense_3_acc_8: 0.0000e+00 - dense_3_acc_9: 0.0078"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 8200/10000 [=======================>......] - ETA: 7s - loss: 20.3653 - dense_3_loss: 2.8667 - dense_3_acc: 0.0000e+00 - dense_3_acc_1: 0.6089 - dense_3_acc_2: 0.2227 - dense_3_acc_3: 0.0064 - dense_3_acc_4: 1.0000 - dense_3_acc_5: 0.0000e+00 - dense_3_acc_6: 0.0000e+00 - dense_3_acc_7: 1.0000 - dense_3_acc_8: 0.0000e+00 - dense_3_acc_9: 0.009 - ETA: 7s - loss: 20.3542 - dense_3_loss: 2.8665 - dense_3_acc: 0.0000e+00 - dense_3_acc_1: 0.6093 - dense_3_acc_2: 0.2221 - dense_3_acc_3: 0.0080 - dense_3_acc_4: 1.0000 - dense_3_acc_5: 0.0000e+00 - dense_3_acc_6: 0.0000e+00 - dense_3_acc_7: 1.0000 - dense_3_acc_8: 0.0000e+00 - dense_3_acc_9: 0.010 - ETA: 7s - loss: 20.3392 - dense_3_loss: 2.8650 - dense_3_acc: 0.0000e+00 - dense_3_acc_1: 0.6100 - dense_3_acc_2: 0.2226 - dense_3_acc_3: 0.0100 - dense_3_acc_4: 1.0000 - dense_3_acc_5: 0.0000e+00 - dense_3_acc_6: 0.0000e+00 - dense_3_acc_7: 1.0000 - dense_3_acc_8: 0.0000e+00 - dense_3_acc_9: 0.011 - ETA: 6s - loss: 20.3277 - dense_3_loss: 2.8650 - dense_3_acc: 0.0000e+00 - dense_3_acc_1: 0.6112 - dense_3_acc_2: 0.2217 - dense_3_acc_3: 0.0122 - dense_3_acc_4: 1.0000 - dense_3_acc_5: 0.0000e+00 - dense_3_acc_6: 0.0000e+00 - dense_3_acc_7: 1.0000 - dense_3_acc_8: 0.0000e+00 - dense_3_acc_9: 0.012 - ETA: 6s - loss: 20.3147 - dense_3_loss: 2.8604 - dense_3_acc: 0.0000e+00 - dense_3_acc_1: 0.6105 - dense_3_acc_2: 0.2214 - dense_3_acc_3: 0.0131 - dense_3_acc_4: 1.0000 - dense_3_acc_5: 0.0000e+00 - dense_3_acc_6: 0.0000e+00 - dense_3_acc_7: 1.0000 - dense_3_acc_8: 0.0000e+00 - dense_3_acc_9: 0.015 - ETA: 6s - loss: 20.3014 - dense_3_loss: 2.8574 - dense_3_acc: 0.0000e+00 - dense_3_acc_1: 0.6105 - dense_3_acc_2: 0.2208 - dense_3_acc_3: 0.0148 - dense_3_acc_4: 1.0000 - dense_3_acc_5: 0.0000e+00 - dense_3_acc_6: 0.0000e+00 - dense_3_acc_7: 1.0000 - dense_3_acc_8: 0.0000e+00 - dense_3_acc_9: 0.016 - ETA: 6s - loss: 20.2927 - dense_3_loss: 2.8558 - dense_3_acc: 0.0000e+00 - dense_3_acc_1: 0.6102 - dense_3_acc_2: 0.2205 - dense_3_acc_3: 0.0151 - dense_3_acc_4: 1.0000 - dense_3_acc_5: 0.0000e+00 - dense_3_acc_6: 0.0000e+00 - dense_3_acc_7: 1.0000 - dense_3_acc_8: 0.0000e+00 - dense_3_acc_9: 0.018 - ETA: 6s - loss: 20.2793 - dense_3_loss: 2.8543 - dense_3_acc: 0.0000e+00 - dense_3_acc_1: 0.6097 - dense_3_acc_2: 0.2224 - dense_3_acc_3: 0.0163 - dense_3_acc_4: 1.0000 - dense_3_acc_5: 0.0000e+00 - dense_3_acc_6: 0.0000e+00 - dense_3_acc_7: 1.0000 - dense_3_acc_8: 0.0000e+00 - dense_3_acc_9: 0.019 - ETA: 6s - loss: 20.2692 - dense_3_loss: 2.8553 - dense_3_acc: 0.0000e+00 - dense_3_acc_1: 0.6097 - dense_3_acc_2: 0.2225 - dense_3_acc_3: 0.0181 - dense_3_acc_4: 1.0000 - dense_3_acc_5: 0.0000e+00 - dense_3_acc_6: 0.0000e+00 - dense_3_acc_7: 1.0000 - dense_3_acc_8: 0.0000e+00 - dense_3_acc_9: 0.020 - ETA: 5s - loss: 20.2569 - dense_3_loss: 2.8524 - dense_3_acc: 0.0000e+00 - dense_3_acc_1: 0.6103 - dense_3_acc_2: 0.2230 - dense_3_acc_3: 0.0189 - dense_3_acc_4: 1.0000 - dense_3_acc_5: 0.0000e+00 - dense_3_acc_6: 0.0000e+00 - dense_3_acc_7: 1.0000 - dense_3_acc_8: 0.0000e+00 - dense_3_acc_9: 0.021 - ETA: 5s - loss: 20.2480 - dense_3_loss: 2.8526 - dense_3_acc: 0.0000e+00 - dense_3_acc_1: 0.6102 - dense_3_acc_2: 0.2226 - dense_3_acc_3: 0.0202 - dense_3_acc_4: 1.0000 - dense_3_acc_5: 0.0000e+00 - dense_3_acc_6: 0.0000e+00 - dense_3_acc_7: 1.0000 - dense_3_acc_8: 0.0000e+00 - dense_3_acc_9: 0.022 - ETA: 5s - loss: 20.2362 - dense_3_loss: 2.8491 - dense_3_acc: 0.0000e+00 - dense_3_acc_1: 0.6109 - dense_3_acc_2: 0.2230 - dense_3_acc_3: 0.0211 - dense_3_acc_4: 1.0000 - dense_3_acc_5: 0.0000e+00 - dense_3_acc_6: 0.0000e+00 - dense_3_acc_7: 1.0000 - dense_3_acc_8: 0.0000e+00 - dense_3_acc_9: 0.023 - ETA: 5s - loss: 20.2299 - dense_3_loss: 2.8481 - dense_3_acc: 0.0000e+00 - dense_3_acc_1: 0.6106 - dense_3_acc_2: 0.2222 - dense_3_acc_3: 0.0215 - dense_3_acc_4: 1.0000 - dense_3_acc_5: 0.0000e+00 - dense_3_acc_6: 0.0000e+00 - dense_3_acc_7: 1.0000 - dense_3_acc_8: 0.0000e+00 - dense_3_acc_9: 0.025 - ETA: 5s - loss: 20.2169 - dense_3_loss: 2.8456 - dense_3_acc: 0.0000e+00 - dense_3_acc_1: 0.6100 - dense_3_acc_2: 0.2225 - dense_3_acc_3: 0.0229 - dense_3_acc_4: 1.0000 - dense_3_acc_5: 0.0000e+00 - dense_3_acc_6: 0.0000e+00 - dense_3_acc_7: 1.0000 - dense_3_acc_8: 7.3529e-04 - dense_3_acc_9: 0.025 - ETA: 5s - loss: 20.2091 - dense_3_loss: 2.8442 - dense_3_acc: 0.0000e+00 - dense_3_acc_1: 0.6094 - dense_3_acc_2: 0.2223 - dense_3_acc_3: 0.0238 - dense_3_acc_4: 1.0000 - dense_3_acc_5: 0.0000e+00 - dense_3_acc_6: 0.0000e+00 - dense_3_acc_7: 1.0000 - dense_3_acc_8: 0.0028 - dense_3_acc_9: 0.0267    - ETA: 4s - loss: 20.2008 - dense_3_loss: 2.8432 - dense_3_acc: 0.0000e+00 - dense_3_acc_1: 0.6084 - dense_3_acc_2: 0.2216 - dense_3_acc_3: 0.0243 - dense_3_acc_4: 1.0000 - dense_3_acc_5: 0.0000e+00 - dense_3_acc_6: 0.0000e+00 - dense_3_acc_7: 1.0000 - dense_3_acc_8: 0.0054 - dense_3_acc_9: 0.027 - ETA: 4s - loss: 20.1875 - dense_3_loss: 2.8421 - dense_3_acc: 0.0000e+00 - dense_3_acc_1: 0.6086 - dense_3_acc_2: 0.2214 - dense_3_acc_3: 0.0252 - dense_3_acc_4: 1.0000 - dense_3_acc_5: 0.0000e+00 - dense_3_acc_6: 0.0000e+00 - dense_3_acc_7: 1.0000 - dense_3_acc_8: 0.0075 - dense_3_acc_9: 0.028 - ETA: 4s - loss: 20.1757 - dense_3_loss: 2.8406 - dense_3_acc: 0.0000e+00 - dense_3_acc_1: 0.6090 - dense_3_acc_2: 0.2224 - dense_3_acc_3: 0.0262 - dense_3_acc_4: 1.0000 - dense_3_acc_5: 0.0000e+00 - dense_3_acc_6: 0.0000e+00 - dense_3_acc_7: 1.0000 - dense_3_acc_8: 0.0088 - dense_3_acc_9: 0.029 - ETA: 4s - loss: 20.1621 - dense_3_loss: 2.8379 - dense_3_acc: 0.0000e+00 - dense_3_acc_1: 0.6096 - dense_3_acc_2: 0.2233 - dense_3_acc_3: 0.0268 - dense_3_acc_4: 1.0000 - dense_3_acc_5: 0.0000e+00 - dense_3_acc_6: 0.0000e+00 - dense_3_acc_7: 1.0000 - dense_3_acc_8: 0.0096 - dense_3_acc_9: 0.030 - ETA: 4s - loss: 20.1536 - dense_3_loss: 2.8374 - dense_3_acc: 0.0000e+00 - dense_3_acc_1: 0.6091 - dense_3_acc_2: 0.2231 - dense_3_acc_3: 0.0282 - dense_3_acc_4: 1.0000 - dense_3_acc_5: 0.0000e+00 - dense_3_acc_6: 0.0000e+00 - dense_3_acc_7: 1.0000 - dense_3_acc_8: 0.0103 - dense_3_acc_9: 0.032 - ETA: 4s - loss: 20.1426 - dense_3_loss: 2.8364 - dense_3_acc: 0.0000e+00 - dense_3_acc_1: 0.6104 - dense_3_acc_2: 0.2232 - dense_3_acc_3: 0.0292 - dense_3_acc_4: 1.0000 - dense_3_acc_5: 0.0000e+00 - dense_3_acc_6: 0.0000e+00 - dense_3_acc_7: 1.0000 - dense_3_acc_8: 0.0104 - dense_3_acc_9: 0.033 - ETA: 3s - loss: 20.1303 - dense_3_loss: 2.8350 - dense_3_acc: 0.0000e+00 - dense_3_acc_1: 0.6114 - dense_3_acc_2: 0.2236 - dense_3_acc_3: 0.0305 - dense_3_acc_4: 1.0000 - dense_3_acc_5: 0.0000e+00 - dense_3_acc_6: 0.0000e+00 - dense_3_acc_7: 1.0000 - dense_3_acc_8: 0.0104 - dense_3_acc_9: 0.034 - ETA: 3s - loss: 20.1205 - dense_3_loss: 2.8337 - dense_3_acc: 0.0000e+00 - dense_3_acc_1: 0.6117 - dense_3_acc_2: 0.2238 - dense_3_acc_3: 0.0314 - dense_3_acc_4: 1.0000 - dense_3_acc_5: 0.0000e+00 - dense_3_acc_6: 0.0000e+00 - dense_3_acc_7: 1.0000 - dense_3_acc_8: 0.0103 - dense_3_acc_9: 0.034 - ETA: 3s - loss: 20.1124 - dense_3_loss: 2.8321 - dense_3_acc: 0.0000e+00 - dense_3_acc_1: 0.6124 - dense_3_acc_2: 0.2231 - dense_3_acc_3: 0.0319 - dense_3_acc_4: 1.0000 - dense_3_acc_5: 0.0000e+00 - dense_3_acc_6: 0.0000e+00 - dense_3_acc_7: 1.0000 - dense_3_acc_8: 0.0101 - dense_3_acc_9: 0.035 - ETA: 3s - loss: 20.1046 - dense_3_loss: 2.8325 - dense_3_acc: 0.0000e+00 - dense_3_acc_1: 0.6130 - dense_3_acc_2: 0.2232 - dense_3_acc_3: 0.0328 - dense_3_acc_4: 1.0000 - dense_3_acc_5: 0.0000e+00 - dense_3_acc_6: 0.0000e+00 - dense_3_acc_7: 1.0000 - dense_3_acc_8: 0.0101 - dense_3_acc_9: 0.036 - ETA: 3s - loss: 20.0968 - dense_3_loss: 2.8316 - dense_3_acc: 0.0000e+00 - dense_3_acc_1: 0.6132 - dense_3_acc_2: 0.2224 - dense_3_acc_3: 0.0331 - dense_3_acc_4: 1.0000 - dense_3_acc_5: 0.0000e+00 - dense_3_acc_6: 0.0000e+00 - dense_3_acc_7: 1.0000 - dense_3_acc_8: 0.0104 - dense_3_acc_9: 0.037 - ETA: 3s - loss: 20.0862 - dense_3_loss: 2.8302 - dense_3_acc: 0.0000e+00 - dense_3_acc_1: 0.6133 - dense_3_acc_2: 0.2220 - dense_3_acc_3: 0.0342 - dense_3_acc_4: 1.0000 - dense_3_acc_5: 0.0000e+00 - dense_3_acc_6: 0.0000e+00 - dense_3_acc_7: 1.0000 - dense_3_acc_8: 0.0105 - dense_3_acc_9: 0.039 - ETA: 2s - loss: 20.0783 - dense_3_loss: 2.8298 - dense_3_acc: 0.0000e+00 - dense_3_acc_1: 0.6144 - dense_3_acc_2: 0.2221 - dense_3_acc_3: 0.0341 - dense_3_acc_4: 1.0000 - dense_3_acc_5: 0.0000e+00 - dense_3_acc_6: 0.0000e+00 - dense_3_acc_7: 1.0000 - dense_3_acc_8: 0.0106 - dense_3_acc_9: 0.0401"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10000/10000 [==============================] - ETA: 2s - loss: 20.0701 - dense_3_loss: 2.8295 - dense_3_acc: 0.0000e+00 - dense_3_acc_1: 0.6146 - dense_3_acc_2: 0.2213 - dense_3_acc_3: 0.0346 - dense_3_acc_4: 1.0000 - dense_3_acc_5: 0.0000e+00 - dense_3_acc_6: 0.0000e+00 - dense_3_acc_7: 1.0000 - dense_3_acc_8: 0.0111 - dense_3_acc_9: 0.041 - ETA: 2s - loss: 20.0603 - dense_3_loss: 2.8273 - dense_3_acc: 0.0000e+00 - dense_3_acc_1: 0.6144 - dense_3_acc_2: 0.2218 - dense_3_acc_3: 0.0354 - dense_3_acc_4: 1.0000 - dense_3_acc_5: 0.0000e+00 - dense_3_acc_6: 0.0000e+00 - dense_3_acc_7: 1.0000 - dense_3_acc_8: 0.0124 - dense_3_acc_9: 0.042 - ETA: 2s - loss: 20.0510 - dense_3_loss: 2.8272 - dense_3_acc: 0.0000e+00 - dense_3_acc_1: 0.6141 - dense_3_acc_2: 0.2226 - dense_3_acc_3: 0.0358 - dense_3_acc_4: 1.0000 - dense_3_acc_5: 0.0000e+00 - dense_3_acc_6: 0.0000e+00 - dense_3_acc_7: 1.0000 - dense_3_acc_8: 0.0132 - dense_3_acc_9: 0.042 - ETA: 2s - loss: 20.0415 - dense_3_loss: 2.8243 - dense_3_acc: 0.0000e+00 - dense_3_acc_1: 0.6135 - dense_3_acc_2: 0.2216 - dense_3_acc_3: 0.0367 - dense_3_acc_4: 1.0000 - dense_3_acc_5: 0.0000e+00 - dense_3_acc_6: 0.0000e+00 - dense_3_acc_7: 1.0000 - dense_3_acc_8: 0.0147 - dense_3_acc_9: 0.043 - ETA: 2s - loss: 20.0330 - dense_3_loss: 2.8230 - dense_3_acc: 0.0000e+00 - dense_3_acc_1: 0.6131 - dense_3_acc_2: 0.2211 - dense_3_acc_3: 0.0370 - dense_3_acc_4: 1.0000 - dense_3_acc_5: 0.0000e+00 - dense_3_acc_6: 0.0000e+00 - dense_3_acc_7: 1.0000 - dense_3_acc_8: 0.0162 - dense_3_acc_9: 0.044 - ETA: 1s - loss: 20.0233 - dense_3_loss: 2.8227 - dense_3_acc: 0.0000e+00 - dense_3_acc_1: 0.6137 - dense_3_acc_2: 0.2211 - dense_3_acc_3: 0.0374 - dense_3_acc_4: 1.0000 - dense_3_acc_5: 0.0000e+00 - dense_3_acc_6: 0.0000e+00 - dense_3_acc_7: 1.0000 - dense_3_acc_8: 0.0178 - dense_3_acc_9: 0.044 - ETA: 1s - loss: 20.0141 - dense_3_loss: 2.8214 - dense_3_acc: 0.0000e+00 - dense_3_acc_1: 0.6129 - dense_3_acc_2: 0.2211 - dense_3_acc_3: 0.0381 - dense_3_acc_4: 1.0000 - dense_3_acc_5: 0.0000e+00 - dense_3_acc_6: 0.0000e+00 - dense_3_acc_7: 1.0000 - dense_3_acc_8: 0.0187 - dense_3_acc_9: 0.045 - ETA: 1s - loss: 20.0049 - dense_3_loss: 2.8206 - dense_3_acc: 0.0000e+00 - dense_3_acc_1: 0.6127 - dense_3_acc_2: 0.2206 - dense_3_acc_3: 0.0391 - dense_3_acc_4: 1.0000 - dense_3_acc_5: 0.0000e+00 - dense_3_acc_6: 0.0000e+00 - dense_3_acc_7: 1.0000 - dense_3_acc_8: 0.0190 - dense_3_acc_9: 0.045 - ETA: 1s - loss: 19.9939 - dense_3_loss: 2.8193 - dense_3_acc: 0.0000e+00 - dense_3_acc_1: 0.6136 - dense_3_acc_2: 0.2212 - dense_3_acc_3: 0.0392 - dense_3_acc_4: 1.0000 - dense_3_acc_5: 0.0000e+00 - dense_3_acc_6: 0.0000e+00 - dense_3_acc_7: 1.0000 - dense_3_acc_8: 0.0189 - dense_3_acc_9: 0.046 - ETA: 1s - loss: 19.9870 - dense_3_loss: 2.8187 - dense_3_acc: 0.0000e+00 - dense_3_acc_1: 0.6141 - dense_3_acc_2: 0.2208 - dense_3_acc_3: 0.0397 - dense_3_acc_4: 1.0000 - dense_3_acc_5: 0.0000e+00 - dense_3_acc_6: 0.0000e+00 - dense_3_acc_7: 1.0000 - dense_3_acc_8: 0.0187 - dense_3_acc_9: 0.047 - ETA: 1s - loss: 19.9766 - dense_3_loss: 2.8175 - dense_3_acc: 0.0000e+00 - dense_3_acc_1: 0.6148 - dense_3_acc_2: 0.2215 - dense_3_acc_3: 0.0405 - dense_3_acc_4: 1.0000 - dense_3_acc_5: 0.0000e+00 - dense_3_acc_6: 0.0000e+00 - dense_3_acc_7: 1.0000 - dense_3_acc_8: 0.0191 - dense_3_acc_9: 0.048 - ETA: 0s - loss: 19.9675 - dense_3_loss: 2.8157 - dense_3_acc: 0.0000e+00 - dense_3_acc_1: 0.6147 - dense_3_acc_2: 0.2210 - dense_3_acc_3: 0.0412 - dense_3_acc_4: 1.0000 - dense_3_acc_5: 0.0000e+00 - dense_3_acc_6: 0.0000e+00 - dense_3_acc_7: 1.0000 - dense_3_acc_8: 0.0193 - dense_3_acc_9: 0.048 - ETA: 0s - loss: 19.9576 - dense_3_loss: 2.8146 - dense_3_acc: 0.0000e+00 - dense_3_acc_1: 0.6137 - dense_3_acc_2: 0.2207 - dense_3_acc_3: 0.0417 - dense_3_acc_4: 1.0000 - dense_3_acc_5: 0.0000e+00 - dense_3_acc_6: 0.0000e+00 - dense_3_acc_7: 1.0000 - dense_3_acc_8: 0.0205 - dense_3_acc_9: 0.049 - ETA: 0s - loss: 19.9502 - dense_3_loss: 2.8149 - dense_3_acc: 0.0000e+00 - dense_3_acc_1: 0.6141 - dense_3_acc_2: 0.2208 - dense_3_acc_3: 0.0418 - dense_3_acc_4: 1.0000 - dense_3_acc_5: 0.0000e+00 - dense_3_acc_6: 0.0000e+00 - dense_3_acc_7: 1.0000 - dense_3_acc_8: 0.0226 - dense_3_acc_9: 0.049 - ETA: 0s - loss: 19.9413 - dense_3_loss: 2.8140 - dense_3_acc: 0.0000e+00 - dense_3_acc_1: 0.6144 - dense_3_acc_2: 0.2210 - dense_3_acc_3: 0.0424 - dense_3_acc_4: 1.0000 - dense_3_acc_5: 0.0000e+00 - dense_3_acc_6: 0.0000e+00 - dense_3_acc_7: 1.0000 - dense_3_acc_8: 0.0249 - dense_3_acc_9: 0.050 - ETA: 0s - loss: 19.9317 - dense_3_loss: 2.8125 - dense_3_acc: 0.0000e+00 - dense_3_acc_1: 0.6143 - dense_3_acc_2: 0.2214 - dense_3_acc_3: 0.0429 - dense_3_acc_4: 1.0000 - dense_3_acc_5: 0.0000e+00 - dense_3_acc_6: 0.0000e+00 - dense_3_acc_7: 1.0000 - dense_3_acc_8: 0.0277 - dense_3_acc_9: 0.050 - ETA: 0s - loss: 19.9202 - dense_3_loss: 2.8110 - dense_3_acc: 0.0000e+00 - dense_3_acc_1: 0.6147 - dense_3_acc_2: 0.2216 - dense_3_acc_3: 0.0434 - dense_3_acc_4: 1.0000 - dense_3_acc_5: 0.0000e+00 - dense_3_acc_6: 0.0000e+00 - dense_3_acc_7: 1.0000 - dense_3_acc_8: 0.0294 - dense_3_acc_9: 0.051 - 17s 2ms/step - loss: 19.9135 - dense_3_loss: 2.8101 - dense_3_acc: 0.0000e+00 - dense_3_acc_1: 0.6144 - dense_3_acc_2: 0.2207 - dense_3_acc_3: 0.0437 - dense_3_acc_4: 1.0000 - dense_3_acc_5: 0.0000e+00 - dense_3_acc_6: 0.0000e+00 - dense_3_acc_7: 1.0000 - dense_3_acc_8: 0.0313 - dense_3_acc_9: 0.0520\n",
      "Epoch 2/100\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 2800/10000 [=======>......................] - ETA: 16s - loss: 19.2860 - dense_3_loss: 2.7698 - dense_3_acc: 0.0000e+00 - dense_3_acc_1: 0.5300 - dense_3_acc_2: 0.1400 - dense_3_acc_3: 0.0500 - dense_3_acc_4: 1.0000 - dense_3_acc_5: 0.0000e+00 - dense_3_acc_6: 0.0000e+00 - dense_3_acc_7: 1.0000 - dense_3_acc_8: 0.1700 - dense_3_acc_9: 0.07 - ETA: 17s - loss: 19.1891 - dense_3_loss: 2.7133 - dense_3_acc: 0.0000e+00 - dense_3_acc_1: 0.5800 - dense_3_acc_2: 0.1550 - dense_3_acc_3: 0.0450 - dense_3_acc_4: 1.0000 - dense_3_acc_5: 0.0000e+00 - dense_3_acc_6: 0.0000e+00 - dense_3_acc_7: 1.0000 - dense_3_acc_8: 0.1250 - dense_3_acc_9: 0.10 - ETA: 16s - loss: 19.0569 - dense_3_loss: 2.6708 - dense_3_acc: 0.0000e+00 - dense_3_acc_1: 0.5933 - dense_3_acc_2: 0.1733 - dense_3_acc_3: 0.0367 - dense_3_acc_4: 1.0000 - dense_3_acc_5: 0.0000e+00 - dense_3_acc_6: 0.0000e+00 - dense_3_acc_7: 1.0000 - dense_3_acc_8: 0.1100 - dense_3_acc_9: 0.11 - ETA: 16s - loss: 18.9934 - dense_3_loss: 2.6647 - dense_3_acc: 0.0000e+00 - dense_3_acc_1: 0.5875 - dense_3_acc_2: 0.1850 - dense_3_acc_3: 0.0375 - dense_3_acc_4: 1.0000 - dense_3_acc_5: 0.0000e+00 - dense_3_acc_6: 0.0000e+00 - dense_3_acc_7: 1.0000 - dense_3_acc_8: 0.1025 - dense_3_acc_9: 0.12 - ETA: 16s - loss: 19.0149 - dense_3_loss: 2.6774 - dense_3_acc: 0.0000e+00 - dense_3_acc_1: 0.5940 - dense_3_acc_2: 0.1940 - dense_3_acc_3: 0.0300 - dense_3_acc_4: 1.0000 - dense_3_acc_5: 0.0000e+00 - dense_3_acc_6: 0.0000e+00 - dense_3_acc_7: 1.0000 - dense_3_acc_8: 0.1160 - dense_3_acc_9: 0.11 - ETA: 16s - loss: 18.9679 - dense_3_loss: 2.6755 - dense_3_acc: 0.0350 - dense_3_acc_1: 0.6017 - dense_3_acc_2: 0.2050 - dense_3_acc_3: 0.0333 - dense_3_acc_4: 1.0000 - dense_3_acc_5: 0.0000e+00 - dense_3_acc_6: 0.0000e+00 - dense_3_acc_7: 1.0000 - dense_3_acc_8: 0.1483 - dense_3_acc_9: 0.1150   - ETA: 15s - loss: 18.9676 - dense_3_loss: 2.6880 - dense_3_acc: 0.1057 - dense_3_acc_1: 0.6043 - dense_3_acc_2: 0.1986 - dense_3_acc_3: 0.0400 - dense_3_acc_4: 1.0000 - dense_3_acc_5: 0.0000e+00 - dense_3_acc_6: 0.0000e+00 - dense_3_acc_7: 1.0000 - dense_3_acc_8: 0.1743 - dense_3_acc_9: 0.10 - ETA: 15s - loss: 18.9600 - dense_3_loss: 2.6805 - dense_3_acc: 0.1800 - dense_3_acc_1: 0.6162 - dense_3_acc_2: 0.2087 - dense_3_acc_3: 0.0400 - dense_3_acc_4: 1.0000 - dense_3_acc_5: 0.0000e+00 - dense_3_acc_6: 0.0000e+00 - dense_3_acc_7: 1.0000 - dense_3_acc_8: 0.2075 - dense_3_acc_9: 0.11 - ETA: 15s - loss: 18.9389 - dense_3_loss: 2.6731 - dense_3_acc: 0.2333 - dense_3_acc_1: 0.6211 - dense_3_acc_2: 0.2111 - dense_3_acc_3: 0.0411 - dense_3_acc_4: 1.0000 - dense_3_acc_5: 0.0000e+00 - dense_3_acc_6: 0.0000e+00 - dense_3_acc_7: 1.0000 - dense_3_acc_8: 0.2267 - dense_3_acc_9: 0.12 - ETA: 15s - loss: 18.9158 - dense_3_loss: 2.6710 - dense_3_acc: 0.2650 - dense_3_acc_1: 0.6140 - dense_3_acc_2: 0.2160 - dense_3_acc_3: 0.0470 - dense_3_acc_4: 1.0000 - dense_3_acc_5: 0.0000e+00 - dense_3_acc_6: 0.0000e+00 - dense_3_acc_7: 1.0000 - dense_3_acc_8: 0.2320 - dense_3_acc_9: 0.13 - ETA: 14s - loss: 18.9201 - dense_3_loss: 2.6743 - dense_3_acc: 0.2927 - dense_3_acc_1: 0.6100 - dense_3_acc_2: 0.2109 - dense_3_acc_3: 0.0491 - dense_3_acc_4: 1.0000 - dense_3_acc_5: 0.0000e+00 - dense_3_acc_6: 0.0000e+00 - dense_3_acc_7: 1.0000 - dense_3_acc_8: 0.2391 - dense_3_acc_9: 0.12 - ETA: 14s - loss: 18.8930 - dense_3_loss: 2.6703 - dense_3_acc: 0.3242 - dense_3_acc_1: 0.6150 - dense_3_acc_2: 0.2133 - dense_3_acc_3: 0.0500 - dense_3_acc_4: 1.0000 - dense_3_acc_5: 0.0000e+00 - dense_3_acc_6: 0.0000e+00 - dense_3_acc_7: 1.0000 - dense_3_acc_8: 0.2433 - dense_3_acc_9: 0.12 - ETA: 14s - loss: 18.8939 - dense_3_loss: 2.6771 - dense_3_acc: 0.3415 - dense_3_acc_1: 0.6100 - dense_3_acc_2: 0.2138 - dense_3_acc_3: 0.0531 - dense_3_acc_4: 1.0000 - dense_3_acc_5: 0.0000e+00 - dense_3_acc_6: 0.0000e+00 - dense_3_acc_7: 1.0000 - dense_3_acc_8: 0.2438 - dense_3_acc_9: 0.12 - ETA: 14s - loss: 18.8818 - dense_3_loss: 2.6807 - dense_3_acc: 0.3607 - dense_3_acc_1: 0.6100 - dense_3_acc_2: 0.2157 - dense_3_acc_3: 0.0579 - dense_3_acc_4: 1.0000 - dense_3_acc_5: 0.0000e+00 - dense_3_acc_6: 0.0000e+00 - dense_3_acc_7: 1.0000 - dense_3_acc_8: 0.2557 - dense_3_acc_9: 0.12 - ETA: 13s - loss: 18.8717 - dense_3_loss: 2.6811 - dense_3_acc: 0.3747 - dense_3_acc_1: 0.6073 - dense_3_acc_2: 0.2160 - dense_3_acc_3: 0.0620 - dense_3_acc_4: 1.0000 - dense_3_acc_5: 0.0000e+00 - dense_3_acc_6: 0.0000e+00 - dense_3_acc_7: 1.0000 - dense_3_acc_8: 0.2580 - dense_3_acc_9: 0.12 - ETA: 13s - loss: 18.8586 - dense_3_loss: 2.6854 - dense_3_acc: 0.3981 - dense_3_acc_1: 0.6163 - dense_3_acc_2: 0.2169 - dense_3_acc_3: 0.0650 - dense_3_acc_4: 1.0000 - dense_3_acc_5: 0.0000e+00 - dense_3_acc_6: 0.0000e+00 - dense_3_acc_7: 1.0000 - dense_3_acc_8: 0.2550 - dense_3_acc_9: 0.12 - ETA: 13s - loss: 18.8449 - dense_3_loss: 2.6874 - dense_3_acc: 0.4124 - dense_3_acc_1: 0.6176 - dense_3_acc_2: 0.2165 - dense_3_acc_3: 0.0653 - dense_3_acc_4: 1.0000 - dense_3_acc_5: 0.0000e+00 - dense_3_acc_6: 0.0000e+00 - dense_3_acc_7: 1.0000 - dense_3_acc_8: 0.2559 - dense_3_acc_9: 0.12 - ETA: 13s - loss: 18.8363 - dense_3_loss: 2.6857 - dense_3_acc: 0.4200 - dense_3_acc_1: 0.6139 - dense_3_acc_2: 0.2144 - dense_3_acc_3: 0.0694 - dense_3_acc_4: 1.0000 - dense_3_acc_5: 0.0000e+00 - dense_3_acc_6: 0.0000e+00 - dense_3_acc_7: 1.0000 - dense_3_acc_8: 0.2533 - dense_3_acc_9: 0.12 - ETA: 13s - loss: 18.8300 - dense_3_loss: 2.6868 - dense_3_acc: 0.4295 - dense_3_acc_1: 0.6132 - dense_3_acc_2: 0.2184 - dense_3_acc_3: 0.0700 - dense_3_acc_4: 1.0000 - dense_3_acc_5: 0.0000e+00 - dense_3_acc_6: 0.0000e+00 - dense_3_acc_7: 1.0000 - dense_3_acc_8: 0.2474 - dense_3_acc_9: 0.12 - ETA: 12s - loss: 18.8242 - dense_3_loss: 2.6937 - dense_3_acc: 0.4425 - dense_3_acc_1: 0.6170 - dense_3_acc_2: 0.2210 - dense_3_acc_3: 0.0705 - dense_3_acc_4: 1.0000 - dense_3_acc_5: 0.0000e+00 - dense_3_acc_6: 0.0000e+00 - dense_3_acc_7: 1.0000 - dense_3_acc_8: 0.2390 - dense_3_acc_9: 0.12 - ETA: 12s - loss: 18.8303 - dense_3_loss: 2.6923 - dense_3_acc: 0.4448 - dense_3_acc_1: 0.6110 - dense_3_acc_2: 0.2195 - dense_3_acc_3: 0.0719 - dense_3_acc_4: 1.0000 - dense_3_acc_5: 0.0000e+00 - dense_3_acc_6: 0.0000e+00 - dense_3_acc_7: 1.0000 - dense_3_acc_8: 0.2290 - dense_3_acc_9: 0.12 - ETA: 12s - loss: 18.8253 - dense_3_loss: 2.6886 - dense_3_acc: 0.4536 - dense_3_acc_1: 0.6123 - dense_3_acc_2: 0.2182 - dense_3_acc_3: 0.0732 - dense_3_acc_4: 1.0000 - dense_3_acc_5: 0.0000e+00 - dense_3_acc_6: 0.0000e+00 - dense_3_acc_7: 1.0000 - dense_3_acc_8: 0.2218 - dense_3_acc_9: 0.12 - ETA: 12s - loss: 18.8233 - dense_3_loss: 2.6926 - dense_3_acc: 0.4583 - dense_3_acc_1: 0.6100 - dense_3_acc_2: 0.2183 - dense_3_acc_3: 0.0743 - dense_3_acc_4: 1.0000 - dense_3_acc_5: 0.0000e+00 - dense_3_acc_6: 0.0000e+00 - dense_3_acc_7: 1.0000 - dense_3_acc_8: 0.2143 - dense_3_acc_9: 0.12 - ETA: 12s - loss: 18.8246 - dense_3_loss: 2.6943 - dense_3_acc: 0.4608 - dense_3_acc_1: 0.6063 - dense_3_acc_2: 0.2187 - dense_3_acc_3: 0.0742 - dense_3_acc_4: 1.0000 - dense_3_acc_5: 0.0000e+00 - dense_3_acc_6: 0.0000e+00 - dense_3_acc_7: 1.0000 - dense_3_acc_8: 0.2096 - dense_3_acc_9: 0.12 - ETA: 12s - loss: 18.8214 - dense_3_loss: 2.6954 - dense_3_acc: 0.4668 - dense_3_acc_1: 0.6064 - dense_3_acc_2: 0.2236 - dense_3_acc_3: 0.0740 - dense_3_acc_4: 1.0000 - dense_3_acc_5: 0.0000e+00 - dense_3_acc_6: 0.0000e+00 - dense_3_acc_7: 1.0000 - dense_3_acc_8: 0.2092 - dense_3_acc_9: 0.12 - ETA: 11s - loss: 18.8179 - dense_3_loss: 2.6991 - dense_3_acc: 0.4727 - dense_3_acc_1: 0.6069 - dense_3_acc_2: 0.2265 - dense_3_acc_3: 0.0746 - dense_3_acc_4: 0.9996 - dense_3_acc_5: 0.0000e+00 - dense_3_acc_6: 0.0000e+00 - dense_3_acc_7: 1.0000 - dense_3_acc_8: 0.2104 - dense_3_acc_9: 0.12 - ETA: 11s - loss: 18.8138 - dense_3_loss: 2.6985 - dense_3_acc: 0.4763 - dense_3_acc_1: 0.6056 - dense_3_acc_2: 0.2296 - dense_3_acc_3: 0.0756 - dense_3_acc_4: 0.9996 - dense_3_acc_5: 0.0000e+00 - dense_3_acc_6: 0.0000e+00 - dense_3_acc_7: 1.0000 - dense_3_acc_8: 0.2144 - dense_3_acc_9: 0.12 - ETA: 11s - loss: 18.8088 - dense_3_loss: 2.6998 - dense_3_acc: 0.4825 - dense_3_acc_1: 0.6071 - dense_3_acc_2: 0.2318 - dense_3_acc_3: 0.0757 - dense_3_acc_4: 0.9996 - dense_3_acc_5: 0.0000e+00 - dense_3_acc_6: 0.0000e+00 - dense_3_acc_7: 1.0000 - dense_3_acc_8: 0.2146 - dense_3_acc_9: 0.1196 5600/10000 [===============>..............] - ETA: 11s - loss: 18.7999 - dense_3_loss: 2.6965 - dense_3_acc: 0.4886 - dense_3_acc_1: 0.6090 - dense_3_acc_2: 0.2307 - dense_3_acc_3: 0.0769 - dense_3_acc_4: 0.9997 - dense_3_acc_5: 0.0000e+00 - dense_3_acc_6: 0.0000e+00 - dense_3_acc_7: 1.0000 - dense_3_acc_8: 0.2131 - dense_3_acc_9: 0.12 - ETA: 11s - loss: 18.8010 - dense_3_loss: 2.6964 - dense_3_acc: 0.4893 - dense_3_acc_1: 0.6057 - dense_3_acc_2: 0.2313 - dense_3_acc_3: 0.0763 - dense_3_acc_4: 0.9997 - dense_3_acc_5: 0.0000e+00 - dense_3_acc_6: 0.0000e+00 - dense_3_acc_7: 1.0000 - dense_3_acc_8: 0.2093 - dense_3_acc_9: 0.11 - ETA: 11s - loss: 18.7985 - dense_3_loss: 2.6984 - dense_3_acc: 0.4910 - dense_3_acc_1: 0.6035 - dense_3_acc_2: 0.2316 - dense_3_acc_3: 0.0755 - dense_3_acc_4: 0.9997 - dense_3_acc_5: 0.0000e+00 - dense_3_acc_6: 0.0000e+00 - dense_3_acc_7: 1.0000 - dense_3_acc_8: 0.2068 - dense_3_acc_9: 0.11 - ETA: 10s - loss: 18.8020 - dense_3_loss: 2.7042 - dense_3_acc: 0.4916 - dense_3_acc_1: 0.6006 - dense_3_acc_2: 0.2319 - dense_3_acc_3: 0.0747 - dense_3_acc_4: 0.9997 - dense_3_acc_5: 0.0000e+00 - dense_3_acc_6: 0.0000e+00 - dense_3_acc_7: 1.0000 - dense_3_acc_8: 0.2053 - dense_3_acc_9: 0.11 - ETA: 10s - loss: 18.7904 - dense_3_loss: 2.7003 - dense_3_acc: 0.4961 - dense_3_acc_1: 0.6018 - dense_3_acc_2: 0.2348 - dense_3_acc_3: 0.0736 - dense_3_acc_4: 0.9997 - dense_3_acc_5: 0.0000e+00 - dense_3_acc_6: 0.0000e+00 - dense_3_acc_7: 1.0000 - dense_3_acc_8: 0.2055 - dense_3_acc_9: 0.11 - ETA: 10s - loss: 18.7793 - dense_3_loss: 2.6979 - dense_3_acc: 0.5035 - dense_3_acc_1: 0.6062 - dense_3_acc_2: 0.2368 - dense_3_acc_3: 0.0724 - dense_3_acc_4: 0.9997 - dense_3_acc_5: 0.0000e+00 - dense_3_acc_6: 0.0000e+00 - dense_3_acc_7: 1.0000 - dense_3_acc_8: 0.2041 - dense_3_acc_9: 0.11 - ETA: 10s - loss: 18.7744 - dense_3_loss: 2.6990 - dense_3_acc: 0.5071 - dense_3_acc_1: 0.6069 - dense_3_acc_2: 0.2369 - dense_3_acc_3: 0.0714 - dense_3_acc_4: 0.9997 - dense_3_acc_5: 0.0000e+00 - dense_3_acc_6: 0.0000e+00 - dense_3_acc_7: 1.0000 - dense_3_acc_8: 0.2049 - dense_3_acc_9: 0.11 - ETA: 10s - loss: 18.7696 - dense_3_loss: 2.6968 - dense_3_acc: 0.5100 - dense_3_acc_1: 0.6069 - dense_3_acc_2: 0.2392 - dense_3_acc_3: 0.0708 - dense_3_acc_4: 0.9997 - dense_3_acc_5: 0.0000e+00 - dense_3_acc_6: 0.0000e+00 - dense_3_acc_7: 1.0000 - dense_3_acc_8: 0.2042 - dense_3_acc_9: 0.11 - ETA: 10s - loss: 18.7673 - dense_3_loss: 2.6975 - dense_3_acc: 0.5122 - dense_3_acc_1: 0.6065 - dense_3_acc_2: 0.2389 - dense_3_acc_3: 0.0700 - dense_3_acc_4: 0.9997 - dense_3_acc_5: 0.0000e+00 - dense_3_acc_6: 0.0000e+00 - dense_3_acc_7: 1.0000 - dense_3_acc_8: 0.2032 - dense_3_acc_9: 0.11 - ETA: 9s - loss: 18.7615 - dense_3_loss: 2.7003 - dense_3_acc: 0.5147 - dense_3_acc_1: 0.6066 - dense_3_acc_2: 0.2389 - dense_3_acc_3: 0.0695 - dense_3_acc_4: 0.9997 - dense_3_acc_5: 0.0000e+00 - dense_3_acc_6: 0.0000e+00 - dense_3_acc_7: 1.0000 - dense_3_acc_8: 0.2034 - dense_3_acc_9: 0.1158 - ETA: 9s - loss: 18.7497 - dense_3_loss: 2.6983 - dense_3_acc: 0.5192 - dense_3_acc_1: 0.6087 - dense_3_acc_2: 0.2379 - dense_3_acc_3: 0.0692 - dense_3_acc_4: 0.9997 - dense_3_acc_5: 0.0000e+00 - dense_3_acc_6: 0.0000e+00 - dense_3_acc_7: 1.0000 - dense_3_acc_8: 0.2023 - dense_3_acc_9: 0.116 - ETA: 9s - loss: 18.7471 - dense_3_loss: 2.6968 - dense_3_acc: 0.5225 - dense_3_acc_1: 0.6097 - dense_3_acc_2: 0.2370 - dense_3_acc_3: 0.0682 - dense_3_acc_4: 0.9998 - dense_3_acc_5: 0.0000e+00 - dense_3_acc_6: 0.0000e+00 - dense_3_acc_7: 1.0000 - dense_3_acc_8: 0.2015 - dense_3_acc_9: 0.114 - ETA: 9s - loss: 18.7401 - dense_3_loss: 2.6951 - dense_3_acc: 0.5237 - dense_3_acc_1: 0.6088 - dense_3_acc_2: 0.2385 - dense_3_acc_3: 0.0688 - dense_3_acc_4: 0.9998 - dense_3_acc_5: 0.0000e+00 - dense_3_acc_6: 0.0000e+00 - dense_3_acc_7: 1.0000 - dense_3_acc_8: 0.2022 - dense_3_acc_9: 0.114 - ETA: 9s - loss: 18.7331 - dense_3_loss: 2.6917 - dense_3_acc: 0.5252 - dense_3_acc_1: 0.6083 - dense_3_acc_2: 0.2388 - dense_3_acc_3: 0.0690 - dense_3_acc_4: 0.9998 - dense_3_acc_5: 0.0000e+00 - dense_3_acc_6: 0.0000e+00 - dense_3_acc_7: 1.0000 - dense_3_acc_8: 0.2036 - dense_3_acc_9: 0.116 - ETA: 9s - loss: 18.7250 - dense_3_loss: 2.6922 - dense_3_acc: 0.5279 - dense_3_acc_1: 0.6091 - dense_3_acc_2: 0.2391 - dense_3_acc_3: 0.0686 - dense_3_acc_4: 0.9998 - dense_3_acc_5: 0.0000e+00 - dense_3_acc_6: 0.0000e+00 - dense_3_acc_7: 1.0000 - dense_3_acc_8: 0.2051 - dense_3_acc_9: 0.116 - ETA: 8s - loss: 18.7178 - dense_3_loss: 2.6915 - dense_3_acc: 0.5300 - dense_3_acc_1: 0.6093 - dense_3_acc_2: 0.2416 - dense_3_acc_3: 0.0682 - dense_3_acc_4: 0.9998 - dense_3_acc_5: 0.0000e+00 - dense_3_acc_6: 0.0000e+00 - dense_3_acc_7: 1.0000 - dense_3_acc_8: 0.2077 - dense_3_acc_9: 0.117 - ETA: 8s - loss: 18.7151 - dense_3_loss: 2.6897 - dense_3_acc: 0.5311 - dense_3_acc_1: 0.6087 - dense_3_acc_2: 0.2420 - dense_3_acc_3: 0.0678 - dense_3_acc_4: 0.9998 - dense_3_acc_5: 0.0000e+00 - dense_3_acc_6: 0.0000e+00 - dense_3_acc_7: 1.0000 - dense_3_acc_8: 0.2067 - dense_3_acc_9: 0.118 - ETA: 8s - loss: 18.7122 - dense_3_loss: 2.6891 - dense_3_acc: 0.5317 - dense_3_acc_1: 0.6076 - dense_3_acc_2: 0.2426 - dense_3_acc_3: 0.0672 - dense_3_acc_4: 0.9998 - dense_3_acc_5: 0.0000e+00 - dense_3_acc_6: 0.0000e+00 - dense_3_acc_7: 1.0000 - dense_3_acc_8: 0.2052 - dense_3_acc_9: 0.117 - ETA: 8s - loss: 18.7048 - dense_3_loss: 2.6877 - dense_3_acc: 0.5336 - dense_3_acc_1: 0.6079 - dense_3_acc_2: 0.2432 - dense_3_acc_3: 0.0674 - dense_3_acc_4: 0.9998 - dense_3_acc_5: 0.0000e+00 - dense_3_acc_6: 0.0000e+00 - dense_3_acc_7: 1.0000 - dense_3_acc_8: 0.2051 - dense_3_acc_9: 0.118 - ETA: 8s - loss: 18.6992 - dense_3_loss: 2.6868 - dense_3_acc: 0.5362 - dense_3_acc_1: 0.6090 - dense_3_acc_2: 0.2454 - dense_3_acc_3: 0.0669 - dense_3_acc_4: 0.9998 - dense_3_acc_5: 0.0000e+00 - dense_3_acc_6: 0.0000e+00 - dense_3_acc_7: 1.0000 - dense_3_acc_8: 0.2054 - dense_3_acc_9: 0.119 - ETA: 8s - loss: 18.6976 - dense_3_loss: 2.6871 - dense_3_acc: 0.5363 - dense_3_acc_1: 0.6076 - dense_3_acc_2: 0.2467 - dense_3_acc_3: 0.0669 - dense_3_acc_4: 0.9998 - dense_3_acc_5: 0.0000e+00 - dense_3_acc_6: 0.0000e+00 - dense_3_acc_7: 1.0000 - dense_3_acc_8: 0.2071 - dense_3_acc_9: 0.118 - ETA: 8s - loss: 18.6886 - dense_3_loss: 2.6839 - dense_3_acc: 0.5382 - dense_3_acc_1: 0.6080 - dense_3_acc_2: 0.2486 - dense_3_acc_3: 0.0676 - dense_3_acc_4: 0.9998 - dense_3_acc_5: 0.0000e+00 - dense_3_acc_6: 0.0000e+00 - dense_3_acc_7: 1.0000 - dense_3_acc_8: 0.2082 - dense_3_acc_9: 0.119 - ETA: 7s - loss: 18.6799 - dense_3_loss: 2.6820 - dense_3_acc: 0.5378 - dense_3_acc_1: 0.6063 - dense_3_acc_2: 0.2518 - dense_3_acc_3: 0.0671 - dense_3_acc_4: 0.9998 - dense_3_acc_5: 0.0000e+00 - dense_3_acc_6: 0.0000e+00 - dense_3_acc_7: 1.0000 - dense_3_acc_8: 0.2092 - dense_3_acc_9: 0.119 - ETA: 7s - loss: 18.6721 - dense_3_loss: 2.6821 - dense_3_acc: 0.5404 - dense_3_acc_1: 0.6075 - dense_3_acc_2: 0.2535 - dense_3_acc_3: 0.0669 - dense_3_acc_4: 0.9998 - dense_3_acc_5: 0.0000e+00 - dense_3_acc_6: 0.0000e+00 - dense_3_acc_7: 1.0000 - dense_3_acc_8: 0.2096 - dense_3_acc_9: 0.120 - ETA: 7s - loss: 18.6642 - dense_3_loss: 2.6820 - dense_3_acc: 0.5423 - dense_3_acc_1: 0.6081 - dense_3_acc_2: 0.2538 - dense_3_acc_3: 0.0679 - dense_3_acc_4: 0.9998 - dense_3_acc_5: 0.0000e+00 - dense_3_acc_6: 0.0000e+00 - dense_3_acc_7: 1.0000 - dense_3_acc_8: 0.2098 - dense_3_acc_9: 0.119 - ETA: 7s - loss: 18.6579 - dense_3_loss: 2.6807 - dense_3_acc: 0.5430 - dense_3_acc_1: 0.6076 - dense_3_acc_2: 0.2543 - dense_3_acc_3: 0.0685 - dense_3_acc_4: 0.9998 - dense_3_acc_5: 0.0000e+00 - dense_3_acc_6: 0.0000e+00 - dense_3_acc_7: 1.0000 - dense_3_acc_8: 0.2094 - dense_3_acc_9: 0.119 - ETA: 7s - loss: 18.6525 - dense_3_loss: 2.6799 - dense_3_acc: 0.5445 - dense_3_acc_1: 0.6080 - dense_3_acc_2: 0.2547 - dense_3_acc_3: 0.0689 - dense_3_acc_4: 0.9998 - dense_3_acc_5: 0.0000e+00 - dense_3_acc_6: 0.0000e+00 - dense_3_acc_7: 1.0000 - dense_3_acc_8: 0.2084 - dense_3_acc_9: 0.120 - ETA: 7s - loss: 18.6483 - dense_3_loss: 2.6804 - dense_3_acc: 0.5452 - dense_3_acc_1: 0.6075 - dense_3_acc_2: 0.2546 - dense_3_acc_3: 0.0684 - dense_3_acc_4: 0.9998 - dense_3_acc_5: 0.0000e+00 - dense_3_acc_6: 0.0000e+00 - dense_3_acc_7: 1.0000 - dense_3_acc_8: 0.2084 - dense_3_acc_9: 0.1195"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 8400/10000 [========================>.....] - ETA: 6s - loss: 18.6458 - dense_3_loss: 2.6803 - dense_3_acc: 0.5454 - dense_3_acc_1: 0.6067 - dense_3_acc_2: 0.2540 - dense_3_acc_3: 0.0679 - dense_3_acc_4: 0.9998 - dense_3_acc_5: 0.0000e+00 - dense_3_acc_6: 0.0000e+00 - dense_3_acc_7: 1.0000 - dense_3_acc_8: 0.2084 - dense_3_acc_9: 0.118 - ETA: 6s - loss: 18.6423 - dense_3_loss: 2.6817 - dense_3_acc: 0.5462 - dense_3_acc_1: 0.6064 - dense_3_acc_2: 0.2547 - dense_3_acc_3: 0.0676 - dense_3_acc_4: 0.9998 - dense_3_acc_5: 0.0000e+00 - dense_3_acc_6: 0.0000e+00 - dense_3_acc_7: 1.0000 - dense_3_acc_8: 0.2090 - dense_3_acc_9: 0.118 - ETA: 6s - loss: 18.6317 - dense_3_loss: 2.6818 - dense_3_acc: 0.5492 - dense_3_acc_1: 0.6083 - dense_3_acc_2: 0.2551 - dense_3_acc_3: 0.0673 - dense_3_acc_4: 0.9998 - dense_3_acc_5: 0.0000e+00 - dense_3_acc_6: 0.0000e+00 - dense_3_acc_7: 1.0000 - dense_3_acc_8: 0.2103 - dense_3_acc_9: 0.119 - ETA: 6s - loss: 18.6265 - dense_3_loss: 2.6811 - dense_3_acc: 0.5502 - dense_3_acc_1: 0.6083 - dense_3_acc_2: 0.2550 - dense_3_acc_3: 0.0682 - dense_3_acc_4: 0.9998 - dense_3_acc_5: 0.0000e+00 - dense_3_acc_6: 0.0000e+00 - dense_3_acc_7: 1.0000 - dense_3_acc_8: 0.2105 - dense_3_acc_9: 0.119 - ETA: 6s - loss: 18.6235 - dense_3_loss: 2.6804 - dense_3_acc: 0.5505 - dense_3_acc_1: 0.6077 - dense_3_acc_2: 0.2543 - dense_3_acc_3: 0.0675 - dense_3_acc_4: 0.9998 - dense_3_acc_5: 0.0000e+00 - dense_3_acc_6: 0.0000e+00 - dense_3_acc_7: 1.0000 - dense_3_acc_8: 0.2105 - dense_3_acc_9: 0.119 - ETA: 6s - loss: 18.6202 - dense_3_loss: 2.6804 - dense_3_acc: 0.5498 - dense_3_acc_1: 0.6061 - dense_3_acc_2: 0.2539 - dense_3_acc_3: 0.0679 - dense_3_acc_4: 0.9998 - dense_3_acc_5: 0.0000e+00 - dense_3_acc_6: 0.0000e+00 - dense_3_acc_7: 1.0000 - dense_3_acc_8: 0.2110 - dense_3_acc_9: 0.118 - ETA: 5s - loss: 18.6148 - dense_3_loss: 2.6810 - dense_3_acc: 0.5516 - dense_3_acc_1: 0.6070 - dense_3_acc_2: 0.2543 - dense_3_acc_3: 0.0675 - dense_3_acc_4: 0.9998 - dense_3_acc_5: 0.0000e+00 - dense_3_acc_6: 0.0000e+00 - dense_3_acc_7: 1.0000 - dense_3_acc_8: 0.2100 - dense_3_acc_9: 0.118 - ETA: 5s - loss: 18.6063 - dense_3_loss: 2.6801 - dense_3_acc: 0.5531 - dense_3_acc_1: 0.6077 - dense_3_acc_2: 0.2558 - dense_3_acc_3: 0.0677 - dense_3_acc_4: 0.9998 - dense_3_acc_5: 0.0000e+00 - dense_3_acc_6: 0.0000e+00 - dense_3_acc_7: 1.0000 - dense_3_acc_8: 0.2097 - dense_3_acc_9: 0.118 - ETA: 5s - loss: 18.6020 - dense_3_loss: 2.6794 - dense_3_acc: 0.5554 - dense_3_acc_1: 0.6091 - dense_3_acc_2: 0.2565 - dense_3_acc_3: 0.0671 - dense_3_acc_4: 0.9998 - dense_3_acc_5: 0.0000e+00 - dense_3_acc_6: 0.0000e+00 - dense_3_acc_7: 1.0000 - dense_3_acc_8: 0.2091 - dense_3_acc_9: 0.118 - ETA: 5s - loss: 18.5974 - dense_3_loss: 2.6798 - dense_3_acc: 0.5556 - dense_3_acc_1: 0.6085 - dense_3_acc_2: 0.2567 - dense_3_acc_3: 0.0668 - dense_3_acc_4: 0.9998 - dense_3_acc_5: 0.0000e+00 - dense_3_acc_6: 0.0000e+00 - dense_3_acc_7: 1.0000 - dense_3_acc_8: 0.2086 - dense_3_acc_9: 0.118 - ETA: 5s - loss: 18.5929 - dense_3_loss: 2.6812 - dense_3_acc: 0.5561 - dense_3_acc_1: 0.6082 - dense_3_acc_2: 0.2573 - dense_3_acc_3: 0.0666 - dense_3_acc_4: 0.9999 - dense_3_acc_5: 0.0000e+00 - dense_3_acc_6: 0.0000e+00 - dense_3_acc_7: 1.0000 - dense_3_acc_8: 0.2072 - dense_3_acc_9: 0.118 - ETA: 5s - loss: 18.5848 - dense_3_loss: 2.6810 - dense_3_acc: 0.5574 - dense_3_acc_1: 0.6087 - dense_3_acc_2: 0.2582 - dense_3_acc_3: 0.0662 - dense_3_acc_4: 0.9999 - dense_3_acc_5: 0.0000e+00 - dense_3_acc_6: 0.0000e+00 - dense_3_acc_7: 1.0000 - dense_3_acc_8: 0.2053 - dense_3_acc_9: 0.117 - ETA: 5s - loss: 18.5788 - dense_3_loss: 2.6804 - dense_3_acc: 0.5571 - dense_3_acc_1: 0.6077 - dense_3_acc_2: 0.2581 - dense_3_acc_3: 0.0671 - dense_3_acc_4: 0.9999 - dense_3_acc_5: 0.0000e+00 - dense_3_acc_6: 0.0000e+00 - dense_3_acc_7: 1.0000 - dense_3_acc_8: 0.2049 - dense_3_acc_9: 0.117 - ETA: 4s - loss: 18.5747 - dense_3_loss: 2.6785 - dense_3_acc: 0.5574 - dense_3_acc_1: 0.6073 - dense_3_acc_2: 0.2594 - dense_3_acc_3: 0.0667 - dense_3_acc_4: 0.9999 - dense_3_acc_5: 0.0000e+00 - dense_3_acc_6: 0.0000e+00 - dense_3_acc_7: 1.0000 - dense_3_acc_8: 0.2037 - dense_3_acc_9: 0.118 - ETA: 4s - loss: 18.5699 - dense_3_loss: 2.6785 - dense_3_acc: 0.5586 - dense_3_acc_1: 0.6077 - dense_3_acc_2: 0.2597 - dense_3_acc_3: 0.0668 - dense_3_acc_4: 0.9999 - dense_3_acc_5: 0.0000e+00 - dense_3_acc_6: 0.0000e+00 - dense_3_acc_7: 1.0000 - dense_3_acc_8: 0.2056 - dense_3_acc_9: 0.118 - ETA: 4s - loss: 18.5620 - dense_3_loss: 2.6780 - dense_3_acc: 0.5596 - dense_3_acc_1: 0.6081 - dense_3_acc_2: 0.2618 - dense_3_acc_3: 0.0668 - dense_3_acc_4: 0.9999 - dense_3_acc_5: 0.0000e+00 - dense_3_acc_6: 0.0000e+00 - dense_3_acc_7: 1.0000 - dense_3_acc_8: 0.2085 - dense_3_acc_9: 0.117 - ETA: 4s - loss: 18.5531 - dense_3_loss: 2.6750 - dense_3_acc: 0.5604 - dense_3_acc_1: 0.6082 - dense_3_acc_2: 0.2629 - dense_3_acc_3: 0.0666 - dense_3_acc_4: 0.9999 - dense_3_acc_5: 0.0000e+00 - dense_3_acc_6: 0.0000e+00 - dense_3_acc_7: 1.0000 - dense_3_acc_8: 0.2115 - dense_3_acc_9: 0.118 - ETA: 4s - loss: 18.5469 - dense_3_loss: 2.6737 - dense_3_acc: 0.5605 - dense_3_acc_1: 0.6077 - dense_3_acc_2: 0.2643 - dense_3_acc_3: 0.0661 - dense_3_acc_4: 0.9999 - dense_3_acc_5: 0.0000e+00 - dense_3_acc_6: 0.0000e+00 - dense_3_acc_7: 1.0000 - dense_3_acc_8: 0.2139 - dense_3_acc_9: 0.119 - ETA: 4s - loss: 18.5373 - dense_3_loss: 2.6721 - dense_3_acc: 0.5624 - dense_3_acc_1: 0.6089 - dense_3_acc_2: 0.2645 - dense_3_acc_3: 0.0667 - dense_3_acc_4: 0.9999 - dense_3_acc_5: 0.0000e+00 - dense_3_acc_6: 0.0000e+00 - dense_3_acc_7: 1.0000 - dense_3_acc_8: 0.2152 - dense_3_acc_9: 0.119 - ETA: 3s - loss: 18.5304 - dense_3_loss: 2.6703 - dense_3_acc: 0.5637 - dense_3_acc_1: 0.6096 - dense_3_acc_2: 0.2649 - dense_3_acc_3: 0.0663 - dense_3_acc_4: 0.9999 - dense_3_acc_5: 0.0000e+00 - dense_3_acc_6: 0.0000e+00 - dense_3_acc_7: 1.0000 - dense_3_acc_8: 0.2184 - dense_3_acc_9: 0.119 - ETA: 3s - loss: 18.5239 - dense_3_loss: 2.6702 - dense_3_acc: 0.5651 - dense_3_acc_1: 0.6104 - dense_3_acc_2: 0.2649 - dense_3_acc_3: 0.0661 - dense_3_acc_4: 0.9999 - dense_3_acc_5: 0.0000e+00 - dense_3_acc_6: 0.0000e+00 - dense_3_acc_7: 1.0000 - dense_3_acc_8: 0.2203 - dense_3_acc_9: 0.119 - ETA: 3s - loss: 18.5181 - dense_3_loss: 2.6701 - dense_3_acc: 0.5655 - dense_3_acc_1: 0.6103 - dense_3_acc_2: 0.2655 - dense_3_acc_3: 0.0660 - dense_3_acc_4: 0.9999 - dense_3_acc_5: 0.0000e+00 - dense_3_acc_6: 0.0000e+00 - dense_3_acc_7: 1.0000 - dense_3_acc_8: 0.2200 - dense_3_acc_9: 0.119 - ETA: 3s - loss: 18.5107 - dense_3_loss: 2.6696 - dense_3_acc: 0.5666 - dense_3_acc_1: 0.6108 - dense_3_acc_2: 0.2651 - dense_3_acc_3: 0.0661 - dense_3_acc_4: 0.9999 - dense_3_acc_5: 0.0000e+00 - dense_3_acc_6: 0.0000e+00 - dense_3_acc_7: 1.0000 - dense_3_acc_8: 0.2220 - dense_3_acc_9: 0.120 - ETA: 3s - loss: 18.5045 - dense_3_loss: 2.6680 - dense_3_acc: 0.5667 - dense_3_acc_1: 0.6104 - dense_3_acc_2: 0.2659 - dense_3_acc_3: 0.0660 - dense_3_acc_4: 0.9999 - dense_3_acc_5: 0.0000e+00 - dense_3_acc_6: 0.0000e+00 - dense_3_acc_7: 1.0000 - dense_3_acc_8: 0.2244 - dense_3_acc_9: 0.120 - ETA: 3s - loss: 18.4998 - dense_3_loss: 2.6685 - dense_3_acc: 0.5678 - dense_3_acc_1: 0.6109 - dense_3_acc_2: 0.2675 - dense_3_acc_3: 0.0653 - dense_3_acc_4: 0.9999 - dense_3_acc_5: 0.0000e+00 - dense_3_acc_6: 0.0000e+00 - dense_3_acc_7: 1.0000 - dense_3_acc_8: 0.2267 - dense_3_acc_9: 0.120 - ETA: 2s - loss: 18.4961 - dense_3_loss: 2.6690 - dense_3_acc: 0.5677 - dense_3_acc_1: 0.6102 - dense_3_acc_2: 0.2680 - dense_3_acc_3: 0.0651 - dense_3_acc_4: 0.9999 - dense_3_acc_5: 0.0000e+00 - dense_3_acc_6: 0.0000e+00 - dense_3_acc_7: 1.0000 - dense_3_acc_8: 0.2277 - dense_3_acc_9: 0.120 - ETA: 2s - loss: 18.4898 - dense_3_loss: 2.6685 - dense_3_acc: 0.5690 - dense_3_acc_1: 0.6111 - dense_3_acc_2: 0.2689 - dense_3_acc_3: 0.0653 - dense_3_acc_4: 0.9999 - dense_3_acc_5: 0.0000e+00 - dense_3_acc_6: 0.0000e+00 - dense_3_acc_7: 1.0000 - dense_3_acc_8: 0.2294 - dense_3_acc_9: 0.120 - ETA: 2s - loss: 18.4854 - dense_3_loss: 2.6686 - dense_3_acc: 0.5688 - dense_3_acc_1: 0.6104 - dense_3_acc_2: 0.2689 - dense_3_acc_3: 0.0652 - dense_3_acc_4: 0.9999 - dense_3_acc_5: 0.0000e+00 - dense_3_acc_6: 0.0000e+00 - dense_3_acc_7: 1.0000 - dense_3_acc_8: 0.2315 - dense_3_acc_9: 0.1204"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-26-160abad11b3a>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mmodel\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mXoh\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0ms0\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mc0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0moutputs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mepochs\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m100\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m100\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32mD:\\Anaconda3\\lib\\site-packages\\keras\\engine\\training.py\u001b[0m in \u001b[0;36mfit\u001b[1;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, **kwargs)\u001b[0m\n\u001b[0;32m   1037\u001b[0m                                         \u001b[0minitial_epoch\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0minitial_epoch\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1038\u001b[0m                                         \u001b[0msteps_per_epoch\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0msteps_per_epoch\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1039\u001b[1;33m                                         validation_steps=validation_steps)\n\u001b[0m\u001b[0;32m   1040\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1041\u001b[0m     def evaluate(self, x=None, y=None,\n",
      "\u001b[1;32mD:\\Anaconda3\\lib\\site-packages\\keras\\engine\\training_arrays.py\u001b[0m in \u001b[0;36mfit_loop\u001b[1;34m(model, f, ins, out_labels, batch_size, epochs, verbose, callbacks, val_f, val_ins, shuffle, callback_metrics, initial_epoch, steps_per_epoch, validation_steps)\u001b[0m\n\u001b[0;32m    197\u001b[0m                     \u001b[0mins_batch\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mi\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mins_batch\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mi\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtoarray\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    198\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 199\u001b[1;33m                 \u001b[0mouts\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mf\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mins_batch\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    200\u001b[0m                 \u001b[0mouts\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mto_list\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mouts\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    201\u001b[0m                 \u001b[1;32mfor\u001b[0m \u001b[0ml\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mo\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mzip\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mout_labels\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mouts\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mD:\\Anaconda3\\lib\\site-packages\\keras\\backend\\tensorflow_backend.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, inputs)\u001b[0m\n\u001b[0;32m   2713\u001b[0m                 \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_legacy_call\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2714\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 2715\u001b[1;33m             \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_call\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   2716\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2717\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[0mpy_any\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mis_tensor\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mx\u001b[0m \u001b[1;32min\u001b[0m \u001b[0minputs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mD:\\Anaconda3\\lib\\site-packages\\keras\\backend\\tensorflow_backend.py\u001b[0m in \u001b[0;36m_call\u001b[1;34m(self, inputs)\u001b[0m\n\u001b[0;32m   2673\u001b[0m             \u001b[0mfetched\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_callable_fn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0marray_vals\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mrun_metadata\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrun_metadata\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2674\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 2675\u001b[1;33m             \u001b[0mfetched\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_callable_fn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0marray_vals\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   2676\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0mfetched\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m:\u001b[0m\u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0moutputs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2677\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mD:\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\client\\session.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1437\u001b[0m           ret = tf_session.TF_SessionRunCallable(\n\u001b[0;32m   1438\u001b[0m               \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_session\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_session\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_handle\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mstatus\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1439\u001b[1;33m               run_metadata_ptr)\n\u001b[0m\u001b[0;32m   1440\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mrun_metadata\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1441\u001b[0m           \u001b[0mproto_data\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtf_session\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mTF_GetBuffer\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mrun_metadata_ptr\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "model.fit([Xoh, s0, c0], outputs, epochs=100, batch_size=100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.load_weights('models/model.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "source: 3 May 1979\n",
      "output: 1979-05-03\n",
      "source: 5 April 09\n",
      "output: 2009-05-05\n",
      "source: 21th of August 2016\n",
      "output: 2016-08-21\n",
      "source: Tue 10 Jul 2007\n",
      "output: 2007-07-10\n",
      "source: Saturday May 9 2018\n",
      "output: 2018-05-09\n",
      "source: March 3 2001\n",
      "output: 2001-03-03\n",
      "source: March 3rd 2001\n",
      "output: 2001-03-03\n",
      "source: 1 March 2001\n",
      "output: 2001-03-01\n"
     ]
    }
   ],
   "source": [
    "EXAMPLES = ['3 May 1979', '5 April 09', '21th of August 2016', 'Tue 10 Jul 2007', 'Saturday May 9 2018', 'March 3 2001', 'March 3rd 2001', '1 March 2001']\n",
    "for example in EXAMPLES:\n",
    "    \n",
    "    source = string_to_int(example, Tx, human_vocab)\n",
    "    source = np.array(list(map(lambda x: to_categorical(x, num_classes=len(human_vocab)), source)))\n",
    "    source = np.expand_dims(source, axis=0)\n",
    "    prediction = model.predict([source, s0, c0])\n",
    "    prediction = np.argmax(prediction, axis = -1)\n",
    "    output = [inv_machine_vocab[int(i)] for i in prediction]\n",
    "    \n",
    "    print(\"source:\", example)\n",
    "    print(\"output:\", ''.join(output))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "机器翻译",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": true
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
