{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 搭建循环神经网络\n",
    "递归神经网络（RNN）对于自然语言处理和其他序列任务非常有效，因为它们具有“记忆”功能。 它们可以一次读取一个输入$x^{\\langle t \\rangle}$ \n",
    " （如单词），并且通过隐藏层激活从一个时间步传递到下一个时间步来记住一些信息/上下文，这允许单向RNN从过去获取信息来处理后面的输入，双向RNN可以从过去和未来中获取上下文。\n",
    "\n",
    "有些东西需要声明：\n",
    "\n",
    "- 上标$[l]$ 表示第$l$ 层\n",
    "\n",
    "  - 举例：$a^{[4]}$表示第4层的激活值，$W^{[5]}$ 与$b^{[5]}$ 是第5 层的参数。\n",
    "- 上标$(i)$表示第$(i)$个样本\n",
    "\n",
    "  - 举例：$x^{(i)}$表示第i个输入的样本。\n",
    "- 上标$\\langle t \\rangle$ 表示第t个时间步\n",
    "\n",
    "  - 举例：$x^{\\langle t \\rangle}$表示输入x 的第t 个时间步，$x^{(i)\\langle t \\rangle}$ 表示输入x 的第i个样本的第t个时间步。\n",
    "- 下标i表示向量的第i项\n",
    "\n",
    "  - 举例：$a^{[l]}_i$表示l层中的第i个项的激活值。我们先来加载所需要的库："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import rnn_utils"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 循环神经网络的前向传播\n",
    " 我们来看一下下面的循环神经网络的图，在这里使用的是$T_x = T_y$. ，我们来实现它。\n",
    " <img src=\"images/1.png\" style=\"width:500;height:300px;\">\n",
    "我们怎么才能实现它呢？有以下步骤：\n",
    "- 实现RNN的一个时间步所需要计算的东西。\n",
    "- 在$T_x$时间步上实现一个循环，以便一次处理所有输入。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### RNN单元\n",
    "循环神经网络可以看作是单元的重复，首先要实现单个时间步的计算，下图描述了RNN单元的单个时间步的操作\n",
    "<img src=\"images/2.png\" style=\"width:500;height:300px;\">\n",
    "现在我们要根据图2来实现一个RNN单元，这需要由以下几步完成：\n",
    "\n",
    "- 使用tanh函数计算隐藏单元的激活值： $a^{\\langle t \\rangle} = \\tanh(W_{aa} a^{\\langle t-1 \\rangle} + W_{ax} x^{\\langle t \\rangle} + b_a)$.\n",
    "- 使用 $a^{\\langle t \\rangle}$ 计算$\\hat{y}^{\\langle t \\rangle} = softmax(W_{ya} a^{\\langle t \\rangle} + b_y)$.softmax在rnn_utils内。\n",
    "\n",
    "- 把$(a^{\\langle t \\rangle}, a^{\\langle t-1 \\rangle}, x^{\\langle t \\rangle}, parameters)$存储到cache中。\n",
    "\n",
    "- 返回 $a^{\\langle t \\rangle}$ , $y^{\\langle t \\rangle}$ 与cache。\n",
    "\n",
    "我们将向量化$m$个样本，因此， $x^{\\langle t \\rangle}$ 的维度为 $(n_x,m)$.$a^{\\langle t \\rangle}$\n",
    " 的维度为 $(n_a,m)$. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def rnn_cell_forward(xt, a_prev, parameters):\n",
    "    \"\"\"\n",
    "    根据图2实现RNN单元的单步前向传播\n",
    "    \n",
    "    参数：\n",
    "        xt -- 时间步“t”输入的数据，维度为（n_x, m）\n",
    "        a_prev -- 时间步“t - 1”的隐藏隐藏状态，维度为（n_a, m）\n",
    "        parameters -- 字典，包含了以下内容:\n",
    "                        Wax -- 矩阵，输入乘以权重，维度为（n_a, n_x）\n",
    "                        Waa -- 矩阵，隐藏状态乘以权重，维度为（n_a, n_a）\n",
    "                        Wya -- 矩阵，隐藏状态与输出相关的权重矩阵，维度为（n_y, n_a）\n",
    "                        ba  -- 偏置，维度为（n_a, 1）\n",
    "                        by  -- 偏置，隐藏状态与输出相关的偏置，维度为（n_y, 1）\n",
    "    \n",
    "    返回：\n",
    "        a_next -- 下一个隐藏状态，维度为（n_a， m）\n",
    "        yt_pred -- 在时间步“t”的预测，维度为（n_y， m）\n",
    "        cache -- 反向传播需要的元组，包含了(a_next, a_prev, xt, parameters)\n",
    "    \"\"\"\n",
    "    \n",
    "    # 从“parameters”获取参数\n",
    "    Wax = parameters[\"Wax\"]\n",
    "    Waa = parameters[\"Waa\"]\n",
    "    Wya = parameters[\"Wya\"]\n",
    "    ba = parameters[\"ba\"]\n",
    "    by = parameters[\"by\"]\n",
    "    \n",
    "    # 使用上面的公式计算下一个激活值\n",
    "    a_next = np.tanh(np.dot(Waa, a_prev) + np.dot(Wax, xt) + ba)\n",
    "    \n",
    "    # 使用上面的公式计算当前单元的输出\n",
    "    yt_pred = rnn_utils.softmax(np.dot(Wya, a_next) + by)\n",
    "    \n",
    "    # 保存反向传播需要的值\n",
    "    cache = (a_next, a_prev, xt, parameters)\n",
    "    \n",
    "    return a_next, yt_pred, cache"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### RNN的前向传播\n",
    "可以看到的是RNN是刚刚构建的单元格的重复连接，如果输入的数据序列经过10个时间步，那么将复制RNN单元10次，每个单元将前一个单元中的隐($a^{\\langle t-1 \\rangle}$)和当前时间步的输入数据 ($x^{\\langle t \\rangle}$)作为输入。 它为此时间步输出隐藏状态($a^{\\langle t \\rangle}$)和预测($y^{\\langle t \\rangle}$)\n",
    "<img src=\"images/3.png\" style=\"width:800px;height:300px;\">"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def rnn_forward(x, a0, parameters):\n",
    "    \"\"\"\n",
    "    根据图3来实现循环神经网络的前向传播\n",
    "    \n",
    "    参数：\n",
    "        x -- 输入的全部数据，维度为(n_x, m, T_x)\n",
    "        a0 -- 初始化隐藏状态，维度为 (n_a, m)\n",
    "        parameters -- 字典，包含了以下内容:\n",
    "                        Wax -- 矩阵，输入乘以权重，维度为（n_a, n_x）\n",
    "                        Waa -- 矩阵，隐藏状态乘以权重，维度为（n_a, n_a）\n",
    "                        Wya -- 矩阵，隐藏状态与输出相关的权重矩阵，维度为（n_y, n_a）\n",
    "                        ba  -- 偏置，维度为（n_a, 1）\n",
    "                        by  -- 偏置，隐藏状态与输出相关的偏置，维度为（n_y, 1）\n",
    "    \n",
    "    返回：\n",
    "        a -- 所有时间步的隐藏状态，维度为(n_a, m, T_x)\n",
    "        y_pred -- 所有时间步的预测，维度为(n_y, m, T_x)\n",
    "        caches -- 为反向传播的保存的元组，维度为（【列表类型】cache, x)）\n",
    "    \"\"\"\n",
    "    \n",
    "    # 初始化“caches”，它将以列表类型包含所有的cache\n",
    "    caches = []\n",
    "    \n",
    "    # 获取 x 与 Wya 的维度信息\n",
    "    n_x, m, T_x = x.shape\n",
    "    n_y, n_a = parameters[\"Wya\"].shape\n",
    "    \n",
    "    # 使用0来初始化“a” 与“y”\n",
    "    a = np.zeros([n_a, m, T_x])\n",
    "    y_pred = np.zeros([n_y, m, T_x])\n",
    "    \n",
    "    # 初始化“next”\n",
    "    a_next = a0\n",
    "    \n",
    "    # 遍历所有时间步\n",
    "    for t in range(T_x):\n",
    "        ## 1.使用rnn_cell_forward函数来更新“next”隐藏状态与cache。\n",
    "        a_next, yt_pred, cache = rnn_cell_forward(x[:, :, t], a_next, parameters)\n",
    "        \n",
    "        ## 2.使用 a 来保存“next”隐藏状态（第 t ）个位置。\n",
    "        a[:, :, t] = a_next\n",
    "        \n",
    "        ## 3.使用 y 来保存预测值。\n",
    "        y_pred[:, :, t] = yt_pred\n",
    "        \n",
    "        ## 4.把cache保存到“caches”列表中。\n",
    "        caches.append(cache)\n",
    "    \n",
    "    # 保存反向传播所需要的参数\n",
    "    caches = (caches, x)\n",
    "    \n",
    "    return a, y_pred, caches"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "我们构建了循环神经网络的前向传播函数，这对于某些应用程序来说已经足够好了，但是它还存在梯度消失的问题。当每个输出$y^{\\langle t \\rangle}$ 是根据局部的上下文来进行预测的时候，它的效果是比较好的（意思是输入的是$x^{\\langle t' \\rangle}$ ,其中 $t'$ 与$t$相隔不是太远）。\n",
    "\n",
    " 接下来我们要构建一个更加复杂的LSTM模型，它可以更好地解决梯度消失的问题，LSTM能够更好地记住一条信息，并且可以在很多时间步中保存。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 长短时记忆 (LSTM)网络\n",
    "<img src=\"images/4.png\" style=\"width:500;height:400px;\">\n",
    "\n",
    "### “门”的介绍\n",
    "#### 遗忘门\n",
    "假设我们正在阅读文本中的单词，并希望使用LSTM来跟踪语法结构，比如主语是单数还是复数。如果主语从单数变为复数，我们需要找到一种方法来摆脱我们先前存储的单复数状态的记忆值。在LSTM中，遗忘门是这样做的:\n",
    "$$\\Gamma_f^{\\langle t \\rangle} = \\sigma(W_f[a^{\\langle t-1 \\rangle}, x^{\\langle t \\rangle}] + b_f)\\tag{1} $$\n",
    "其中，$W_f$是控制遗忘门的权值，我们把 $a^{\\langle t-1 \\rangle}$, $x^{\\langle t \\rangle}$ 连接起来变为$[a^{\\langle t-1 \\rangle}, x^{\\langle t \\rangle}]$ ,然后乘以 $W_f$，结果就是得到了一个矢量 $\\Gamma_f^{\\langle t \\rangle}$,其值在0与1 之间。这个遗忘门向量将与前一个单元状态 $c^{\\langle t-1 \\rangle}$ 相乘，因此，如果 $\\Gamma_f^{\\langle t \\rangle}$ 的一个值是0 （或者≈0），则意味着LSTM应该删除对应的信息，如果其中有为1 的值，那么LSTM将保留该信息。\n",
    "#### 更新门\n",
    "一旦我们“忘记”所讨论的过去的主题是单数，我们需要找到一种方法来更新它，以反映新的主题现在是复数。这里是更新门的公式：\n",
    "$$\\Gamma_u^{\\langle t \\rangle} = \\sigma(W_u[a^{\\langle t-1 \\rangle}, x^{\\{t\\}}] + b_u)\\tag{2} $$ \n",
    "与遗忘门相似， $\\Gamma_u^{\\langle t \\rangle}$ 向量的值是在0 与1 之间，为了计算$c^{\\langle t \\rangle}$,它会与$\\tilde{c}^{\\langle t \\rangle}$相乘\n",
    "#### 更新单元\n",
    "为了要更新主题，我们需要创建一个新的向量，我们可以将其添加到之前的单元状态中。我们使用的公式是：\n",
    " $$ \\tilde{c}^{\\langle t \\rangle} = \\tanh(W_c[a^{\\langle t-1 \\rangle}, x^{\\langle t \\rangle}] + b_c)\\tag{3} $$\n",
    "最后，单元的新状态是：\n",
    " $$ c^{\\langle t \\rangle} = \\Gamma_f^{\\langle t \\rangle}* c^{\\langle t-1 \\rangle} + \\Gamma_u^{\\langle t \\rangle} *\\tilde{c}^{\\langle t \\rangle} \\tag{4} $$\n",
    "#### 输出门\n",
    " 为了决定我们将使用哪种输出，我们将使用以下两个公式：\n",
    "$$ \\Gamma_o^{\\langle t \\rangle}=  \\sigma(W_o[a^{\\langle t-1 \\rangle}, x^{\\langle t \\rangle}] + b_o)\\tag{5}$$ \n",
    "$$ a^{\\langle t \\rangle} = \\Gamma_o^{\\langle t \\rangle}* \\tanh(c^{\\langle t \\rangle})\\tag{6} $$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### LSTM单元\n",
    "我们根据图4来实现一个LSTM单元，步骤如下：\n",
    "\n",
    "- 把 $a^{\\langle t-1 \\rangle}$ , $x^{\\langle t \\rangle}$连接起来变为一个矩阵：$concat = \\begin{bmatrix} a^{\\langle t-1 \\rangle} \\\\ x^{\\langle t \\rangle} \\end{bmatrix}$\n",
    "- 计算公式1-6，我们可以使用sigmoid()（在rnn_utils内）与np.tanh()。\n",
    "\n",
    "- 计算预测 $y^{\\langle t \\rangle}$我们可以使用softmax()（在rnn_utils内）。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def lstm_cell_forward(xt, a_prev, c_prev, parameters):\n",
    "    \"\"\"\n",
    "    根据图4实现一个LSTM单元的前向传播。\n",
    "    \n",
    "    参数：\n",
    "        xt -- 在时间步“t”输入的数据，维度为(n_x, m)\n",
    "        a_prev -- 上一个时间步“t-1”的隐藏状态，维度为(n_a, m)\n",
    "        c_prev -- 上一个时间步“t-1”的记忆状态，维度为(n_a, m)\n",
    "        parameters -- 字典类型的变量，包含了：\n",
    "                        Wf -- 遗忘门的权值，维度为(n_a, n_a + n_x)\n",
    "                        bf -- 遗忘门的偏置，维度为(n_a, 1)\n",
    "                        Wi -- 更新门的权值，维度为(n_a, n_a + n_x)\n",
    "                        bi -- 更新门的偏置，维度为(n_a, 1)\n",
    "                        Wc -- 第一个“tanh”的权值，维度为(n_a, n_a + n_x)\n",
    "                        bc -- 第一个“tanh”的偏置，维度为(n_a, n_a + n_x)\n",
    "                        Wo -- 输出门的权值，维度为(n_a, n_a + n_x)\n",
    "                        bo -- 输出门的偏置，维度为(n_a, 1)\n",
    "                        Wy -- 隐藏状态与输出相关的权值，维度为(n_y, n_a)\n",
    "                        by -- 隐藏状态与输出相关的偏置，维度为(n_y, 1)\n",
    "    返回：\n",
    "        a_next -- 下一个隐藏状态，维度为(n_a, m)\n",
    "        c_next -- 下一个记忆状态，维度为(n_a, m)\n",
    "        yt_pred -- 在时间步“t”的预测，维度为(n_y, m)\n",
    "        cache -- 包含了反向传播所需要的参数，包含了(a_next, c_next, a_prev, c_prev, xt, parameters)\n",
    "        \n",
    "    注意：\n",
    "        ft/it/ot表示遗忘/更新/输出门，cct表示候选值(c tilda)，c表示记忆值。\n",
    "    \"\"\"\n",
    "    \n",
    "    # 从“parameters”中获取相关值\n",
    "    Wf = parameters[\"Wf\"]\n",
    "    bf = parameters[\"bf\"]\n",
    "    Wi = parameters[\"Wi\"]\n",
    "    bi = parameters[\"bi\"]\n",
    "    Wc = parameters[\"Wc\"]\n",
    "    bc = parameters[\"bc\"]\n",
    "    Wo = parameters[\"Wo\"]\n",
    "    bo = parameters[\"bo\"]\n",
    "    Wy = parameters[\"Wy\"]\n",
    "    by = parameters[\"by\"]\n",
    "    \n",
    "    # 获取 xt 与 Wy 的维度信息\n",
    "    n_x, m = xt.shape\n",
    "    n_y, n_a = Wy.shape\n",
    "    \n",
    "    # 1.连接 a_prev 与 xt\n",
    "    contact = np.zeros([n_a + n_x, m])\n",
    "    contact[: n_a, :] = a_prev\n",
    "    contact[n_a :, :] = xt\n",
    "    \n",
    "    # 2.根据公式计算ft、it、cct、c_next、ot、a_next\n",
    "    \n",
    "    ## 遗忘门，公式1\n",
    "    ft = rnn_utils.sigmoid(np.dot(Wf, contact) + bf)\n",
    "    \n",
    "    ## 更新门，公式2\n",
    "    it = rnn_utils.sigmoid(np.dot(Wi, contact) + bi)\n",
    "    \n",
    "    ## 更新单元，公式3\n",
    "    cct = np.tanh(np.dot(Wc, contact) + bc)\n",
    "    \n",
    "    ## 更新单元，公式4\n",
    "    #c_next = np.multiply(ft, c_prev) + np.multiply(it, cct)\n",
    "    c_next = ft * c_prev + it * cct\n",
    "    ## 输出门，公式5\n",
    "    ot = rnn_utils.sigmoid(np.dot(Wo, contact) + bo)\n",
    "    \n",
    "    ## 输出门，公式6\n",
    "    #a_next = np.multiply(ot, np.tan(c_next))\n",
    "    a_next = ot * np.tanh(c_next)\n",
    "    # 3.计算LSTM单元的预测值\n",
    "    yt_pred = rnn_utils.softmax(np.dot(Wy, a_next) + by)\n",
    "    \n",
    "    # 保存包含了反向传播所需要的参数\n",
    "    cache = (a_next, c_next, a_prev, c_prev, ft, it, cct, ot, xt, parameters)\n",
    "    \n",
    "    return a_next, c_next, yt_pred, cache"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### LSTM的前向传播\n",
    " 我们已经实现了LSTM单元的一个时间步的前向传播，现在我们要对LSTM网络进行前向传播进行计算\n",
    " <img src=\"images/5.png\" style=\"width:500;height:300px;\">"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def lstm_forward(x, a0, parameters):\n",
    "    \"\"\"\n",
    "    根据图5来实现LSTM单元组成的的循环神经网络\n",
    "    \n",
    "    参数：\n",
    "        x -- 所有时间步的输入数据，维度为(n_x, m, T_x)\n",
    "        a0 -- 初始化隐藏状态，维度为(n_a, m)\n",
    "        parameters -- python字典，包含了以下参数：\n",
    "                        Wf -- 遗忘门的权值，维度为(n_a, n_a + n_x)\n",
    "                        bf -- 遗忘门的偏置，维度为(n_a, 1)\n",
    "                        Wi -- 更新门的权值，维度为(n_a, n_a + n_x)\n",
    "                        bi -- 更新门的偏置，维度为(n_a, 1)\n",
    "                        Wc -- 第一个“tanh”的权值，维度为(n_a, n_a + n_x)\n",
    "                        bc -- 第一个“tanh”的偏置，维度为(n_a, n_a + n_x)\n",
    "                        Wo -- 输出门的权值，维度为(n_a, n_a + n_x)\n",
    "                        bo -- 输出门的偏置，维度为(n_a, 1)\n",
    "                        Wy -- 隐藏状态与输出相关的权值，维度为(n_y, n_a)\n",
    "                        by -- 隐藏状态与输出相关的偏置，维度为(n_y, 1)\n",
    "        \n",
    "    返回：\n",
    "        a -- 所有时间步的隐藏状态，维度为(n_a, m, T_x)\n",
    "        y -- 所有时间步的预测值，维度为(n_y, m, T_x)\n",
    "        caches -- 为反向传播的保存的元组，维度为（【列表类型】cache, x)）\n",
    "    \"\"\"\n",
    "    \n",
    "    # 初始化“caches”\n",
    "    caches = []\n",
    "    \n",
    "    # 获取 xt 与 Wy 的维度信息\n",
    "    n_x, m, T_x = x.shape\n",
    "    n_y, n_a = parameters[\"Wy\"].shape\n",
    "    \n",
    "    # 使用0来初始化“a”、“c”、“y”\n",
    "    a = np.zeros([n_a, m, T_x])\n",
    "    c = np.zeros([n_a, m, T_x])\n",
    "    y = np.zeros([n_y, m, T_x])\n",
    "    \n",
    "    # 初始化“a_next”、“c_next”\n",
    "    a_next = a0\n",
    "    c_next = np.zeros([n_a, m])\n",
    "    \n",
    "    # 遍历所有的时间步\n",
    "    for t in range(T_x):\n",
    "        # 更新下一个隐藏状态，下一个记忆状态，计算预测值，获取cache\n",
    "        a_next, c_next, yt_pred, cache = lstm_cell_forward(x[:,:,t], a_next, c_next, parameters)\n",
    "        \n",
    "        # 保存新的下一个隐藏状态到变量a中\n",
    "        a[:, :, t] = a_next\n",
    "        \n",
    "        # 保存预测值到变量y中\n",
    "        y[:, :, t] = yt_pred\n",
    "        \n",
    "        # 保存下一个单元状态到变量c中\n",
    "        c[:, :, t] = c_next\n",
    "        \n",
    "        # 把cache添加到caches中\n",
    "        caches.append(cache)\n",
    "    \n",
    "    # 保存反向传播需要的参数\n",
    "    caches = (caches, x)\n",
    "    \n",
    "    return a, y, c, caches"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 字符级语言模型 - 恐龙岛\n",
    "欢迎来到恐龙岛，恐龙生活于在6500万年前，现在研究人员在试着复活恐龙，而你的任务就是给恐龙命名，如果一只恐龙不喜欢它的名字，它可能会狂躁不安，所以你要谨慎选择。\n",
    "<img src=\"images/6.jpg\" style=\"width:250;height:300px;\">\n",
    "你的助手已经收集了他们能够找到的所有恐龙名字，并编入了这个[数据集](dinos.txt),为了构建字符级语言模型来生成新的名称，你的模型将学习不同的名称模式，并随机生成新的名字。希望这个算法能让你和你的团队远离恐龙的愤怒。\n",
    "\n",
    "在这里你将学习到：\n",
    "\n",
    "- 如何存储文本数据以便使用RNN进行处理。\n",
    "\n",
    "- 如何合成数据，通过每次采样预测，并将其传递给下一个rnn单元。\n",
    "\n",
    "- 如何构建字符级文本生成循环神经网络。\n",
    "\n",
    "- 为什么梯度修剪很重要?\n",
    "\n",
    "我们将首先加载我们在rnn_utils中提供的一些函数。具体地说，我们可以使用rnn_forward和rnn_backward等函数，这些函数与前面实现的函数相同。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import random\n",
    "import time\n",
    "import cllm_utils"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 问题描述\n",
    "### 数据集与预处理\n",
    " 我们先来读取恐龙名称的数据集，创建一个唯一字符列表（如AZ），并计算数据集和词汇量大小。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['f', 'm', 'p', 'b', '\\n', 'j', 'g', 'i', 'h', 'r', 'c', 'w', 'y', 't', 'v', 'n', 'k', 'q', 'd', 's', 'x', 'e', 'o', 'a', 'u', 'z', 'l']\n",
      "共计有19909个字符，唯一字符有27个\n"
     ]
    }
   ],
   "source": [
    "# 获取名称\n",
    "data = open(\"dinos.txt\", \"r\").read()\n",
    "\n",
    "# 转化为小写字符\n",
    "data = data.lower()\n",
    "\n",
    "# 转化为无序且不重复的元素列表\n",
    "chars = list(set(data))\n",
    "\n",
    "# 获取大小信息\n",
    "data_size, vocab_size = len(data), len(chars)\n",
    "\n",
    "print(chars)\n",
    "print(\"共计有%d个字符，唯一字符有%d个\"%(data_size,vocab_size))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Aachenosaurus\\nAardonyx\\nAbdallahsaurus\\nAbelisaurus\\nAbrictosaurus\\nAbrosaurus\\nAbydosaurus\\nAcanthopholis\\nAchelousaurus\\nAcheroraptor\\nAchillesaurus\\nAchillobator\\nAcristavus\\nAcrocanthosaurus\\nAcrotholus\\nActiosaurus\\nAdamantisaurus\\nAdasaurus\\nAdelolophus\\nAdeopapposaurus\\nAegyptosaurus\\nAeolosaurus\\nAepisaurus\\nAepyornithomimus\\nAerosteon\\nAetonyxAfromimus\\nAfrovenator\\nAgathaumas\\nAggiosaurus\\nAgilisaurus\\nAgnosphitys\\nAgrosaurus\\nAgujaceratops\\nAgustinia\\nAhshislepelta\\nAirakoraptor\\nAjancingenia\\nAjkaceratops\\nAlamosaurus\\nAlaskacephale\\nAlbalophosaurus\\nAlbertaceratops\\nAlbertadromeus\\nAlbertavenator\\nAlbertonykus\\nAlbertosaurus\\nAlbinykus\\nAlbisaurus\\nAlcovasaurus\\nAlectrosaurus\\nAletopelta\\nAlgoasaurus\\nAlioramus\\nAliwalia\\nAllosaurus\\nAlmas\\nAlnashetri\\nAlocodon\\nAltirhinus\\nAltispinax\\nAlvarezsaurus\\nAlwalkeria\\nAlxasaurus\\nAmargasaurus\\nAmargastegos\\nAmargatitanis\\nAmazonsaurus\\nAmmosaurus\\nAmpelosaurus\\nAmphicoelias\\nAmphicoelicaudia\\nAmphisaurus\\nAmtocephale\\nAmtosaurus\\nAmurosaurus\\nAmygdalodon\\nAnabisetia\\nAnasazisaurus\\nAnatosaurus\\nAnatotitan\\nAnchiceratops\\nAnchiornis\\nAnchisaurus\\nAndesaurus\\nAndhrasaurus\\nAngaturama\\nAngloposeidon\\nAngolatitan\\nAngulomastacator\\nAniksosaurus\\nAnimantarx\\nAnkistrodon\\nAnkylosaurus\\nAnodontosaurus\\nAnoplosaurus\\nAnserimimus\\nAntarctopelta\\nAntarctosaurus\\nAntetonitrus\\nAnthodon\\nAntrodemus\\nAnzu\\nAoniraptor\\nAorun\\nApatodon\\nApatoraptor\\nApatosaurus\\nAppalachiosaurus\\nAquilops\\nAragosaurus\\nAralosaurus\\nAraucanoraptor\\nArchaeoceratops\\nArchaeodontosaurus\\nArchaeopteryx\\nArchaeoraptor\\nArchaeornis\\nArchaeornithoides\\nArchaeornithomimus\\nArcovenator\\nArctosaurus\\nArcusaurus\\nArenysaurus\\nArgentinosaurus\\nArgyrosaurus\\nAristosaurus\\nAristosuchus\\nArizonasaurus\\nArkansaurus\\nArkharavia\\nArrhinoceratops\\nArstanosaurus\\nAsiaceratops\\nAsiamericana\\nAsiatosaurus\\nAstrodon\\nAstrodonius\\nAstrodontaurus\\nAstrophocaudia\\nAsylosaurus\\nAtacamatitan\\nAtlantosaurus\\nAtlasaurus\\nAtlascopcosaurus\\nAtrociraptor\\nAtsinganosaurus\\nAublysodon\\nAucasaurus\\nAugustia\\nAugustynolophus\\nAuroraceratops\\nAurornis\\nAustralodocus\\nAustralovenator\\nAustrocheirus\\nAustroposeidon\\nAustroraptor\\nAustrosaurus\\nAvaceratops\\nAvalonia\\nAvalonianus\\nAviatyrannis\\nAvimimus\\nAvisaurus\\nAvipes\\nAzendohsaurus\\nBactrosaurus\\nBagaceratops\\nBagaraatan\\nBahariasaurus\\nBainoceratops\\nBakesaurus\\nBalaur\\nBalochisaurus\\nBambiraptor\\nBanji\\nBaotianmansaurus\\nBarapasaurus\\nBarilium\\nBarosaurus\\nBarrosasaurus\\nBarsboldia\\nBaryonyx\\nBashunosaurus\\nBasutodon\\nBathygnathus\\nBatyrosaurus\\nBaurutitan\\nBayosaurus\\nBecklespinax\\nBeelemodon\\nBeibeilong\\nBeipiaognathus\\nBeipiaosaurus\\nBeishanlong\\nBellusaurus\\nBelodon\\nBerberosaurus\\nBetasuchus\\nBicentenaria\\nBienosaurus\\nBihariosaurus\\nBilbeyhallorum\\nBissektipelta\\nBistahieversor\\nBlancocerosaurus\\nBlasisaurus\\nBlikanasaurus\\nBolong\\nBonapartenykus\\nBonapartesaurus\\nBonatitan\\nBonitasaura\\nBorealopelta\\nBorealosaurus\\nBoreonykus\\nBorogovia\\nBothriospondylus\\nBrachiosaurus\\nBrachyceratops\\nBrachylophosaurus\\nBrachypodosaurus\\nBrachyrophus\\nBrachytaenius\\nBrachytrachelopan\\nBradycneme\\nBrasileosaurus\\nBrasilotitan\\nBravoceratops\\nBreviceratops\\nBrohisaurus\\nBrontomerus\\nBrontoraptor\\nBrontosaurus\\nBruhathkayosaurus\\nBugenasaura\\nBuitreraptor\\nBurianosaurus\\nBuriolestes\\nByranjaffia\\nByronosaurus\\nCaenagnathasia\\nCaenagnathus\\nCalamosaurus\\nCalamospondylus\\nCalamospondylus\\nCallovosaurus\\nCamarasaurus\\nCamarillasaurus\\nCamelotia\\nCamposaurus\\nCamptonotus\\nCamptosaurus\\nCampylodon\\nCampylodoniscus\\nCanardia\\nCapitalsaurus\\nCarcharodontosaurus\\nCardiodon\\nCarnotaurus\\nCaseosaurus\\nCathartesaura\\nCathetosaurus\\nCaudipteryx\\nCaudocoelus\\nCaulodon\\nCedarosaurus\\nCedarpelta\\nCedrorestes\\nCentemodon\\nCentrosaurus\\nCerasinops\\nCeratonykus\\nCeratops\\nCeratosaurus\\nCetiosauriscus\\nCetiosaurus\\nChangchunsaurus\\nChangdusaurus\\nChangyuraptor\\nChaoyangsaurus\\nCharonosaurus\\nChasmosaurus\\nChassternbergia\\nChebsaurus\\nChenanisaurus\\nCheneosaurus\\nChialingosaurus\\nChiayusaurus\\nChienkosaurus\\nChihuahuasaurus\\nChilantaisaurus\\nChilesaurus\\nChindesaurus\\nChingkankousaurus\\nChinshakiangosaurus\\nChirostenotes\\nChoconsaurus\\nChondrosteosaurus\\nChromogisaurus\\nChuandongocoelurus\\nChuanjiesaurus\\nChuanqilong\\nChubutisaurus\\nChungkingosaurus\\nChuxiongosaurus\\nCinizasaurus\\nCionodon\\nCitipati\\nCladeiodon\\nClaorhynchus\\nClaosaurus\\nClarencea\\nClasmodosaurus\\nClepsysaurus\\nCoahuilaceratops\\nCoelophysis\\nCoelosaurus\\nCoeluroides\\nCoelurosauravus\\nCoelurus\\nColepiocephale\\nColoradia\\nColoradisaurus\\nColossosaurus\\nComahuesaurus\\nComanchesaurus\\nCompsognathus\\nCompsosuchus\\nConcavenator\\nConchoraptor\\nCondorraptor\\nCoronosaurus\\nCorythoraptor\\nCorythosaurus\\nCraspedodon\\nCrataeomus\\nCraterosaurus\\nCreosaurus\\nCrichtonpelta\\nCrichtonsaurus\\nCristatusaurus\\nCrosbysaurus\\nCruxicheiros\\nCryolophosaurus\\nCryptodraco\\nCryptoraptor\\nCryptosaurus\\nCryptovolans\\nCumnoria\\nDaanosaurus\\nDacentrurus\\nDachongosaurus\\nDaemonosaurus\\nDahalokely\\nDakosaurus\\nDakotadon\\nDakotaraptor\\nDaliansaurus\\nDamalasaurus\\nDandakosaurus\\nDanubiosaurus\\nDaptosaurus\\nDarwinsaurus\\nDashanpusaurus\\nDaspletosaurus\\nDasygnathoides\\nDasygnathus\\nDatanglong\\nDatonglong\\nDatousaurus\\nDaurosaurus\\nDaxiatitan\\nDeinocheirus\\nDeinodon\\nDeinonychus\\nDelapparentia\\nDeltadromeus\\nDemandasaurus\\nDenversaurus\\nDeuterosaurus\\nDiabloceratops\\nDiamantinasaurus\\nDianchungosaurus\\nDiceratops\\nDiceratusDiclonius\\nDicraeosaurus\\nDidanodonDilong\\nDilophosaurus\\nDiluvicursor\\nDimodosaurus\\nDinheirosaurus\\nDinodocus\\nDinotyrannus\\nDiplodocus\\nDiplotomodon\\nDiracodon\\nDolichosuchus\\nDollodon\\nDomeykosaurus\\nDongbeititan\\nDongyangopelta\\nDongyangosaurus\\nDoratodon\\nDoryphorosaurus\\nDraconyx\\nDracopelta\\nDracoraptor\\nDracorex\\nDracovenator\\nDravidosaurus\\nDreadnoughtus\\nDrinker\\nDromaeosauroides\\nDromaeosaurus\\nDromiceiomimus\\nDromicosaurus\\nDrusilasaura\\nDryosaurus\\nDryptosauroides\\nDryptosaurus\\nDubreuillosaurus\\nDuriatitan\\nDuriavenator\\nDynamosaurus\\nDyoplosaurus\\nDysalotosaurus\\nDysganus\\nDyslocosaurus\\nDystrophaeus\\nDystylosaurus\\nEchinodon\\nEdmarka\\nEdmontonia\\nEdmontosaurus\\nEfraasia\\nEiniosaurus\\nEkrixinatosaurus\\nElachistosuchus\\nElaltitan\\nElaphrosaurus\\nElmisaurus\\nElopteryx\\nElosaurus\\nElrhazosaurus\\nElvisaurus\\nEmausaurus\\nEmbasaurus\\nEnigmosaurus\\nEoabelisaurus\\nEobrontosaurus\\nEocarcharia\\nEoceratops\\nEocursor\\nEodromaeus\\nEohadrosaurus\\nEolambia\\nEomamenchisaurus\\nEoplophysis\\nEoraptor\\nEosinopteryx\\nEotrachodon\\nEotriceratops\\nEotyrannus\\nEousdryosaurus\\nEpachthosaurus\\nEpanterias\\nEphoenosaurus\\nEpicampodon\\nEpichirostenotes\\nEpidendrosaurus\\nEpidexipteryx\\nEquijubus\\nErectopus\\nErketu\\nErliansaurus\\nErlikosaurus\\nEshanosaurus\\nEuacanthus\\nEucamerotus\\nEucentrosaurus\\nEucercosaurus\\nEucnemesaurus\\nEucoelophysis\\nEugongbusaurus\\nEuhelopus\\nEuoplocephalus\\nEupodosaurus\\nEureodon\\nEurolimnornis\\nEuronychodon\\nEuropasaurus\\nEuropatitan\\nEuropelta\\nEuskelosaurus\\nEustreptospondylus\\nFabrosaurus\\nFalcarius\\nFendusaurus\\nFenestrosaurus\\nFerganasaurus\\nFerganastegos\\nFerganocephale\\nForaminacephale\\nFosterovenator\\nFrenguellisaurus\\nFruitadens\\nFukuiraptor\\nFukuisaurus\\nFukuititan\\nFukuivenator\\nFulengia\\nFulgurotherium\\nFusinasus\\nFusuisaurus\\nFutabasaurus\\nFutalognkosaurus\\nGadolosaurus\\nGaleamopus\\nGalesaurus\\nGallimimus\\nGaltonia\\nGalveosaurus\\nGalvesaurus\\nGannansaurus\\nGansutitan\\nGanzhousaurus\\nGargoyleosaurus\\nGarudimimus\\nGasosaurus\\nGasparinisaura\\nGastonia\\nGavinosaurus\\nGeminiraptor\\nGenusaurus\\nGenyodectes\\nGeranosaurus\\nGideonmantellia\\nGiganotosaurus\\nGigantoraptor\\nGigantosaurus\\nGigantosaurus\\nGigantoscelus\\nGigantspinosaurus\\nGilmoreosaurus\\nGinnareemimus\\nGiraffatitan\\nGlacialisaurus\\nGlishades\\nGlyptodontopelta\\nSkeleton\\nGobiceratops\\nGobisaurus\\nGobititan\\nGobivenator\\nGodzillasaurus\\nGojirasaurus\\nGondwanatitan\\nGongbusaurus\\nGongpoquansaurus\\nGongxianosaurus\\nGorgosaurus\\nGoyocephale\\nGraciliceratops\\nGraciliraptor\\nGracilisuchus\\nGravitholus\\nGresslyosaurus\\nGriphornis\\nGriphosaurus\\nGryphoceratops\\nGryponyx\\nGryposaurus\\nGspsaurus\\nGuaibasaurus\\nGualicho\\nGuanlong\\nGwyneddosaurus\\nGyposaurus\\nHadrosauravus\\nHadrosaurus\\nHaestasaurus\\nHagryphus\\nHallopus\\nHalszkaraptor\\nHalticosaurus\\nHanssuesia\\nHanwulosaurus\\nHaplocanthosaurus\\nHaplocanthus\\nHaplocheirus\\nHarpymimus\\nHaya\\nHecatasaurus\\nHeilongjiangosaurus\\nHeishansaurus\\nHelioceratops\\nHelopus\\nHeptasteornis\\nHerbstosaurus\\nHerrerasaurus\\nHesperonychus\\nHesperosaurus\\nHeterodontosaurus\\nHeterosaurus\\nHexing\\nHexinlusaurus\\nHeyuannia\\nHierosaurus\\nHippodraco\\nHironosaurus\\nHisanohamasaurus\\nHistriasaurus\\nHomalocephale\\nHonghesaurus\\nHongshanosaurus\\nHoplitosaurus\\nHoplosaurus\\nHorshamosaurus\\nHortalotarsus\\nHuabeisaurus\\nHualianceratops\\nHuanansaurus\\nHuanghetitan\\nHuangshanlong\\nHuaxiagnathus\\nHuaxiaosaurus\\nHuaxiasaurus\\nHuayangosaurus\\nHudiesaurus\\nHuehuecanauhtlus\\nHulsanpes\\nHungarosaurus\\nHuxleysaurus\\nHylaeosaurus\\nHylosaurusHypacrosaurus\\nHypselorhachis\\nHypselosaurus\\nHypselospinus\\nHypsibema\\nHypsilophodon\\nHypsirhophus\\nhabodcraniosaurus\\nIchthyovenator\\nIgnavusaurus\\nIguanacolossus\\nIguanodon\\nIguanoides\\nSkeleton\\nIguanosaurus\\nIliosuchus\\nIlokelesia\\nIncisivosaurus\\nIndosaurus\\nIndosuchus\\nIngenia\\nInosaurus\\nIrritator\\nIsaberrysaura\\nIsanosaurus\\nIschioceratops\\nIschisaurus\\nIschyrosaurus\\nIsisaurus\\nIssasaurus\\nItemirus\\nIuticosaurus\\nJainosaurus\\nJaklapallisaurus\\nJanenschia\\nJaxartosaurus\\nJeholosaurus\\nJenghizkhan\\nJensenosaurus\\nJeyawati\\nJianchangosaurus\\nJiangjunmiaosaurus\\nJiangjunosaurus\\nJiangshanosaurus\\nJiangxisaurus\\nJianianhualong\\nJinfengopteryx\\nJingshanosaurus\\nJintasaurus\\nJinzhousaurus\\nJiutaisaurus\\nJobaria\\nJubbulpuria\\nJudiceratops\\nJurapteryx\\nJurassosaurus\\nJuratyrant\\nJuravenator\\nKagasaurus\\nKaijiangosaurus\\nKakuru\\nKangnasaurus\\nKarongasaurus\\nKatepensaurus\\nKatsuyamasaurus\\nKayentavenator\\nKazaklambia\\nKelmayisaurus\\nKemkemiaKentrosaurus\\nKentrurosaurus\\nKerberosaurus\\nKentrosaurus\\nKhaan\\nKhetranisaurus\\nKileskus\\nKinnareemimus\\nKitadanisaurus\\nKittysaurus\\nKlamelisaurusKol\\nKoparion\\nKoreaceratops\\nKoreanosaurus\\nKoreanosaurus\\nKoshisaurus\\nKosmoceratops\\nKotasaurus\\nKoutalisaurus\\nKritosaurus\\nKryptops\\nKrzyzanowskisaurus\\nKukufeldia\\nKulceratops\\nKulindadromeus\\nKulindapteryx\\nKunbarrasaurus\\nKundurosaurus\\nKunmingosaurus\\nKuszholia\\nLabocania\\nLabrosaurus\\nLaelaps\\nLaevisuchus\\nLagerpeton\\nLagosuchus\\nLaiyangosaurus\\nLamaceratops\\nLambeosaurus\\nLametasaurus\\nLamplughsaura\\nLanasaurus\\nLancangosaurus\\nLancanjiangosaurus\\nLanzhousaurus\\nLaosaurus\\nLapampasaurus\\nLaplatasaurus\\nLapparentosaurus\\nLaquintasaura\\nLatenivenatrix\\nLatirhinus\\nLeaellynasaura\\nLeinkupal\\nLeipsanosaurus\\nLengosaurus\\nLeonerasaurus\\nLepidocheirosaurus\\nLepidus\\nLeptoceratops\\nLeptorhynchos\\nLeptospondylus\\nLeshansaurus\\nLesothosaurus\\nLessemsaurus\\nLevnesovia\\nLewisuchus\\nLexovisaurus\\nLeyesaurus\\nLiaoceratops\\nLiaoningosaurus\\nLiaoningtitan\\nLiaoningvenator\\nLiassaurus\\nLibycosaurus\\nLigabueino\\nLigabuesaurus\\nLigomasaurus\\nLikhoelesaurus\\nLiliensternus\\nLimaysaurus\\nLimnornis\\nLimnosaurus\\nLimusaurus\\nLinhenykus\\nLinheraptor\\nLinhevenator\\nLirainosaurus\\nLisboasaurusLiubangosaurus\\nLohuecotitan\\nLoncosaurus\\nLongisquama\\nLongosaurus\\nLophorhothon\\nLophostropheus\\nLoricatosaurus\\nLoricosaurus\\nLosillasaurus\\nLourinhanosaurus\\nLourinhasaurus\\nLuanchuanraptor\\nLuanpingosaurus\\nLucianosaurus\\nLucianovenator\\nLufengosaurus\\nLukousaurus\\nLuoyanggia\\nLurdusaurus\\nLusitanosaurus\\nLusotitan\\nLycorhinus\\nLythronax\\nMacelognathus\\nMachairasaurus\\nMachairoceratops\\nMacrodontophion\\nMacrogryphosaurus\\nMacrophalangia\\nMacroscelosaurus\\nMacrurosaurus\\nMadsenius\\nMagnapaulia\\nMagnamanus\\nMagnirostris\\nMagnosaurus\\nMagulodon\\nMagyarosaurus\\nMahakala\\nMaiasaura\\nMajungasaurus\\nMajungatholus\\nMalarguesaurus\\nMalawisaurus\\nMaleevosaurus\\nMaleevus\\nMamenchisaurus\\nManidens\\nMandschurosaurus\\nManospondylus\\nMantellisaurus\\nMantellodon\\nMapusaurus\\nMarasuchus\\nMarisaurus\\nMarmarospondylus\\nMarshosaurus\\nMartharaptor\\nMasiakasaurus\\nMassospondylus\\nMatheronodon\\nMaxakalisaurus\\nMedusaceratops\\nMegacervixosaurus\\nMegadactylus\\nMegadontosaurus\\nMegalosaurus\\nMegapnosaurus\\nMegaraptor\\nMei\\nMelanorosaurus\\nMendozasaurus\\nMercuriceratops\\nMeroktenos\\nMetriacanthosaurus\\nMicrocephale\\nMicroceratops\\nMicroceratus\\nMicrocoelus\\nMicrodontosaurus\\nMicrohadrosaurus\\nMicropachycephalosaurus\\nMicroraptor\\nMicrovenator\\nMierasaurus\\nMifunesaurus\\nMinmi\\nMinotaurasaurus\\nMiragaia\\nMirischia\\nMoabosaurus\\nMochlodon\\nMohammadisaurus\\nMojoceratops\\nMongolosaurus\\nMonkonosaurus\\nMonoclonius\\nMonolophosaurus\\nMononychus\\nMononykus\\nMontanoceratops\\nMorelladon\\nMorinosaurus\\nMorosaurus\\nMorrosaurus\\nMosaiceratops\\nMoshisaurus\\nMtapaiasaurus\\nMtotosaurus\\nMurusraptor\\nMussaurus\\nMuttaburrasaurus\\nMuyelensaurus\\nMymoorapelta\\nNaashoibitosaurus\\nNambalia\\nNankangia\\nNanningosaurus\\nNanosaurus\\nNanotyrannus\\nNanshiungosaurus\\nNanuqsaurus\\nNanyangosaurus\\nNarambuenatitan\\nNasutoceratops\\nNatronasaurus\\nNebulasaurus\\nNectosaurus\\nNedcolbertia\\nNedoceratops\\nNeimongosaurus\\nNemegtia\\nNemegtomaia\\nNemegtosaurus\\nNeosaurus\\nNeosodon\\nNeovenator\\nNeuquenraptor\\nNeuquensaurus\\nNewtonsaurus\\nNgexisaurus\\nNicksaurus\\nNigersaurus\\nNingyuansaurus\\nNiobrarasaurus\\nNipponosaurus\\nNoasaurus\\nNodocephalosaurus\\nNodosaurus\\nNomingia\\nNopcsaspondylus\\nNormanniasaurus\\nNothronychus\\nNotoceratops\\nNotocolossus\\nNotohypsilophodon\\nNqwebasaurus\\nNteregosaurus\\nNurosaurus\\nNuthetes\\nNyasasaurus\\nNyororosaurus\\nOhmdenosaurus\\nOjoceratops\\nOjoraptorsaurus\\nOligosaurus\\nOlorotitan\\nOmeisaurus\\nOmosaurus\\nOnychosaurus\\nOohkotokia\\nOpisthocoelicaudia\\nOplosaurus\\nOrcomimus\\nOrinosaurusOrkoraptor\\nOrnatotholusOrnithodesmus\\nOrnithoides\\nOrnitholestes\\nOrnithomerus\\nOrnithomimoides\\nOrnithomimus\\nOrnithopsis\\nOrnithosuchus\\nOrnithotarsus\\nOrodromeus\\nOrosaurus\\nOrthogoniosaurus\\nOrthomerus\\nOryctodromeus\\nOshanosaurus\\nOsmakasaurus\\nOstafrikasaurus\\nOstromia\\nOthnielia\\nOthnielosaurus\\nOtogosaurus\\nOuranosaurus\\nOverosaurus\\nOviraptor\\nOvoraptor\\nOwenodon\\nOxalaia\\nOzraptor\\nPachycephalosaurus\\nPachyrhinosaurus\\nPachysauriscus\\nPachysaurops\\nPachysaurus\\nPachyspondylus\\nPachysuchus\\nPadillasaurus\\nPakisaurus\\nPalaeoctonus\\nPalaeocursornis\\nPalaeolimnornis\\nPalaeopteryx\\nPalaeosauriscus\\nPalaeosaurus\\nPalaeosaurus\\nPalaeoscincus\\nPaleosaurus\\nPaludititan\\nPaluxysaurus\\nPampadromaeus\\nPamparaptor\\nPanamericansaurus\\nPandoravenator\\nPanguraptor\\nPanoplosaurus\\nPanphagia\\nPantydraco\\nParaiguanodon\\nParalititan\\nParanthodon\\nPararhabdodon\\nParasaurolophus\\nPareiasaurus\\nParksosaurus\\nParonychodon\\nParrosaurus\\nParvicursor\\nPatagonykus\\nPatagosaurus\\nPatagotitan\\nPawpawsaurus\\nPectinodon\\nPedopenna\\nPegomastax\\nPeishansaurus\\nPekinosaurus\\nPelecanimimus\\nPellegrinisaurus\\nPeloroplites\\nPelorosaurus\\nPeltosaurus\\nPenelopognathus\\nPentaceratops\\nPetrobrasaurus\\nPhaedrolosaurus\\nPhilovenator\\nPhuwiangosaurus\\nPhyllodon\\nPiatnitzkysaurus\\nPicrodon\\nPinacosaurus\\nPisanosaurus\\nPitekunsaurus\\nPiveteausaurus\\nPlanicoxa\\nPlateosauravus\\nPlateosaurus\\nPlatyceratops\\nPlesiohadros\\nPleurocoelus\\nPleuropeltus\\nPneumatoarthrus\\nPneumatoraptor\\nPodokesaurus\\nPoekilopleuron\\nPolacanthoides\\nPolacanthus\\nPolyodontosaurus\\nPolyonax\\nPonerosteus\\nPoposaurus\\nParasaurolophus\\nPostosuchus\\nPowellvenator\\nPradhania\\nPrenocephale\\nPrenoceratops\\nPriconodon\\nPriodontognathus\\nProa\\nProbactrosaurus\\nProbrachylophosaurus\\nProceratops\\nProceratosaurus\\nProcerosaurus\\nProcerosaurus\\nProcheneosaurus\\nProcompsognathus\\nProdeinodon\\nProiguanodon\\nPropanoplosaurus\\nProplanicoxa\\nProsaurolophus\\nProtarchaeopteryx\\nProtecovasaurus\\nProtiguanodon\\nProtoavis\\nProtoceratops\\nProtognathosaurus\\nProtognathus\\nProtohadros\\nProtorosaurus\\nProtorosaurus\\nProtrachodon\\nProyandusaurus\\nPseudolagosuchus\\nPsittacosaurus\\nPteropelyx\\nPterospondylus\\nPuertasaurus\\nPukyongosaurus\\nPulanesaura\\nPycnonemosaurus\\nPyroraptor\\nQantassaurus\\nQianzhousaurus\\nQiaowanlong\\nQijianglong\\nQinlingosaurus\\nQingxiusaurus\\nQiupalong\\nQuaesitosaurus\\nQuetecsaurus\\nQuilmesaurus\\nRachitrema\\nRahiolisaurus\\nRahona\\nRahonavis\\nRajasaurus\\nRapator\\nRapetosaurus\\nRaptorex\\nRatchasimasaurus\\nRativates\\nRayososaurus\\nRazanandrongobe\\nRebbachisaurus\\nRegaliceratops\\nRegnosaurus\\nRevueltosaurus\\nRhabdodon\\nRhadinosaurus\\nRhinorex\\nRhodanosaurus\\nRhoetosaurus\\nRhopalodon\\nRiabininohadros\\nRichardoestesia\\nRileya\\nRileyasuchus\\nRinchenia\\nRinconsaurus\\nRioarribasaurus\\nRiodevasaurus\\nRiojasaurus\\nRiojasuchus\\nRocasaurus\\nRoccosaurus\\nRubeosaurus\\nRuehleia\\nRugocaudia\\nRugops\\nRukwatitan\\nRuyangosaurus\\nSacisaurus\\nSahaliyania\\nSaichania\\nSaldamosaurus\\nSalimosaurus\\nSaltasaurus\\nSaltopus\\nSaltriosaurus\\nSanchusaurus\\nSangonghesaurus\\nSanjuansaurus\\nSanpasaurus\\nSantanaraptor\\nSaraikimasoom\\nSarahsaurus\\nSarcolestes\\nSarcosaurus\\nSarmientosaurus\\nSaturnalia\\nSauraechinodon\\nSaurolophus\\nSauroniops\\nSauropelta\\nSaurophaganax\\nSaurophagus\\nSauroplites\\nSauroposeidon\\nSaurornithoides\\nSaurornitholestes\\nSavannasaurus\\nScansoriopteryx\\nScaphonyx\\nScelidosaurus\\nScipionyx\\nSciurumimus\\nScleromochlus\\nScolosaurus\\nScutellosaurus\\nSecernosaurus\\nSefapanosaurus\\nSegisaurus\\nSegnosaurus\\nSeismosaurus\\nSeitaad\\nSelimanosaurus\\nSellacoxa\\nSellosaurus\\nSerendipaceratops\\nSerikornis\\nShamosaurus\\nShanag\\nShanshanosaurus\\nShantungosaurus\\nShanxia\\nShanyangosaurus\\nShaochilong\\nShenzhousaurus\\nShidaisaurus\\nShingopana\\nShixinggia\\nShuangbaisaurus\\nShuangmiaosaurus\\nShunosaurus\\nShuvosaurus\\nShuvuuia\\nSiamodon\\nSiamodracon\\nSiamosaurus\\nSiamotyrannus\\nSiats\\nSibirosaurus\\nSibirotitan\\nSidormimus\\nSigilmassasaurus\\nSilesaurus\\nSiluosaurus\\nSilvisaurus\\nSimilicaudipteryx\\nSinocalliopteryx\\nSinoceratops\\nSinocoelurus\\nSinopelta\\nSinopeltosaurus\\nSinornithoides\\nSinornithomimus\\nSinornithosaurus\\nSinosauropteryx\\nSinosaurus\\nSinotyrannus\\nSinovenator\\nSinraptor\\nSinusonasus\\nSirindhorna\\nSkorpiovenator\\nSmilodon\\nSonidosaurus\\nSonorasaurus\\nSoriatitan\\nSphaerotholus\\nSphenosaurus\\nSphenospondylus\\nSpiclypeus\\nSpinophorosaurus\\nSpinops\\nSpinosaurus\\nSpinostropheus\\nSpinosuchus\\nSpondylosoma\\nSqualodon\\nStaurikosaurus\\nStegoceras\\nStegopelta\\nStegosaurides\\nStegosaurus\\nStenonychosaurus\\nStenopelix\\nStenotholus\\nStephanosaurus\\nStereocephalus\\nSterrholophus\\nStokesosaurus\\nStormbergia\\nStrenusaurus\\nStreptospondylus\\nStruthiomimus\\nStruthiosaurus\\nStygimoloch\\nStygivenator\\nStyracosaurus\\nSuccinodon\\nSuchomimus\\nSuchosaurus\\nSuchoprion\\nSugiyamasaurus\\nSkeleton\\nSulaimanisaurus\\nSupersaurus\\nSuuwassea\\nSuzhousaurus\\nSymphyrophus\\nSyngonosaurus\\nSyntarsus\\nSyrmosaurus\\nSzechuanosaurus\\nTachiraptor\\nTalarurus\\nTalenkauen\\nTalos\\nTambatitanis\\nTangvayosaurus\\nTanius\\nTanycolagreus\\nTanystropheus\\nTanystrosuchus\\nTaohelong\\nTapinocephalus\\nTapuiasaurus\\nTarascosaurus\\nTarbosaurus\\nTarchia\\nTastavinsaurus\\nTatankacephalus\\nTatankaceratops\\nTataouinea\\nTatisaurus\\nTaurovenator\\nTaveirosaurus\\nTawa\\nTawasaurus\\nTazoudasaurus\\nTechnosaurus\\nTecovasaurus\\nTehuelchesaurus\\nTeihivenator\\nTeinurosaurus\\nTeleocrater\\nTelmatosaurus\\nTenantosaurus\\nTenchisaurus\\nTendaguria\\nTengrisaurus\\nTenontosaurus\\nTeratophoneus\\nTeratosaurus\\nTermatosaurus\\nTethyshadros\\nTetragonosaurus\\nTexacephale\\nTexasetes\\nTeyuwasu\\nThecocoelurus\\nThecodontosaurus\\nThecospondylus\\nTheiophytalia\\nTherizinosaurus\\nTherosaurus\\nThescelosaurus\\nThespesius\\nThotobolosaurus\\nTianchisaurus\\nTianchungosaurus\\nTianyulong\\nTianyuraptor\\nTianzhenosaurus\\nTichosteus\\nTienshanosaurus\\nTimimus\\nTimurlengia\\nTitanoceratops\\nTitanosaurus\\nTitanosaurus\\nTochisaurus\\nTomodon\\nTonganosaurus\\nTongtianlong\\nTonouchisaurus\\nTorilion\\nTornieria\\nTorosaurus\\nTorvosaurus\\nTototlmimus\\nTrachodon\\nTraukutitan\\nTrialestes\\nTriassolestes\\nTribelesodon\\nTriceratops\\nTrigonosaurus\\nTrimucrodon\\nTrinisaura\\nTriunfosaurus\\nTroodon\\nTsaagan\\nTsagantegia\\nTsintaosaurus\\nTugulusaurus\\nTuojiangosaurus\\nTuranoceratops\\nTuriasaurus\\nTylocephale\\nTylosteus\\nTyrannosaurus\\nTyrannotitan\\nIllustration\\nUberabatitan\\nUdanoceratops\\nUgrosaurus\\nUgrunaaluk\\nUintasaurus\\nUltrasauros\\nUltrasaurus\\nUltrasaurus\\nUmarsaurus\\nUnaysaurus\\nUnenlagia\\nUnescoceratops\\nUnicerosaurus\\nUnquillosaurus\\nUrbacodon\\nUtahceratops\\nUtahraptor\\nUteodon\\nVagaceratops\\nVahiny\\nValdoraptor\\nValdosaurus\\nVariraptor\\nVelociraptor\\nVectensia\\nVectisaurus\\nVelafrons\\nVelocipes\\nVelociraptor\\nVelocisaurus\\nVenaticosuchus\\nVenenosaurus\\nVeterupristisaurus\\nViavenator\\nVitakridrinda\\nVitakrisaurus\\nVolkheimeria\\nVouivria\\nVulcanodon\\nWadhurstia\\nWakinosaurus\\nWalgettosuchus\\nWalkeria\\nWalkersaurus\\nWangonisaurus\\nWannanosaurus\\nWellnhoferia\\nWendiceratops\\nWiehenvenator\\nWillinakaqe\\nWintonotitan\\nWuerhosaurus\\nWulagasaurus\\nWulatelong\\nWyleyia\\nWyomingraptor\\nXenoceratops\\nXenoposeidon\\nXenotarsosaurus\\nXianshanosaurus\\nXiaosaurus\\nXingxiulong\\nXinjiangovenator\\nXinjiangtitan\\nXiongguanlong\\nXixianykus\\nXixiasaurus\\nXixiposaurus\\nXuanhanosaurus\\nXuanhuaceratops\\nXuanhuasaurus\\nXuwulong\\nYaleosaurus\\nYamaceratops\\nYandusaurus\\nYangchuanosaurus\\nYaverlandia\\nYehuecauhceratops\\nYezosaurus\\nYibinosaurus\\nYimenosaurus\\nYingshanosaurus\\nYinlong\\nYixianosaurus\\nYizhousaurus\\nYongjinglong\\nYuanmouraptor\\nYuanmousaurus\\nYueosaurus\\nYulong\\nYunganglong\\nYunmenglong\\nYunnanosaurus\\nYunxianosaurus\\nYurgovuchia\\nYutyrannus\\nZanabazar\\nZanclodon\\nZapalasaurus\\nZapsalis\\nZaraapelta\\nZatomusZby\\nZephyrosaurus\\nZhanghenglong\\nZhejiangosaurus\\nZhenyuanlong\\nZhongornis\\nZhongjianosaurus\\nZhongyuansaurus\\nZhuchengceratops\\nZhuchengosaurus\\nZhuchengtitan\\nZhuchengtyrannus\\nZiapelta\\nZigongosaurus\\nZizhongosaurus\\nZuniceratops\\nZunityrannus\\nZuolong\\nZuoyunlong\\nZupaysaurus\\nZuul'"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data = open(\"dinos.txt\", \"r\").read()\n",
    "data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "这些字符是a-z（26个英文字符）加上“\\n”（换行字符），在这里换行字符起到了在视频中类似的EOS（句子结尾）的作用，这里表示了名字的结束而不是句子的结尾。下面我们将创建一个字典，每个字符映射到0-26的索引，然后再创建一个字典，它将该字典将每个索引映射回相应的字符字符，它会帮助我们找出softmax层的概率分布输出中的字符。我们来创建char_to_ix 与 ix_to_char字典。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'\\n': 0, 'a': 1, 'b': 2, 'c': 3, 'd': 4, 'e': 5, 'f': 6, 'g': 7, 'h': 8, 'i': 9, 'j': 10, 'k': 11, 'l': 12, 'm': 13, 'n': 14, 'o': 15, 'p': 16, 'q': 17, 'r': 18, 's': 19, 't': 20, 'u': 21, 'v': 22, 'w': 23, 'x': 24, 'y': 25, 'z': 26}\n",
      "{0: '\\n', 1: 'a', 2: 'b', 3: 'c', 4: 'd', 5: 'e', 6: 'f', 7: 'g', 8: 'h', 9: 'i', 10: 'j', 11: 'k', 12: 'l', 13: 'm', 14: 'n', 15: 'o', 16: 'p', 17: 'q', 18: 'r', 19: 's', 20: 't', 21: 'u', 22: 'v', 23: 'w', 24: 'x', 25: 'y', 26: 'z'}\n"
     ]
    }
   ],
   "source": [
    "char_to_ix = {ch:i for i, ch in enumerate(sorted(chars))}\n",
    "ix_to_char = {i:ch for i, ch in enumerate(sorted(chars))}\n",
    "\n",
    "print(char_to_ix)\n",
    "print(ix_to_char)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 模型回顾\n",
    "模型的结构如下：\n",
    "\n",
    "- 初始化参数\n",
    "\n",
    "- 循环：\n",
    "\n",
    "  - 前向传播计算损失\n",
    "\n",
    "  - 反向传播计算关于损失的梯度\n",
    "\n",
    "  - 修剪梯度以免梯度爆炸\n",
    "\n",
    "  - 用梯度下降更新规则更新参数。\n",
    "\n",
    "- 返回学习后了的参数\n",
    "<img src=\"6.png\" style=\"width:450;height:300px;\">\n",
    "在每个时间步，RNN会预测给定字符的下一个字符是什么。数据集  $X = (x^{\\langle 1 \\rangle}, x^{\\langle 2 \\rangle}, ..., x^{\\langle T_x \\rangle})$ 是一个列表类型的字符训练集，同时$Y = (y^{\\langle 1 \\rangle}, y^{\\langle 2 \\rangle}, ..., y^{\\langle T_x \\rangle})$在每个时间步t亦是如此，因此$y^{\\langle t \\rangle} = x^{\\langle t+1 \\rangle}$. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 构建模型中的模块\n",
    "在这部分，我们将来构建整个模型中的两个重要的模块：\n",
    "- 梯度修剪：避免梯度爆炸\n",
    "- 取样:一种用来产生字符的技术\n",
    "### 梯度修剪\n",
    " 在这里，我们将实现在优化循环中调用的clip函数。回想一下，整个循环结构通常包括前向传播、成本计算、反向传播和参数更新。在更新参数之前，我们将在需要时执行梯度修剪，以确保我们的梯度不是“爆炸”的。\n",
    "\n",
    " 接下来我们将实现一个修剪函数，该函数输入一个梯度字典输出一个已经修剪过了的梯度。有很多的方法来修剪梯度，我们在这里使用一个比较简单的方法。梯度向量的每一个元素都被限制在[−N，N]的范围，通俗的说，有一个maxValue（比如10），如果梯度的任何值大于10，那么它将被设置为10，如果梯度的任何值小于-10，那么它将被设置为-10，如果它在-10与10之间，那么它将不变。\n",
    " <img src=\"7.png\" style=\"width:400;height:150px;\">"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clip(gradients, maxValue):\n",
    "    \"\"\"\n",
    "    使用maxValue来修剪梯度\n",
    "    \n",
    "    参数：\n",
    "        gradients -- 字典类型，包含了以下参数：\"dWaa\", \"dWax\", \"dWya\", \"db\", \"dby\"\n",
    "        maxValue -- 阈值，把梯度值限制在[-maxValue, maxValue]内\n",
    "        \n",
    "    返回：\n",
    "        gradients -- 修剪后的梯度\n",
    "    \"\"\"\n",
    "    # 获取参数\n",
    "    dWaa, dWax, dWya, db, dby = gradients['dWaa'], gradients['dWax'], gradients['dWya'], gradients['db'], gradients['dby']\n",
    "    \n",
    "    # 梯度修剪\n",
    "    for gradient in [dWaa, dWax, dWya, db, dby]:\n",
    "        np.clip(gradient, -maxValue, maxValue, out=gradient)\n",
    "\n",
    "    gradients = {\"dWaa\": dWaa, \"dWax\": dWax, \"dWya\": dWya, \"db\": db, \"dby\": dby}\n",
    "    \n",
    "    return gradients"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 采样\n",
    " 现在假设我们的模型已经训练过了，我们希望生成新的文本，生成的过程如下图：\n",
    " <img src=\"images/8.png\" style=\"width:500;height:300px;\">\n",
    "  在这幅图中，我们假设模型已经经过了训练。我们在第一步传入$x^{\\langle 1\\rangle} = \\vec{0}$，然后让网络一次对一个字符进行采样。\n",
    " 现在我们来实现sample函数，它由以下四步构成：\n",
    "- 步骤1：网络的第一个“虚”输入$x^{\\langle 1 \\rangle} = \\vec{0}$（零向量），这是在生成字符之前的默认输入，同时我们设置 $a^{\\langle 0 \\rangle} = \\vec{0}$\n",
    "- 步骤2：运行一次前向传播，然后得到$a^{\\langle 1 \\rangle}$ and $\\hat{y}^{\\langle 1 \\rangle}$ 公式如下:\n",
    "$$ a^{\\langle t+1 \\rangle} = \\tanh(W_{ax}  x^{\\langle t \\rangle } + W_{aa} a^{\\langle t \\rangle } + b)\\tag{1}$$\n",
    "\n",
    "$$ z^{\\langle t + 1 \\rangle } = W_{ya}  a^{\\langle t + 1 \\rangle } + b_y \\tag{2}$$\n",
    "\n",
    "$$ \\hat{y}^{\\langle t+1 \\rangle } = softmax(z^{\\langle t + 1 \\rangle })\\tag{3}$$需要注意的是$\\hat{y}^{\\langle t+1 \\rangle }$ 是一个softmax概率向量（其下的值在0到1之间，总和为1），$\\hat{y}^{\\langle t+1 \\rangle}_i$ 表示索引“i”的字符是下一个字符的概率\n",
    "- 步骤3：采样：根据 $\\hat{y}^{\\langle t+1 \\rangle }$指定的概率分布选择下一个字符的索引，假如$\\hat{y}^{\\langle t+1 \\rangle }_i = 0.16$那么选择索引“i”的概率为16%，为了实现它，我们可以使用np.random.choice函数，下面是np.random.choice()使用的例子：\n",
    "```python\n",
    "np.random.seed(0)\n",
    "p = np.array([0.1, 0.0, 0.7, 0.2])\n",
    "index = np.random.choice([0, 1, 2, 3], p = p.ravel())\n",
    "```\n",
    "这意味着你将根据分布选择索引\n",
    "$P(index = 0) = 0.1, P(index = 1) = 0.0, P(index = 2) = 0.7, P(index = 3) = 0.2$.\n",
    "- 步骤4：在sample()中实现的最后一步是用 $x^{\\langle t +1\\rangle }$的值覆盖变量x(当前存储$x^{\\langle t \\rangle }$)我们将通过创建一个与我们所选择的字符相对应的一个独热向量来表示 $x^{\\langle t +1\\rangle }$。然后在步骤1中向前传播 $x^{\\langle t +1\\rangle }$,并不断重复这个过程，直到得到一个“\\n”字符，表明已经到达恐龙名称的末尾。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sample(parameters, char_to_ix, seed):\n",
    "    \"\"\"\n",
    "    根据RNN输出的概率分布序列对字符序列进行采样\n",
    "    \n",
    "    参数：\n",
    "        parameters -- 包含了Waa, Wax, Wya, by, b的字典\n",
    "        char_to_ix -- 字符映射到索引的字典\n",
    "        seed -- 随机种子\n",
    "        \n",
    "    返回：\n",
    "        indices -- 包含采样字符索引的长度为n的列表。\n",
    "    \"\"\"\n",
    "    \n",
    "    # 从parameters 中获取参数\n",
    "    Waa, Wax, Wya, by, b = parameters['Waa'], parameters['Wax'], parameters['Wya'], parameters['by'], parameters['b']\n",
    "    vocab_size = by.shape[0]\n",
    "    n_a = Waa.shape[1]\n",
    "    \n",
    "    # 步骤1 \n",
    "    ## 创建独热向量x\n",
    "    x = np.zeros((vocab_size,1))\n",
    "    \n",
    "    ## 使用0初始化a_prev\n",
    "    a_prev = np.zeros((n_a,1))\n",
    "    \n",
    "    # 创建索引的空列表，这是包含要生成的字符的索引的列表。\n",
    "    indices = []\n",
    "    \n",
    "    # IDX是检测换行符的标志，我们将其初始化为-1。\n",
    "    idx = -1\n",
    "    \n",
    "    # 循环遍历时间步骤t。在每个时间步中，从概率分布中抽取一个字符，\n",
    "    # 并将其索引附加到“indices”上，如果我们达到50个字符，\n",
    "    #（我们应该不太可能有一个训练好的模型），我们将停止循环，这有助于调试并防止进入无限循环\n",
    "    counter = 0\n",
    "    newline_character = char_to_ix[\"\\n\"]\n",
    "    \n",
    "    while (idx != newline_character and counter < 50):\n",
    "        # 步骤2：使用公式1、2、3进行前向传播\n",
    "        a = np.tanh(np.dot(Wax, x) + np.dot(Waa, a_prev) + b)\n",
    "        z = np.dot(Wya, a) + by\n",
    "        y = cllm_utils.softmax(z)\n",
    "        \n",
    "        # 设定随机种子\n",
    "        np.random.seed(counter + seed)\n",
    "        \n",
    "        # 步骤3：从概率分布y中抽取词汇表中字符的索引\n",
    "        idx = np.random.choice(list(range(vocab_size)), p=y.ravel())\n",
    "        \n",
    "        # 添加到索引中\n",
    "        indices.append(idx)\n",
    "        \n",
    "        # 步骤4:将输入字符重写为与采样索引对应的字符。\n",
    "        x = np.zeros((vocab_size,1))\n",
    "        x[idx] = 1\n",
    "        \n",
    "        # 更新a_prev为a\n",
    "        a_prev = a \n",
    "        \n",
    "        # 累加器\n",
    "        seed += 1\n",
    "        counter +=1\n",
    "    \n",
    "    if(counter == 50):\n",
    "        indices.append(char_to_ix[\"\\n\"])\n",
    "    \n",
    "    return indices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sampling:\n",
      "list of sampled indices: [12, 17, 24, 14, 13, 9, 10, 22, 24, 6, 13, 11, 12, 6, 21, 15, 21, 14, 3, 2, 1, 21, 18, 24, 7, 25, 6, 25, 18, 10, 16, 2, 3, 8, 15, 12, 11, 7, 1, 12, 10, 2, 7, 7, 11, 17, 24, 12, 3, 1, 0]\n",
      "list of sampled characters: ['l', 'q', 'x', 'n', 'm', 'i', 'j', 'v', 'x', 'f', 'm', 'k', 'l', 'f', 'u', 'o', 'u', 'n', 'c', 'b', 'a', 'u', 'r', 'x', 'g', 'y', 'f', 'y', 'r', 'j', 'p', 'b', 'c', 'h', 'o', 'l', 'k', 'g', 'a', 'l', 'j', 'b', 'g', 'g', 'k', 'q', 'x', 'l', 'c', 'a', '\\n']\n"
     ]
    }
   ],
   "source": [
    "np.random.seed(2)\n",
    "_, n_a = 20, 100\n",
    "Wax, Waa, Wya = np.random.randn(n_a, vocab_size), np.random.randn(n_a, n_a), np.random.randn(vocab_size, n_a)\n",
    "b, by = np.random.randn(n_a, 1), np.random.randn(vocab_size, 1)\n",
    "parameters = {\"Wax\": Wax, \"Waa\": Waa, \"Wya\": Wya, \"b\": b, \"by\": by}\n",
    "\n",
    "\n",
    "indices = sample(parameters, char_to_ix, 0)\n",
    "print(\"Sampling:\")\n",
    "print(\"list of sampled indices:\", indices)\n",
    "print(\"list of sampled characters:\", [ix_to_char[i] for i in indices])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 构建语言模型\n",
    "### 梯度下降\n",
    " 在这里，我们将实现一个执行随机梯度下降的一个步骤的函数（带有梯度修剪）。我们将一次训练一个样本，所以优化算法将是随机梯度下降，这里是RNN的一个通用的优化循环的步骤：\n",
    "\n",
    "- 前向传播计算损失\n",
    "\n",
    "- 反向传播计算关于参数的梯度损失\n",
    "\n",
    "- 修剪梯度\n",
    "\n",
    "- 使用梯度下降更新参数\n",
    "\n",
    " 我们来实现这一优化过程（单步随机梯度下降），这里我们提供了一些函数\n",
    "\n",
    "```python\n",
    "def rnn_forward(X, Y, a_prev, parameters):\n",
    "    \"\"\"\n",
    "    通过RNN进行前向传播，计算交叉熵损失。\n",
    "\n",
    "    它返回损失的值以及存储在反向传播中使用的“缓存”值。\n",
    "    \"\"\"\n",
    "    ....\n",
    "    return loss, cache\n",
    "    \n",
    "def rnn_backward(X, Y, parameters, cache):\n",
    "    \"\"\" \n",
    "    通过时间进行反向传播，计算相对于参数的梯度损失。它还返回所有隐藏的状态\n",
    "    \"\"\"\n",
    "    ...\n",
    "    return gradients, a\n",
    "\n",
    "def update_parameters(parameters, gradients, learning_rate):\n",
    "    \"\"\"\n",
    "    Updates parameters using the Gradient Descent Update Rule\n",
    "    \"\"\"\n",
    "    ...\n",
    "    return parameters\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "code_folding": []
   },
   "outputs": [],
   "source": [
    "def optimize(X, Y, a_prev, parameters, learning_rate = 0.01):\n",
    "    \"\"\"\n",
    "    执行训练模型的单步优化。\n",
    "    \n",
    "    参数：\n",
    "        X -- 整数列表，其中每个整数映射到词汇表中的字符。\n",
    "        Y -- 整数列表，与X完全相同，但向左移动了一个索引。\n",
    "        a_prev -- 上一个隐藏状态\n",
    "        parameters -- 字典，包含了以下参数：\n",
    "                        Wax -- 权重矩阵乘以输入，维度为(n_a, n_x)\n",
    "                        Waa -- 权重矩阵乘以隐藏状态，维度为(n_a, n_a)\n",
    "                        Wya -- 隐藏状态与输出相关的权重矩阵，维度为(n_y, n_a)\n",
    "                        b -- 偏置，维度为(n_a, 1)\n",
    "                        by -- 隐藏状态与输出相关的权重偏置，维度为(n_y, 1)\n",
    "        learning_rate -- 模型学习的速率\n",
    "    \n",
    "    返回：\n",
    "        loss -- 损失函数的值（交叉熵损失）\n",
    "        gradients -- 字典，包含了以下参数：\n",
    "                        dWax -- 输入到隐藏的权值的梯度，维度为(n_a, n_x)\n",
    "                        dWaa -- 隐藏到隐藏的权值的梯度，维度为(n_a, n_a)\n",
    "                        dWya -- 隐藏到输出的权值的梯度，维度为(n_y, n_a)\n",
    "                        db -- 偏置的梯度，维度为(n_a, 1)\n",
    "                        dby -- 输出偏置向量的梯度，维度为(n_y, 1)\n",
    "        a[len(X)-1] -- 最后的隐藏状态，维度为(n_a, 1)\n",
    "    \"\"\"\n",
    "    \n",
    "    # 前向传播\n",
    "    loss, cache = cllm_utils.rnn_forward(X, Y, a_prev, parameters)\n",
    "    \n",
    "    # 反向传播\n",
    "    gradients, a = cllm_utils.rnn_backward(X, Y, parameters, cache)\n",
    "    \n",
    "    # 梯度修剪，[-5 , 5]\n",
    "    gradients = clip(gradients,5)\n",
    "    \n",
    "    # 更新参数\n",
    "    parameters = cllm_utils.update_parameters(parameters,gradients,learning_rate)\n",
    "    \n",
    "    return loss, gradients, a[len(X)-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss = 126.50397572165345\n",
      "gradients[\"dWaa\"][1][2] = 0.19470931534725341\n",
      "np.argmax(gradients[\"dWax\"]) = 93\n",
      "gradients[\"dWya\"][1][2] = -0.007773876032004315\n",
      "gradients[\"db\"][4] = [-0.06809825]\n",
      "gradients[\"dby\"][1] = [0.01538192]\n",
      "a_last[4] = [-1.]\n"
     ]
    }
   ],
   "source": [
    "np.random.seed(1)\n",
    "vocab_size, n_a = 27, 100\n",
    "a_prev = np.random.randn(n_a, 1)\n",
    "Wax, Waa, Wya = np.random.randn(n_a, vocab_size), np.random.randn(n_a, n_a), np.random.randn(vocab_size, n_a)\n",
    "b, by = np.random.randn(n_a, 1), np.random.randn(vocab_size, 1)\n",
    "parameters = {\"Wax\": Wax, \"Waa\": Waa, \"Wya\": Wya, \"b\": b, \"by\": by}\n",
    "X = [12,3,5,11,22,3]\n",
    "Y = [4,14,11,22,25, 26]\n",
    "\n",
    "loss, gradients, a_last = optimize(X, Y, a_prev, parameters, learning_rate = 0.01)\n",
    "print(\"Loss =\", loss)\n",
    "print(\"gradients[\\\"dWaa\\\"][1][2] =\", gradients[\"dWaa\"][1][2])\n",
    "print(\"np.argmax(gradients[\\\"dWax\\\"]) =\", np.argmax(gradients[\"dWax\"]))\n",
    "print(\"gradients[\\\"dWya\\\"][1][2] =\", gradients[\"dWya\"][1][2])\n",
    "print(\"gradients[\\\"db\\\"][4] =\", gradients[\"db\"][4])\n",
    "print(\"gradients[\\\"dby\\\"][1] =\", gradients[\"dby\"][1])\n",
    "print(\"a_last[4] =\", a_last[4])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 训练模型\n",
    "给定恐龙名称的数据集，我们使用数据集的每一行(一个名称)作为一个训练样本。每100步随机梯度下降，你将抽样10个随机选择的名字，看看算法是怎么做的。记住要打乱数据集，以便随机梯度下降以随机顺序访问样本。当examples[index]包含一个恐龙名称（String）时，为了创建一个样本（X,Y），你可以使用这个\n",
    "```python\n",
    "        index = j % len(examples)\n",
    "        X = [None] + [char_to_ix[ch] for ch in examples[index]] \n",
    "        Y = X[1:] + [char_to_ix[\"\\n\"]]\n",
    "```\n",
    "需要注意的是我们使用了`index= j % len(examples)`，其中`j = 1....num_iterations`，为了确保examples[index]总是有效的（index小于len(examples)）,rnn_forward()会将X的第一个值None解释为$x^{\\langle 0 \\rangle} = \\vec{0}$。此外，为了确保Y等于X，会向左移动一步，并添加一个附加的“\\n”以表示恐龙名称的结束。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def model(data, ix_to_char, char_to_ix, num_iterations=3500, \n",
    "          n_a=50, dino_names=7,vocab_size=27):\n",
    "    \"\"\"\n",
    "    训练模型并生成恐龙名字\n",
    "    \n",
    "    参数：\n",
    "        data -- 语料库\n",
    "        ix_to_char -- 索引映射字符字典\n",
    "        char_to_ix -- 字符映射索引字典\n",
    "        num_iterations -- 迭代次数\n",
    "        n_a -- RNN单元数量\n",
    "        dino_names -- 每次迭代中采样的数量\n",
    "        vocab_size -- 在文本中的唯一字符的数量\n",
    "    \n",
    "    返回：\n",
    "        parameters -- 学习后了的参数\n",
    "    \"\"\"\n",
    "    \n",
    "    # 从vocab_size中获取n_x、n_y\n",
    "    n_x, n_y = vocab_size, vocab_size\n",
    "    \n",
    "    # 初始化参数\n",
    "    parameters = cllm_utils.initialize_parameters(n_a, n_x, n_y)\n",
    "    \n",
    "    # 初始化损失\n",
    "    loss = cllm_utils.get_initial_loss(vocab_size, dino_names)\n",
    "    \n",
    "    # 构建恐龙名称列表\n",
    "    with open(\"dinos.txt\") as f:\n",
    "        examples = f.readlines()\n",
    "    examples = [x.lower().strip() for x in examples]\n",
    "    \n",
    "    # 打乱全部的恐龙名称\n",
    "    np.random.seed(0)\n",
    "    np.random.shuffle(examples)\n",
    "    \n",
    "    # 初始化LSTM隐藏状态\n",
    "    a_prev = np.zeros((n_a,1))\n",
    "    \n",
    "    # 循环\n",
    "    for j in range(num_iterations):\n",
    "        # 定义一个训练样本\n",
    "        index = j % len(examples)\n",
    "        X = [None] + [char_to_ix[ch] for ch in examples[index]] \n",
    "        Y = X[1:] + [char_to_ix[\"\\n\"]]\n",
    "        \n",
    "        # 执行单步优化：前向传播 -> 反向传播 -> 梯度修剪 -> 更新参数\n",
    "        # 选择学习率为0.01\n",
    "        curr_loss, gradients, a_prev = optimize(X, Y, a_prev, parameters)\n",
    "        \n",
    "        # 使用延迟来保持损失平滑,这是为了加速训练。\n",
    "        loss = cllm_utils.smooth(loss, curr_loss)\n",
    "        \n",
    "        # 每2000次迭代，通过sample()生成“\\n”字符，检查模型是否学习正确\n",
    "        if j % 2000 == 0:\n",
    "            print(\"第\" + str(j+1) + \"次迭代，损失值为：\" + str(loss))\n",
    "            \n",
    "            seed = 0\n",
    "            for name in range(dino_names):\n",
    "                # 采样\n",
    "                sampled_indices = sample(parameters, char_to_ix, seed)\n",
    "                cllm_utils.print_sample(sampled_indices, ix_to_char)\n",
    "                \n",
    "                # 为了得到相同的效果，随机种子+1\n",
    "                seed += 1\n",
    "            \n",
    "            print(\"\\n\")\n",
    "    return parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "D:\\Anaconda3\\lib\\site-packages\\ipykernel_launcher.py:2: DeprecationWarning: time.clock has been deprecated in Python 3.3 and will be removed from Python 3.8: use time.perf_counter or time.process_time instead\n",
      "  \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "第1次迭代，损失值为：23.087336085484605\n",
      "Nkzxwtdmfqoeyhsqwasjkjvu\n",
      "Kneb\n",
      "Kzxwtdmfqoeyhsqwasjkjvu\n",
      "Neb\n",
      "Zxwtdmfqoeyhsqwasjkjvu\n",
      "Eb\n",
      "Xwtdmfqoeyhsqwasjkjvu\n",
      "\n",
      "\n",
      "第2001次迭代，损失值为：27.884160491415773\n",
      "Liusskeomnolxeros\n",
      "Hmdaairus\n",
      "Hytroligoraurus\n",
      "Lecalosapaus\n",
      "Xusicikoraurus\n",
      "Abalpsamantisaurus\n",
      "Tpraneronxeros\n",
      "\n",
      "\n",
      "执行了：0分4秒\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "D:\\Anaconda3\\lib\\site-packages\\ipykernel_launcher.py:8: DeprecationWarning: time.clock has been deprecated in Python 3.3 and will be removed from Python 3.8: use time.perf_counter or time.process_time instead\n",
      "  \n"
     ]
    }
   ],
   "source": [
    "#开始时间\n",
    "start_time = time.clock()\n",
    "\n",
    "#开始训练\n",
    "parameters = model(data, ix_to_char, char_to_ix, num_iterations=3500)\n",
    "\n",
    "#结束时间\n",
    "end_time = time.clock()\n",
    "\n",
    "#计算时差\n",
    "minium = end_time - start_time\n",
    "\n",
    "print(\"执行了：\" + str(int(minium / 60)) + \"分\" + str(int(minium%60)) + \"秒\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    },
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'music21'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-16-2047214b9e64>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      8\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mIPython\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      9\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0msys\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 10\u001b[1;33m \u001b[1;32mfrom\u001b[0m \u001b[0mmusic21\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[1;33m*\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     11\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0mgrammar\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[1;33m*\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     12\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0mqa\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[1;33m*\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mModuleNotFoundError\u001b[0m: No module named 'music21'"
     ]
    }
   ],
   "source": [
    "from keras.models import load_model, Model\n",
    "from keras.layers import Dense, Activation, Dropout, Input, LSTM, Reshape, Lambda, RepeatVector\n",
    "from keras.initializers import glorot_uniform\n",
    "from keras.utils import to_categorical\n",
    "from keras.optimizers import Adam\n",
    "from keras import backend as K\n",
    "import numpy as np\n",
    "import IPython\n",
    "import sys\n",
    "from music21 import *\n",
    "from grammar import *\n",
    "from qa import *\n",
    "from preprocess import * \n",
    "from music_utils import *\n",
    "from data_utils import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {
    "height": "calc(100% - 180px)",
    "left": "10px",
    "top": "150px",
    "width": "304.475px"
   },
   "toc_section_display": true,
   "toc_window_display": true
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
