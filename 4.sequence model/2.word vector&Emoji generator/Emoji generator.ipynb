{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "在这里，我们要学习使用词向量来构建一个表情生成器。\n",
    "\n",
    "  你有没有想过让你的文字也有更丰富表达能力呢？比如写下“Congratulations on the promotion! Lets get coffee and talk. Love you!”，那么你的表情生成器就会自动生成“Congratulations on the promotion! 👍 Lets get coffee and talk. ☕️ Love you! ❤️”。\n",
    "\n",
    "  另一方面，如果你对这些表情不感冒，而你的朋友给你发了一大堆的带表情的文字，那么你也可以使用表情生成器来怼回去。\n",
    "\n",
    "  我们要构建一个模型，输入的是文字（比如“Let’s go see the baseball game tonight!”），输出的是表情（⚾️）。在众多的Emoji表情中，比如“❤️”代表的是“心”而不是“爱”，但是如果你使用词向量，那么你会发现即使你的训练集只明确地将几个单词与特定的表情符号相关联，你的模型也了能够将测试集中的单词归纳、总结到同一个表情符号，甚至有些单词没有出现在你的训练集中也可以。\n",
    "\n",
    "  在这里，我们将开始构建一个使用词向量的基准模型（Emojifier-V1），然后我们会构建一个更复杂的包含了LSTM的模型（Emojifier-V2）。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import emo_utils\n",
    "import emoji\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 基准模型：Emojifier-V1\n",
    "## 数据集\n",
    "我们来构建一个简单的分类器，首先是数据集（X，Y）：\n",
    "- X：包含了127个字符串类型的短句\n",
    "- Y：包含了对应短句的标签（0-4）\n",
    "<img src=\"images/1.png\" style=\"width:700px;height:300px;\">"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, Y_train = emo_utils.read_csv('data/train_emoji.csv')\n",
    "X_test, Y_test = emo_utils.read_csv('data/test.csv')\n",
    "\n",
    "maxLen = len(max(X_train, key=len).split())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Miss you so much ❤️\n"
     ]
    }
   ],
   "source": [
    "index  = 3\n",
    "print(X_train[index], emo_utils.label_to_emoji(Y_train[index]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Emojifier-V1的结构\n",
    "在这里，我们要实现一个叫“Emojifier-V1”的基准模型。\n",
    "<img src=\"images/2.png\" style=\"width:900px;height:300px;\">\n",
    "模型的输入是一段文字（比如“l lov you”），输出的是维度为(1,5)的向量，最后在argmax层找寻最大可能性的输出。\n",
    "\n",
    "现在我们将我们的标签Y YY转换成softmax分类器所需要的格式，即从(m,1) 转换为独热编码(m,5)，每一行都是经过编码后的样本，其中Y_oh指的是“Y-one-hot”。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "Y_oh_train = emo_utils.convert_to_one_hot(Y_train, C=5)\n",
    "Y_oh_test = emo_utils.convert_to_one_hot(Y_test, C=5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 实现Emojifier-V1\n",
    "第一步就是把输入的句子转换为词向量，然后获取均值，我们依然使用50维的词嵌入，现在我们加载词嵌入："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "word_to_index, index_to_word, word_to_vec_map = emo_utils.read_glove_vecs('data/glove.6B.50d.txt')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "我们加载了：\n",
    "- word_to_index：字典类型的词汇（400,001个）与索引的映射（有效范围：0-400,000）\n",
    "- index_to_word：字典类型的索引与词汇之间的映射。\n",
    "- word_to_vec_map：字典类型的词汇与对应GloVe向量的映射。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "单词cucumber对应的索引是：113317\n",
      "索引113317对应的单词是：cucumber\n"
     ]
    }
   ],
   "source": [
    "word = \"cucumber\"\n",
    "index = 113317\n",
    "print(\"单词{0}对应的索引是：{1}\".format(word, word_to_index[word]))\n",
    "print(\"索引{0}对应的单词是：{1}\".format(index, index_to_word[index]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "我们将实现sentence_to_avg()函数，我们可以将之分为以下两个步骤：\n",
    "\n",
    "- 把每个句子转换为小写，然后分割为列表。我们可以使用X.lower() 与 X.split()。\n",
    "- 对于句子中的每一个单词，转换为GloVe向量，然后对它们取平均。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sentence_to_avg(sentence, word_to_vec_map):\n",
    "    \"\"\"\n",
    "    将句子转换为单词列表，提取其GloVe向量，然后将其平均。\n",
    "    \n",
    "    参数：\n",
    "        sentence -- 字符串类型，从X中获取的样本。\n",
    "        word_to_vec_map -- 字典类型，单词映射到50维的向量的字典\n",
    "        \n",
    "    返回：\n",
    "        avg -- 对句子的均值编码，维度为(50,)\n",
    "    \"\"\"\n",
    "    \n",
    "    # 第一步：分割句子，转换为列表。\n",
    "    words = sentence.lower().split()\n",
    "    \n",
    "    # 初始化均值词向量\n",
    "    avg = np.zeros(50,)\n",
    "    \n",
    "    # 第二步：对词向量取平均。\n",
    "    for w in words:\n",
    "        avg += word_to_vec_map[w]\n",
    "    avg = np.divide(avg, len(words))\n",
    "    \n",
    "    return avg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "avg =  [-0.008005    0.56370833 -0.50427333  0.258865    0.55131103  0.03104983\n",
      " -0.21013718  0.16893933 -0.09590267  0.141784   -0.15708967  0.18525867\n",
      "  0.6495785   0.38371117  0.21102167  0.11301667  0.02613967  0.26037767\n",
      "  0.05820667 -0.01578167 -0.12078833 -0.02471267  0.4128455   0.5152061\n",
      "  0.38756167 -0.898661   -0.535145    0.33501167  0.68806933 -0.2156265\n",
      "  1.797155    0.10476933 -0.36775333  0.750785    0.10282583  0.348925\n",
      " -0.27262833  0.66768    -0.10706167 -0.283635    0.59580117  0.28747333\n",
      " -0.3366635   0.23393817  0.34349183  0.178405    0.1166155  -0.076433\n",
      "  0.1445417   0.09808667]\n"
     ]
    }
   ],
   "source": [
    "avg = sentence_to_avg(\"Morrocan couscous is my favorite dish\", word_to_vec_map)\n",
    "print(\"avg = \", avg)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "我们现在应该实现所有的模型结构了，在使用sentence_to_avg()之后，进行前向传播，计算损失，再进行反向传播，最后再更新参数。\n",
    "\n",
    "我们根据图2-2实现model()函数，Yon是已经经过独热编码后的Y ，那么前向传播以及计算损失的公式如下：\n",
    "    $$ z^{(i)} = W . avg^{(i)} + b$$\n",
    "$$ a^{(i)} = softmax(z^{(i)})$$\n",
    "$$ \\mathcal{L}^{(i)} = - \\sum_{k = 0}^{n_y - 1} Yoh^{(i)}_k * log(a^{(i)}_k)$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def model(X, Y, word_to_vec_map, learning_rate=0.01, num_iterations=400):\n",
    "    \"\"\"\n",
    "    在numpy中训练词向量模型。\n",
    "    \n",
    "    参数：\n",
    "        X -- 输入的字符串类型的数据，维度为(m, 1)。\n",
    "        Y -- 对应的标签，0-7的数组，维度为(m, 1)。\n",
    "        word_to_vec_map -- 字典类型的单词到50维词向量的映射。\n",
    "        learning_rate -- 学习率.\n",
    "        num_iterations -- 迭代次数。\n",
    "        \n",
    "    返回：\n",
    "        pred -- 预测的向量，维度为(m, 1)。\n",
    "        W -- 权重参数，维度为(n_y, n_h)。\n",
    "        b -- 偏置参数，维度为(n_y,)\n",
    "    \"\"\"\n",
    "    np.random.seed(1)\n",
    "    \n",
    "    # 定义训练数量\n",
    "    m = Y.shape[0]\n",
    "    n_y = 5\n",
    "    n_h = 50\n",
    "    \n",
    "    # 使用Xavier初始化参数\n",
    "    W = np.random.randn(n_y, n_h) / np.sqrt(n_h)\n",
    "    b = np.zeros((n_y,))\n",
    "    \n",
    "    # 将Y转换成独热编码\n",
    "    Y_oh = emo_utils.convert_to_one_hot(Y, C=n_y)\n",
    "    \n",
    "    # 优化循环\n",
    "    for t in range(num_iterations):\n",
    "        for i in range(m):\n",
    "            # 获取第i个训练样本的均值\n",
    "            avg = sentence_to_avg(X[i], word_to_vec_map)\n",
    "            \n",
    "            # 前向传播\n",
    "            z = np.dot(W, avg) + b\n",
    "            a = emo_utils.softmax(z)\n",
    "            \n",
    "            # 计算第i个训练的损失\n",
    "            cost = -np.sum(Y_oh[i]*np.log(a))\n",
    "            \n",
    "            # 计算梯度\n",
    "            dz = a - Y_oh[i]\n",
    "            dW = np.dot(dz.reshape(n_y,1), avg.reshape(1, n_h))\n",
    "            db = dz\n",
    "            \n",
    "            # 更新参数\n",
    "            W = W - learning_rate * dW\n",
    "            b = b - learning_rate * db\n",
    "        if t % 100 == 0:\n",
    "            print(\"第{t}轮，损失为{cost}\".format(t=t,cost=cost))\n",
    "            pred = emo_utils.predict(X, Y, W, b, word_to_vec_map)\n",
    "            \n",
    "    return pred, W, b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "第0轮，损失为1.9520498812810072\n",
      "Accuracy: 0.3484848484848485\n",
      "第100轮，损失为0.07971818726014807\n",
      "Accuracy: 0.9318181818181818\n",
      "第200轮，损失为0.04456369243681402\n",
      "Accuracy: 0.9545454545454546\n",
      "第300轮，损失为0.03432267378786059\n",
      "Accuracy: 0.9696969696969697\n"
     ]
    }
   ],
   "source": [
    "pred, W, b = model(X_train, Y_train, word_to_vec_map)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=====训练集====\n",
      "Accuracy: 0.9772727272727273\n",
      "=====测试集====\n",
      "Accuracy: 0.8571428571428571\n"
     ]
    }
   ],
   "source": [
    "print(\"=====训练集====\")\n",
    "pred_train = emo_utils.predict(X_train, Y_train, W, b, word_to_vec_map)\n",
    "print(\"=====测试集====\")\n",
    "pred_test = emo_utils.predict(X_test, Y_test, W, b, word_to_vec_map)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "假设有5个类别，随机猜测的准确率在20%左右，但是仅仅经过127个样本的训练，就有很好的表现。在训练集中，算法看到了“I love you”的句子，其标签为“❤️”，在训练集中没有“adore”这个词汇，如果我们写“I adore you”会发生什么？"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.8333333333333334\n",
      "\n",
      "i adore you ❤️\n",
      "i love you ❤️\n",
      "funny lol 😄\n",
      "lets play with a ball ⚾\n",
      "food is ready 🍴\n",
      "you are not happy ❤️\n"
     ]
    }
   ],
   "source": [
    "X_my_sentences = np.array([\"i adore you\", \"i love you\", \"funny lol\", \"lets play with a ball\", \"food is ready\", \"you are not happy\"])\n",
    "Y_my_labels = np.array([[0], [0], [2], [1], [4],[3]])\n",
    "\n",
    "pred = emo_utils.predict(X_my_sentences, Y_my_labels , W, b, word_to_vec_map)\n",
    "emo_utils.print_predictions(X_my_sentences, pred)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "完成了这一部分之后，你需要记住的是：\n",
    "- 即使你只有128个训练样本，你也可以得到很好地表情符号模型，因为词向量是训练好了的，它会给你一个较好的概括能力。\n",
    "- Emojifier-V1是有缺陷的，比如它不会把“This movie is not good and not enjoyable”划分为不好一类，因为它只是将所有单词的向量做了平均，没有关心过顺序。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Emojifier-V2：在Keras中使用LSTM模块\n",
    "现在我们构建一个能够接受输入文字序列的模型，这个模型会考虑到文字的顺序。Emojifier-V2依然会使用已经训练好的词嵌入。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "np.random.seed(0)\n",
    "import keras\n",
    "from keras.models import Model\n",
    "from keras.layers import Dense, Input, Dropout, LSTM, Activation\n",
    "from keras.layers.embeddings import Embedding\n",
    "from keras.preprocessing import sequence\n",
    "\n",
    "np.random.seed(1)\n",
    "from keras.initializers import glorot_uniform"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 模型预览\n",
    "我们将实现下面这一个模型\n",
    "<img src=\"images/3.png\" style=\"width:700px;height:400px;\"> <br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Keras与mini-batching\n",
    "在这个部分中，我们会使用mini-batches来训练Keras模型，但是大部分深度学习框架需要使用相同的长度的文字，这是因为如果你使用3个单词与4个单词的句子，那么转化为向量之后，计算步骤就有所不同（一个是需要3个LSTM，另一个需要4个LSTM），所以我们不可能对这些句子进行同时训练。\n",
    "\n",
    "那么通用的解决方案是使用填充。指定最长句子的长度，然后对其他句子进行填充到相同长度。比如：指定最大的句子的长度为20，我们可以对每个句子使用“0”来填充，直到句子长度为20，因此，句子“I love you”就可以表示为$(e_{i}, e_{love}, e_{you}, \\vec{0}, \\vec{0}, \\ldots, \\vec{0})$，在这个例子中，任何任何一个超过20个单词的句子将被截取，所以一个比较简单的方式就是找到最长句子，获取它的长度，然后指定它的长度为最长句子的长度。\n",
    "## 嵌入层（ The Embedding layer）\n",
    "在keras里面，嵌入矩阵被表示为“layer”，并将正整数（对应单词的索引）映射到固定大小的Dense向量（词嵌入向量），它可以使用训练好的词嵌入来接着训练或者直接初始化。在这里，我们将学习如何在Keras中创建一个Embedding()层，然后使用Glove的50维向量来初始化。因为我们的数据集很小，所以我们不会更新词嵌入，而是会保留词嵌入的值。\n",
    "在Embedding()层中，输入一个整数矩阵（batch的大小，最大的输入长度），我们可以看看下图：\n",
    "<img src=\"4.png\" style=\"width:700px;height:250px;\">\n",
    "这个例子展示了两个样本通过embedding层，两个样本都经过了`max_len=5`的填充处理，最终的维度就变成了`(2, max_len, 50)`，这是因为使用了50维的词嵌入。\n",
    "\n",
    "第一步就是把所有的要训练的句子转换成索引列表，然后对这些列表使用0填充，直到列表长度为最长句子的长度。\n",
    "\n",
    "  我们先来实现一个函数，输入的是X（字符串类型的句子的数组），再转化为对应的句子列表，输出的是能够让Embedding()函数接受的列表或矩阵（参见图2-4）。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sentences_to_indices(X, word_to_index, max_len):\n",
    "    \"\"\"\n",
    "    输入的是X（字符串类型的句子的数组），再转化为对应的句子列表，\n",
    "    输出的是能够让Embedding()函数接受的列表或矩阵（参见图4）。\n",
    "    \n",
    "    参数：\n",
    "        X -- 句子数组，维度为(m, 1)\n",
    "        word_to_index -- 字典类型的单词到索引的映射\n",
    "        max_len -- 最大句子的长度，数据集中所有的句子的长度都不会超过它。\n",
    "        \n",
    "    返回：\n",
    "        X_indices -- 对应于X中的单词索引数组，维度为(m, max_len)\n",
    "    \"\"\"\n",
    "    \n",
    "    m = X.shape[0]  # 训练集数量\n",
    "    # 使用0初始化X_indices\n",
    "    X_indices = np.zeros((m, max_len))\n",
    "    \n",
    "    for i in range(m):\n",
    "        # 将第i个句子转化为小写并按单词分开。\n",
    "        sentences_words = X[i].lower().split()\n",
    "        \n",
    "        # 初始化j为0\n",
    "        j = 0\n",
    "        \n",
    "        # 遍历这个单词列表\n",
    "        for w in sentences_words:\n",
    "            # 将X_indices的第(i, j)号元素为对应的单词索引\n",
    "            X_indices[i, j] = word_to_index[w]\n",
    "            \n",
    "            j += 1\n",
    "            \n",
    "    return X_indices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X1 = ['funny lol' 'lets play baseball' 'food is ready for you']\n",
      "X1_indices = [[155345. 225122.      0.      0.      0.]\n",
      " [220930. 286375.  69714.      0.      0.]\n",
      " [151204. 192973. 302254. 151349. 394475.]]\n"
     ]
    }
   ],
   "source": [
    "X1 = np.array([\"funny lol\", \"lets play baseball\", \"food is ready for you\"])\n",
    "X1_indices = sentences_to_indices(X1,word_to_index, max_len = 5)\n",
    "print(\"X1 =\", X1)\n",
    "print(\"X1_indices =\", X1_indices)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "现在我们就在Keras中构建Embedding()层，我们使用的是已经训练好了的词向量，在构建之后，使用sentences_to_indices()生成的数据作为输入，Embedding()层将返回每个句子的词嵌入。\n",
    "\n",
    "我们现在就实现pretrained_embedding_layer()函数，它可以分为以下几个步骤：\n",
    "- 使用0来初始化嵌入矩阵。\n",
    "- 使用word_to_vec_map来将词嵌入矩阵填充进嵌入矩阵。\n",
    "- 在Keras中定义嵌入层，当调用Embedding()的时候需要让这一层的参数不能被训练，所以我们可以设置trainable=False。\n",
    "- 将词嵌入的权值设置为词嵌入的值"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "def pretrained_embedding_layer(word_to_vec_map, word_to_index):\n",
    "    \"\"\"\n",
    "    创建Keras Embedding()层，加载已经训练好了的50维GloVe向量\n",
    "    \n",
    "    参数：\n",
    "        word_to_vec_map -- 字典类型的单词与词嵌入的映射\n",
    "        word_to_index -- 字典类型的单词到词汇表（400,001个单词）的索引的映射。\n",
    "        \n",
    "    返回：\n",
    "        embedding_layer() -- 训练好了的Keras的实体层。\n",
    "    \"\"\"\n",
    "    vocab_len = len(word_to_index) + 1\n",
    "    emb_dim = word_to_vec_map[\"cucumber\"].shape[0]\n",
    "    \n",
    "    # 初始化嵌入矩阵\n",
    "    emb_matrix = np.zeros((vocab_len, emb_dim))\n",
    "    \n",
    "    # 将嵌入矩阵的每行的“index”设置为词汇“index”的词向量表示\n",
    "    for word, index in word_to_index.items():\n",
    "        emb_matrix[index, :] = word_to_vec_map[word]\n",
    "    \n",
    "    # 定义Keras的embbeding层\n",
    "    embedding_layer = Embedding(vocab_len, emb_dim, trainable=False)\n",
    "    \n",
    "    # 构建embedding层。\n",
    "    embedding_layer.build((None,))\n",
    "    \n",
    "    # 将嵌入层的权重设置为嵌入矩阵。\n",
    "    embedding_layer.set_weights([emb_matrix])\n",
    "    \n",
    "    return embedding_layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "weights[0][1][3] = -0.3403\n"
     ]
    }
   ],
   "source": [
    "embedding_layer = pretrained_embedding_layer(word_to_vec_map, word_to_index)\n",
    "print(\"weights[0][1][3] =\", embedding_layer.get_weights()[0][1][3])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 构建Emojifier-V2\n",
    "现在我们开始构建Emojifier-V2模型。embedding层我们已经构建完成了，现在我们将它的输出输入到LSTM中。\n",
    "<img src=\"5.png\" style=\"width:700px;height:400px;\"> <br>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "def Emojify_V2(input_shape, word_to_vec_map, word_to_index):\n",
    "    \"\"\"\n",
    "    实现Emojify-V2模型的计算图\n",
    "    \n",
    "    参数：\n",
    "        input_shape -- 输入的维度，通常是(max_len,)\n",
    "        word_to_vec_map -- 字典类型的单词与词嵌入的映射。\n",
    "        word_to_index -- 字典类型的单词到词汇表（400,001个单词）的索引的映射。\n",
    "    \n",
    "    返回：\n",
    "        model -- Keras模型实体\n",
    "    \"\"\"\n",
    "    # 定义sentence_indices为计算图的输入，维度为(input_shape,)，类型为dtype 'int32' \n",
    "    sentence_indices = Input(input_shape, dtype='int32')\n",
    "    \n",
    "    # 创建embedding层\n",
    "    embedding_layer = pretrained_embedding_layer(word_to_vec_map, word_to_index)\n",
    "    \n",
    "    # 通过嵌入层传播sentence_indices，你会得到嵌入的结果\n",
    "    embeddings = embedding_layer(sentence_indices)\n",
    "    \n",
    "    # 通过带有128维隐藏状态的LSTM层传播嵌入\n",
    "    # 需要注意的是，返回的输出应该是一批序列。\n",
    "    X = LSTM(128, return_sequences=True)(embeddings)\n",
    "    # 使用dropout，概率为0.5\n",
    "    X = Dropout(0.5)(X)\n",
    "    # 通过另一个128维隐藏状态的LSTM层传播X\n",
    "    # 注意，返回的输出应该是单个隐藏状态，而不是一组序列。\n",
    "    X = LSTM(128, return_sequences=False)(X)\n",
    "    # 使用dropout，概率为0.5\n",
    "    X = Dropout(0.5)(X)\n",
    "    # 通过softmax激活的Dense层传播X，得到一批5维向量。\n",
    "    X = Dense(5)(X)\n",
    "    # 添加softmax激活\n",
    "    X = Activation('softmax')(X)\n",
    "    \n",
    "    # 创建模型实体\n",
    "    model = Model(inputs=sentence_indices, outputs=X)\n",
    "    \n",
    "    return model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "因为数据集中所有句子都小于10个单词，所以我们选择max_len=10。在接下来的代码中，你应该可以看到有“20,223,927”个参数，其中“20,000,050”个参数没有被训练（这是因为它是词向量），剩下的是有“223,877”被训练了的。因为我们的单词表有400,001个单词，所以是400,001∗50=20,000,050 个不可训练的参数。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From D:\\Anaconda3\\lib\\site-packages\\keras\\backend\\tensorflow_backend.py:3445: calling dropout (from tensorflow.python.ops.nn_ops) with keep_prob is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use `rate` instead of `keep_prob`. Rate should be set to `rate = 1 - keep_prob`.\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_1 (InputLayer)         (None, 10)                0         \n",
      "_________________________________________________________________\n",
      "embedding_4 (Embedding)      (None, 10, 50)            20000050  \n",
      "_________________________________________________________________\n",
      "lstm_1 (LSTM)                (None, 10, 128)           91648     \n",
      "_________________________________________________________________\n",
      "dropout_1 (Dropout)          (None, 10, 128)           0         \n",
      "_________________________________________________________________\n",
      "lstm_2 (LSTM)                (None, 128)               131584    \n",
      "_________________________________________________________________\n",
      "dropout_2 (Dropout)          (None, 128)               0         \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 5)                 645       \n",
      "_________________________________________________________________\n",
      "activation_1 (Activation)    (None, 5)                 0         \n",
      "=================================================================\n",
      "Total params: 20,223,927\n",
      "Trainable params: 223,877\n",
      "Non-trainable params: 20,000,050\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model = Emojify_V2((10,), word_to_vec_map, word_to_index)\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_indices = sentences_to_indices(X_train, word_to_index, maxLen)\n",
    "Y_train_oh = emo_utils.convert_to_one_hot(Y_train, C = 5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From D:\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\ops\\math_ops.py:3066: to_int32 (from tensorflow.python.ops.math_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use tf.cast instead.\n",
      "Epoch 1/50\n",
      "132/132 [==============================] - 7s 57ms/step - loss: 1.6083 - acc: 0.1970\n",
      "Epoch 2/50\n",
      "132/132 [==============================] - 0s 2ms/step - loss: 1.5324 - acc: 0.2955\n",
      "Epoch 3/50\n",
      "132/132 [==============================] - 0s 2ms/step - loss: 1.5011 - acc: 0.3258\n",
      "Epoch 4/50\n",
      "132/132 [==============================] - 0s 1ms/step - loss: 1.4391 - acc: 0.3561\n",
      "Epoch 5/50\n",
      "132/132 [==============================] - 0s 1ms/step - loss: 1.3480 - acc: 0.4545\n",
      "Epoch 6/50\n",
      "132/132 [==============================] - 0s 1ms/step - loss: 1.2345 - acc: 0.5152\n",
      "Epoch 7/50\n",
      "132/132 [==============================] - 0s 2ms/step - loss: 1.1767 - acc: 0.4470\n",
      "Epoch 8/50\n",
      "132/132 [==============================] - 0s 1ms/step - loss: 1.0551 - acc: 0.5758\n",
      "Epoch 9/50\n",
      "132/132 [==============================] - 0s 1ms/step - loss: 0.8772 - acc: 0.7121\n",
      "Epoch 10/50\n",
      "132/132 [==============================] - 0s 1ms/step - loss: 0.8227 - acc: 0.6970\n",
      "Epoch 11/50\n",
      "132/132 [==============================] - 0s 1ms/step - loss: 0.7030 - acc: 0.7500\n",
      "Epoch 12/50\n",
      "132/132 [==============================] - 0s 1ms/step - loss: 0.5999 - acc: 0.8030\n",
      "Epoch 13/50\n",
      "132/132 [==============================] - 0s 1ms/step - loss: 0.4925 - acc: 0.8333\n",
      "Epoch 14/50\n",
      "132/132 [==============================] - 0s 2ms/step - loss: 0.5097 - acc: 0.8333\n",
      "Epoch 15/50\n",
      "132/132 [==============================] - 0s 1ms/step - loss: 0.4789 - acc: 0.8258\n",
      "Epoch 16/50\n",
      "132/132 [==============================] - 0s 1ms/step - loss: 0.3543 - acc: 0.8636\n",
      "Epoch 17/50\n",
      "132/132 [==============================] - 0s 1ms/step - loss: 0.3910 - acc: 0.8561\n",
      "Epoch 18/50\n",
      "132/132 [==============================] - 0s 1ms/step - loss: 0.6488 - acc: 0.8106\n",
      "Epoch 19/50\n",
      "132/132 [==============================] - 0s 1ms/step - loss: 0.5177 - acc: 0.8182\n",
      "Epoch 20/50\n",
      "132/132 [==============================] - 0s 1ms/step - loss: 0.3954 - acc: 0.8409\n",
      "Epoch 21/50\n",
      "132/132 [==============================] - 0s 1ms/step - loss: 0.4705 - acc: 0.8182\n",
      "Epoch 22/50\n",
      "132/132 [==============================] - 0s 1ms/step - loss: 0.3883 - acc: 0.8636\n",
      "Epoch 23/50\n",
      "132/132 [==============================] - 0s 1ms/step - loss: 0.3783 - acc: 0.8561\n",
      "Epoch 24/50\n",
      "132/132 [==============================] - 0s 1ms/step - loss: 0.3052 - acc: 0.9091\n",
      "Epoch 25/50\n",
      "132/132 [==============================] - 0s 1ms/step - loss: 0.3477 - acc: 0.8864\n",
      "Epoch 26/50\n",
      "132/132 [==============================] - 0s 1ms/step - loss: 0.2418 - acc: 0.9394\n",
      "Epoch 27/50\n",
      "132/132 [==============================] - 0s 1ms/step - loss: 0.3171 - acc: 0.8788\n",
      "Epoch 28/50\n",
      "132/132 [==============================] - 0s 1ms/step - loss: 0.2404 - acc: 0.9318\n",
      "Epoch 29/50\n",
      "132/132 [==============================] - 0s 1ms/step - loss: 0.3936 - acc: 0.8712\n",
      "Epoch 30/50\n",
      "132/132 [==============================] - 0s 1ms/step - loss: 0.2669 - acc: 0.9091\n",
      "Epoch 31/50\n",
      "132/132 [==============================] - 0s 1ms/step - loss: 0.2952 - acc: 0.8864\n",
      "Epoch 32/50\n",
      "132/132 [==============================] - 0s 1ms/step - loss: 0.2034 - acc: 0.9318\n",
      "Epoch 33/50\n",
      "132/132 [==============================] - 0s 1ms/step - loss: 0.2117 - acc: 0.9470\n",
      "Epoch 34/50\n",
      "132/132 [==============================] - 0s 1ms/step - loss: 0.1577 - acc: 0.9621\n",
      "Epoch 35/50\n",
      "132/132 [==============================] - 0s 1ms/step - loss: 0.1644 - acc: 0.9621\n",
      "Epoch 36/50\n",
      "132/132 [==============================] - 0s 1ms/step - loss: 0.1889 - acc: 0.9394A: 0s - loss: 0.2063 - acc: 0.927\n",
      "Epoch 37/50\n",
      "132/132 [==============================] - 0s 1ms/step - loss: 0.1792 - acc: 0.9394\n",
      "Epoch 38/50\n",
      "132/132 [==============================] - 0s 1ms/step - loss: 0.2159 - acc: 0.9394\n",
      "Epoch 39/50\n",
      "132/132 [==============================] - 0s 1ms/step - loss: 0.1399 - acc: 0.9621\n",
      "Epoch 40/50\n",
      "132/132 [==============================] - 0s 1ms/step - loss: 0.1548 - acc: 0.9545\n",
      "Epoch 41/50\n",
      "132/132 [==============================] - 0s 1ms/step - loss: 0.0870 - acc: 0.9848\n",
      "Epoch 42/50\n",
      "132/132 [==============================] - 0s 1ms/step - loss: 0.0807 - acc: 0.9773\n",
      "Epoch 43/50\n",
      "132/132 [==============================] - 0s 1ms/step - loss: 0.0806 - acc: 0.9848\n",
      "Epoch 44/50\n",
      "132/132 [==============================] - 0s 2ms/step - loss: 0.0495 - acc: 0.9924\n",
      "Epoch 45/50\n",
      "132/132 [==============================] - 0s 1ms/step - loss: 0.0782 - acc: 0.9848\n",
      "Epoch 46/50\n",
      "132/132 [==============================] - 0s 1ms/step - loss: 0.1017 - acc: 0.9773\n",
      "Epoch 47/50\n",
      "132/132 [==============================] - 0s 1ms/step - loss: 0.1453 - acc: 0.9470\n",
      "Epoch 48/50\n",
      "132/132 [==============================] - 0s 1ms/step - loss: 0.3145 - acc: 0.9242\n",
      "Epoch 49/50\n",
      "132/132 [==============================] - 0s 1ms/step - loss: 0.1128 - acc: 0.9848\n",
      "Epoch 50/50\n",
      "132/132 [==============================] - 0s 1ms/step - loss: 0.1712 - acc: 0.9545\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x2009dc92e48>"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.fit(X_train_indices, Y_train_oh, epochs = 50, batch_size = 32, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "56/56 [==============================] - 1s 9ms/step\n",
      "Test accuracy =  0.8214285629136222\n"
     ]
    }
   ],
   "source": [
    "X_test_indices = sentences_to_indices(X_test, word_to_index, max_len = maxLen)\n",
    "Y_test_oh = emo_utils.convert_to_one_hot(Y_test, C = 5)\n",
    "loss, acc = model.evaluate(X_test_indices, Y_test_oh)\n",
    "\n",
    "print(\"Test accuracy = \", acc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "正确表情：😄   预测结果： she got me a nice present\t❤️\n",
      "正确表情：😞   预测结果： work is hard\t😄\n",
      "正确表情：😞   预测结果： This girl is messing with me\t❤️\n",
      "正确表情：🍴   预测结果： any suggestions for dinner\t😄\n",
      "正确表情：❤️   预测结果： I love taking breaks\t😞\n",
      "正确表情：😄   预测结果： you brighten my day\t❤️\n",
      "正确表情：😄   预测结果： will you be my valentine\t❤️\n",
      "正确表情：🍴   预测结果： See you at the restaurant\t😄\n",
      "正确表情：😞   预测结果： go away\t⚾\n",
      "正确表情：🍴   预测结果： I did not have breakfast ❤️\n"
     ]
    }
   ],
   "source": [
    "C = 5\n",
    "y_test_oh = np.eye(C)[Y_test.reshape(-1)]\n",
    "X_test_indices = sentences_to_indices(X_test, word_to_index, maxLen)\n",
    "pred = model.predict(X_test_indices)\n",
    "for i in range(len(X_test)):\n",
    "    x = X_test_indices\n",
    "    num = np.argmax(pred[i])\n",
    "    if(num != Y_test[i]):\n",
    "        print('正确表情：'+ emo_utils.label_to_emoji(Y_test[i]) + '   预测结果： '+ X_test[i] + emo_utils.label_to_emoji(num).strip())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "you are so beautiful ❤️\n"
     ]
    }
   ],
   "source": [
    "#可以试试自己写一些话来预测\n",
    "x_test = np.array(['you are so beautiful'])\n",
    "X_test_indices = sentences_to_indices(x_test, word_to_index, maxLen)\n",
    "print(x_test[0] +' '+  emo_utils.label_to_emoji(np.argmax(model.predict(X_test_indices))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "you are so lucky 😞\n"
     ]
    }
   ],
   "source": [
    "x_test = np.array(['you are so lucky'])\n",
    "X_test_indices = sentences_to_indices(x_test, word_to_index, maxLen)\n",
    "print(x_test[0] +' '+  emo_utils.label_to_emoji(np.argmax(model.predict(X_test_indices))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "you are so nice ❤️\n"
     ]
    }
   ],
   "source": [
    "x_test = np.array(['you are so nice'])\n",
    "X_test_indices = sentences_to_indices(x_test, word_to_index, maxLen)\n",
    "print(x_test[0] +' '+  emo_utils.label_to_emoji(np.argmax(model.predict(X_test_indices))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Emoji generator",
   "toc_cell": false,
   "toc_position": {
    "height": "calc(100% - 180px)",
    "left": "10px",
    "top": "150px",
    "width": "304.475px"
   },
   "toc_section_display": true,
   "toc_window_display": true
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
